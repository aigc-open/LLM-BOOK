# LMCache æ¡†æ¶æ·±åº¦è§£æï¼šæ¶æ„ä¸äºŒæ¬¡å¼€å‘æŒ‡å—

## ç›®å½•
1. [LMCache æ˜¯ä»€ä¹ˆ](#lmcache-æ˜¯ä»€ä¹ˆ)
2. [å·¥ç¨‹è¿˜æ˜¯ç®—æ³•ï¼Ÿä¸ GPU çš„å…³ç³»](#å·¥ç¨‹è¿˜æ˜¯ç®—æ³•ä¸-gpu-çš„å…³ç³»)
3. [æ¡†æ¶é€‚é…æƒ…å†µ](#æ¡†æ¶é€‚é…æƒ…å†µ)
4. [æ ¸å¿ƒæ¶æ„è§£æ](#æ ¸å¿ƒæ¶æ„è§£æ)
5. [äºŒæ¬¡å¼€å‘æŒ‡å—](#äºŒæ¬¡å¼€å‘æŒ‡å—)
6. [æ€»ç»“](#æ€»ç»“)

---

## LMCache æ˜¯ä»€ä¹ˆ

### 1.1 æ ¸å¿ƒå®šä½

**LMCache æ˜¯ä¸€ä¸ª LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰æ¨ç†åŠ é€Ÿçš„å¼•æ“æ‰©å±•**ï¼Œå®ƒçš„æ ¸å¿ƒç›®æ ‡æ˜¯ï¼š

- **å‡å°‘ TTFTï¼ˆTime To First Tokenï¼‰**ï¼šé¦–ä¸ª token çš„ç”Ÿæˆæ—¶é—´
- **æå‡ååé‡**ï¼šæ•´ä½“æ¨ç†æ€§èƒ½
- **ä¼˜åŒ–é•¿ä¸Šä¸‹æ–‡åœºæ™¯**ï¼šç‰¹åˆ«é€‚åˆå¤šè½®å¯¹è¯ã€RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç­‰åœºæ™¯

### 1.2 å·¥ä½œåŸç†

LMCache çš„æ ¸å¿ƒæ€æƒ³æ˜¯ **KV Cache çš„å¤ç”¨**ï¼š

åœ¨å¤§è¯­è¨€æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ª token éƒ½ä¼šäº§ç”Ÿ Key å’Œ Value å¼ é‡ï¼ˆKV Cacheï¼‰ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸­ï¼Œæ¯æ¬¡æ¨ç†éƒ½éœ€è¦é‡æ–°è®¡ç®—è¿™äº› KV Cacheï¼Œå³ä½¿è¾“å…¥çš„æ–‡æœ¬éƒ¨åˆ†é‡å¤ã€‚

**LMCache çš„åˆ›æ–°**ï¼š
- å°†ä»»ä½•å¯é‡ç”¨çš„æ–‡æœ¬ç‰‡æ®µçš„ KV Cache å­˜å‚¨èµ·æ¥
- ä¸ä»…é™äºå‰ç¼€ï¼ˆprefixï¼‰åŒ¹é…ï¼Œä»»ä½•é‡å¤å‡ºç°çš„æ–‡æœ¬æ®µéƒ½å¯ä»¥å¤ç”¨
- è·¨è¯·æ±‚ã€è·¨å®ä¾‹å…±äº« KV Cache

**æ€§èƒ½æå‡**ï¼š
æ ¹æ®å®˜æ–¹æ•°æ®ï¼Œç»“åˆ vLLM ä½¿ç”¨æ—¶ï¼Œå¯ä»¥åœ¨å¤šè½® QA å’Œ RAG åœºæ™¯ä¸‹å®ç°ï¼š
- **3-10x å»¶è¿Ÿé™ä½**
- **3-10x GPU è®¡ç®—å‘¨æœŸèŠ‚çœ**

### 1.3 å…¸å‹åº”ç”¨åœºæ™¯

1. **å¤šè½®å¯¹è¯ï¼ˆMulti-round QAï¼‰**
   - ç³»ç»Ÿæç¤ºè¯ï¼ˆSystem Promptï¼‰æ¯æ¬¡éƒ½ä¸€æ ·
   - å†å²å¯¹è¯å†…å®¹å¯ä»¥å¤ç”¨
   - å¤§å¹…å‡å°‘é‡å¤è®¡ç®—

2. **RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰**
   - æ£€ç´¢åˆ°çš„æ–‡æ¡£å¯ä»¥ç¼“å­˜
   - ç›¸åŒæŸ¥è¯¢å¯ä»¥ç›´æ¥å¤ç”¨ KV Cache
   - åŠ é€Ÿæ–‡æ¡£ç†è§£å’Œé—®ç­”

3. **é•¿æ–‡æ¡£å¤„ç†**
   - é•¿æ–‡æ¡£åªéœ€ prefill ä¸€æ¬¡
   - åç»­æŸ¥è¯¢ç›´æ¥å¤ç”¨
   - èŠ‚çœå¤§é‡ GPU æ˜¾å­˜å’Œè®¡ç®—

4. **Agent ç³»ç»Ÿ**
   - å·¥å…·æè¿°ã€å‡½æ•°ç­¾åç­‰å›ºå®šå†…å®¹
   - å¤ç”¨å…¬å…±çŸ¥è¯†åº“å†…å®¹

---

## å·¥ç¨‹ï¼Œç®—æ³•ï¼Œä¸ GPU çš„å…³ç³»

### 2.1 é¡¹ç›®å®šä½ï¼šå·¥ç¨‹ä¸ºä¸»ï¼Œç®—æ³•ä¸ºè¾…

**LMCache æ˜¯ä¸€ä¸ªä»¥å·¥ç¨‹å®ç°ä¸ºæ ¸å¿ƒçš„é¡¹ç›®**ï¼Œä½†ä¹Ÿèåˆäº†ç®—æ³•åˆ›æ–°ï¼š

#### å·¥ç¨‹æ–¹é¢ï¼ˆä¸»ä½“ï¼‰

1. **å¤šå±‚çº§å­˜å‚¨æ¶æ„**
   - GPU Memoryï¼ˆæ´»è·ƒ KV Cacheï¼‰
   - CPU DRAMï¼ˆçƒ­ç¼“å­˜ï¼Œä½¿ç”¨ pinned memory åŠ é€Ÿä¼ è¾“ï¼‰
   - Local Storageï¼ˆæœ¬åœ°ç£ç›˜ã€NVMe GDSï¼‰
   - Remote Storageï¼ˆRedisã€Mooncakeã€InfiniStore ç­‰ï¼‰

2. **é«˜æ€§èƒ½ C++/CUDA å®ç°**
   ```
   csrc/
   â”œâ”€â”€ ac_dec.cu          # ç®—æœ¯ç¼–ç è§£ç 
   â”œâ”€â”€ ac_enc.cu          # ç®—æœ¯ç¼–ç 
   â”œâ”€â”€ cal_cdf.cu         # CDF è®¡ç®—
   â”œâ”€â”€ mem_kernels.cu     # å†…å­˜æ“ä½œæ ¸å‡½æ•°
   â”œâ”€â”€ pos_kernels.cu     # ä½ç½®ç¼–ç æ ¸å‡½æ•°
   â””â”€â”€ ...
   ```

3. **å¼‚æ­¥åŒ–è®¾è®¡**
   - å¼‚æ­¥ Offloadï¼šGPU â†’ CPU â†’ Disk/Remote
   - å¼‚æ­¥ Prefetchï¼šDisk/Remote â†’ CPU â†’ GPU
   - ä¸é˜»å¡æ¨ç†ä¸»çº¿ç¨‹

4. **åˆ†å¸ƒå¼åè°ƒ**
   - P2P KV Cache å…±äº«
   - Disaggregated Prefillï¼ˆPrefill-Decode åˆ†ç¦»ï¼‰
   - è·¨å®ä¾‹ç¼“å­˜åŒæ­¥

#### ç®—æ³•æ–¹é¢ï¼ˆå¢å¼ºï¼‰

1. **CacheGen å‹ç¼©ç®—æ³•**
   - KV Cache å‹ç¼©å’Œæµå¼ä¼ è¾“
   - åŸºäºç®—æœ¯ç¼–ç çš„æ— æŸ/æœ‰æŸå‹ç¼©
   - è®ºæ–‡ï¼š*CacheGen: KV Cache Compression and Streaming for Fast LLM Serving* (SIGCOMM 2024)

2. **CacheBlend èåˆæŠ€æœ¯**
   - ç¼“å­˜çŸ¥è¯†èåˆ
   - æ™ºèƒ½ KV Cache æ··åˆ
   - è®ºæ–‡ï¼š*CacheBlend: Fast LLM Serving with Cached Knowledge Fusion* (EuroSys 2025)

3. **ç¼“å­˜ç­–ç•¥**
   - LRUï¼ˆLeast Recently Usedï¼‰é©±é€ç­–ç•¥
   - Layerwise ç¼“å­˜ä¼˜åŒ–
   - Token åºåˆ—å“ˆå¸Œå’Œç´¢å¼•

### 2.2 ä¸ GPU çš„æ·±åº¦å…³ç³»

**LMCache ä¸ GPU é«˜åº¦ç›¸å…³ï¼Œä½†æ”¯æŒå¤šç§ç¡¬ä»¶**ï¼š

#### NVIDIA GPU æ”¯æŒï¼ˆä¸»æµï¼‰

- **è¦æ±‚**ï¼šCompute Capability 7.0+
- **æ”¯æŒå¡å‹**ï¼šV100, T4, RTX 20xx, A100, L4, H100, B200 ç­‰
- **CUDA ç‰ˆæœ¬**ï¼š12.1+
- **æ„å»ºæ–¹å¼**ï¼š
  ```bash
  # ä½¿ç”¨ CUDA æ„å»º
  uv pip install lmcache
  ```

#### AMD GPU æ”¯æŒï¼ˆROCmï¼‰

- **æ”¯æŒå¡å‹**ï¼šMI300X ç­‰ AMD Instinct ç³»åˆ—
- **ROCm æ”¯æŒ**ï¼šé€šè¿‡ HIP ç¼–è¯‘
- **æ„å»ºæ–¹å¼**ï¼š
  ```bash
  PYTORCH_ROCM_ARCH="gfx942" \
  TORCH_DONT_CHECK_COMPILER_ABI=1 \
  CXX=hipcc \
  BUILD_WITH_HIP=1 \
  python3 -m pip install --no-build-isolation -e .
  ```

#### è®¾å¤‡å±‚çº§åˆ’åˆ†

LMCache çš„è®¾è®¡å¤©ç„¶éœ€è¦**åŒºåˆ†ä¸åŒè®¾å¤‡**ï¼š

| å­˜å‚¨å±‚çº§ | å®¹é‡ | é€Ÿåº¦ | ç”¨é€” |
|---------|------|------|------|
| GPU Memory | å°ï¼ˆ40GB-80GBï¼‰ | æå¿« | æ´»è·ƒ KV Cache |
| CPU DRAM | ä¸­ï¼ˆæ•°ç™¾ GBï¼‰ | å¿« | çƒ­ç¼“å­˜ï¼Œpinned memory |
| Local Disk | å¤§ï¼ˆæ•° TBï¼‰ | ä¸­ | é•¿æ–‡æ¡£ã€å†å²ç¼“å­˜ |
| Remote Storage | æå¤§ | æ…¢ | æŒä¹…åŒ–ã€è·¨å®ä¾‹å…±äº« |

**å…³é”®æŠ€æœ¯**ï¼š
- **Pinned Memory**ï¼šCPU ä¾§ä½¿ç”¨é”é¡µå†…å­˜ï¼ŒåŠ é€Ÿ GPU-CPU ä¼ è¾“
- **NUMA-aware Allocation**ï¼šNUMA æ„ŸçŸ¥çš„å†…å­˜åˆ†é…
- **Async Copy**ï¼šå¼‚æ­¥å†…å­˜æ‹·è´ï¼Œä¸é˜»å¡ CUDA Stream

### 2.3 åº•å±‚å®ç°è¯­è¨€

```python
# Python å±‚ï¼ˆlmcache/ï¼‰
- é…ç½®ç®¡ç†ã€å­˜å‚¨åç«¯æŠ½è±¡
- ä¸æ¨ç†æ¡†æ¶é›†æˆï¼ˆvLLM, SGLangï¼‰
- ç¼“å­˜ç­–ç•¥ã€æ§åˆ¶é€»è¾‘

# C++/CUDA å±‚ï¼ˆcsrc/ï¼‰
- é«˜æ€§èƒ½å†…å­˜æ“ä½œ
- CUDA æ ¸å‡½æ•°ï¼ˆå‹ç¼©ã€ç¼–ç ã€å†…å­˜æ‹·è´ï¼‰
- ä½ç½®ç¼–ç è®¡ç®—

# HIP å±‚ï¼ˆROCm æ”¯æŒï¼‰
- AMD GPU é€‚é…
- é€šè¿‡ hipify è‡ªåŠ¨è½¬æ¢
```

---

## æ¡†æ¶é€‚é…æƒ…å†µ

### 3.1 å¹¶éå¼€ç®±å³ç”¨æ‰€æœ‰æ¡†æ¶

**LMCache ä¸æ˜¯ä»»æ„æ¨ç†æ¡†æ¶éƒ½èƒ½ç›´æ¥ä½¿ç”¨çš„**ï¼Œå®ƒéœ€è¦ä¸ç‰¹å®šæ¡†æ¶æ·±åº¦é›†æˆã€‚

### 3.2 å®˜æ–¹æ”¯æŒçš„æ¡†æ¶

#### 1. vLLMï¼ˆä¸»è¦é›†æˆï¼‰

**vLLM v1 é›†æˆï¼ˆæ¨èï¼‰**

- **é›†æˆæ–¹å¼**ï¼šé€šè¿‡ KV Connector
- **æ”¯æŒç‰¹æ€§**ï¼š
  - âœ… é«˜æ€§èƒ½ CPU KV Cache offloading
  - âœ… Disaggregated prefillï¼ˆPrefill-Decode åˆ†ç¦»ï¼‰
  - âœ… P2P KV Cache å…±äº«
  - âœ… å¤šç§å­˜å‚¨åç«¯

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
from vllm import LLM, SamplingParams

# åˆ›å»º vLLM å®ä¾‹ï¼Œé€šè¿‡é…ç½®å¯ç”¨ LMCache
llm = LLM(
    model="meta-llama/Llama-2-7b-hf",
    kv_transfer_config={
        "kv_connector": "LMCacheConnector",
        "kv_buffer_size": 1e9,  # 1GB
    }
)

# æ­£å¸¸æ¨ç†å³å¯ï¼ŒLMCache è‡ªåŠ¨ç®¡ç† KV Cache
outputs = llm.generate(prompts, sampling_params)
```

**vLLM v0 æ”¯æŒï¼ˆæ—§ç‰ˆï¼‰**

- é€šè¿‡ `examples/others/lmcache/cpu_offload_lmcache.py`
- åŠŸèƒ½ç›¸å¯¹å—é™

**å…¼å®¹æ€§çŸ©é˜µ**ï¼ˆæ ¸å¿ƒç‰ˆæœ¬ï¼‰ï¼š

| vLLM ç‰ˆæœ¬ | LMCache 0.3.7 | LMCache 0.3.6 | LMCache 0.3.5 |
|-----------|---------------|---------------|---------------|
| 0.10.2.x  | âœ… | âœ… | ğŸ•¯ï¸ (torch ä¸å…¼å®¹) |
| 0.10.1.x  | ğŸ•¯ï¸ | âŒ (API ä¸å…¼å®¹) | âœ… |
| 0.10.0.x  | ğŸ•¯ï¸ | âŒ | âœ… |

ğŸ•¯ï¸ è¡¨ç¤ºéœ€è¦ `--no-build-isolation` è§£å†³ torch ç‰ˆæœ¬å†²çª

#### 2. SGLang é›†æˆ

- **æ”¯æŒç¨‹åº¦**ï¼šKV cache offloading
- **é›†æˆè·¯å¾„**ï¼š`lmcache/integration/sglang/`
- **åŠŸèƒ½**ï¼šCPU offload ä¸ºä¸»

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
# é€šè¿‡é…ç½®æ–‡ä»¶å¯ç”¨ LMCache
import sglang as sgl

# é…ç½® LMCache
lmcache_config = {
    "local_device": "cpu",
    "remote_url": None,
}

# SGLang å¼•æ“ä¼šè‡ªåŠ¨ä½¿ç”¨ LMCache
```

#### 3. Production Stack æ”¯æŒ

LMCache å·²è¢«å®˜æ–¹é›†æˆåˆ°ï¼š
- **vLLM Production Stack**ï¼šä¼ä¸šçº§éƒ¨ç½²æ–¹æ¡ˆ
- **llm-d**ï¼šéƒ¨ç½²å·¥å…·é“¾
- **KServe**ï¼šKubernetes åŸç”Ÿæ¨¡å‹æœåŠ¡

### 3.3 é›†æˆæ¶æ„ï¼ˆConnector æ¨¡å¼ï¼‰

LMCache ä½¿ç”¨ **Connector æ¨¡å¼** ä¸æ¨ç†æ¡†æ¶é›†æˆï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Inference Engine (vLLM/SGLang)       â”‚
â”‚                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚   KV Memory Manager             â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                  â”‚                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚   LMCache Connector             â”‚â—„â”€â”¼â”€â”€ Integration Layer
â”‚   â”‚   - Cache Lookup                â”‚  â”‚
â”‚   â”‚   - Cache Store                 â”‚  â”‚
â”‚   â”‚   - Async Operations            â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   LMCache Storage Backend       â”‚
    â”‚   - GPU â†’ CPU â†’ Disk â†’ Remote   â”‚
    â”‚   - Multi-tier Cache Management â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”® Connector å®ç°**ï¼š

```python
# lmcache/integration/vllm/lmcache_connector_v1.py
class LMCacheConnector:
    """vLLM v1 çš„ KV Connector"""
    
    def __init__(self, config):
        self.cache_engine = LMCacheEngine(config)
    
    def lookup(self, token_ids):
        """æŸ¥è¯¢ KV Cache"""
        return self.cache_engine.retrieve(token_ids)
    
    def store(self, token_ids, kv_cache):
        """å­˜å‚¨ KV Cache"""
        self.cache_engine.store(token_ids, kv_cache)
```

### 3.4 å¦‚ä½•æ”¯æŒæ–°æ¡†æ¶ï¼Ÿ

è¦å°† LMCache é›†æˆåˆ°æ–°çš„æ¨ç†æ¡†æ¶ï¼Œéœ€è¦ï¼š

1. **å®ç° Connector æ¥å£**
   - `lookup(token_ids)`: æŸ¥è¯¢ç¼“å­˜
   - `store(token_ids, kv_cache)`: å­˜å‚¨ç¼“å­˜
   - `exists(token_ids)`: æ£€æŸ¥æ˜¯å¦å­˜åœ¨

2. **Hook åˆ°æ¡†æ¶çš„ KV Cache ç®¡ç†**
   - Prefill é˜¶æ®µï¼šæ£€æŸ¥ç¼“å­˜å‘½ä¸­
   - Decode é˜¶æ®µï¼šå­˜å‚¨æ–°ç”Ÿæˆçš„ KV

3. **å¤„ç†åˆ†å¸ƒå¼åœºæ™¯**
   - Tensor Parallelï¼šè·¨ GPU åè°ƒ
   - Pipeline Parallelï¼šè·¨ stage ä¼ è¾“

å‚è€ƒå®ç°ï¼š`lmcache/integration/vllm/` å’Œ `lmcache/integration/sglang/`

---

## æ ¸å¿ƒæ¶æ„è§£æ

### 4.1 ç³»ç»Ÿæ¶æ„å›¾

```
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  LMCache Controller (ç®¡ç† API)         â”‚
                   â”‚  - Lookup / Clear / Compress / Move   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                         â”‚                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Worker 1     â”‚        â”‚  Worker 2       â”‚      â”‚  Worker N       â”‚
â”‚               â”‚        â”‚                 â”‚      â”‚                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ GPU Mem â”‚  â”‚        â”‚  â”‚ GPU Mem â”‚    â”‚      â”‚  â”‚ GPU Mem â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â”‚        â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â”‚      â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â”‚
â”‚       â”‚       â”‚        â”‚       â”‚         â”‚      â”‚       â”‚         â”‚
â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”‚        â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”‚      â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ CPU Mem â”‚  â”‚        â”‚  â”‚ CPU Mem â”‚    â”‚      â”‚  â”‚ CPU Mem â”‚    â”‚
â”‚  â”‚ (pinned)â”‚  â”‚        â”‚  â”‚ (pinned)â”‚    â”‚      â”‚  â”‚ (pinned)â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â”‚        â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â”‚      â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                         â”‚                         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                   â”‚                   â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Local Disk     â”‚  â”‚ Redis/KV Storeâ”‚  â”‚ Remote Storage â”‚
     â”‚  (NVMe, GDS)    â”‚  â”‚               â”‚  â”‚ (Mooncake, etc)â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 æ ¸å¿ƒç»„ä»¶è¯¦è§£

#### 1. Cache Indexï¼ˆToken Databaseï¼‰

**åŠŸèƒ½**ï¼šç»´æŠ¤ Token åºåˆ—åˆ° KV Cache çš„æ˜ å°„

```python
# æ ¸å¿ƒæ•°æ®ç»“æ„
{
    "token_hash_123": {
        "tokens": [1, 2, 3, ...],
        "location": "cpu",  # or "disk", "remote"
        "chunk_id": "chunk_abc",
        "metadata": {...}
    }
}
```

**å“ˆå¸Œç­–ç•¥**ï¼š
- é»˜è®¤æŒ‰ 256 tokens åˆ†å—
- ä½¿ç”¨å¿«é€Ÿå“ˆå¸Œç®—æ³•ï¼ˆéåŠ å¯†å“ˆå¸Œï¼‰
- æ”¯æŒå‰ç¼€æ ‘ï¼ˆTrieï¼‰ä¼˜åŒ–æŸ¥æ‰¾

#### 2. Multi-tier Storage Backend

**GPU Memory**
- å­˜å‚¨ï¼šæ´»è·ƒçš„ KV Cache
- å®¹é‡ï¼šå—é™äº GPU æ˜¾å­˜
- è®¿é—®ï¼šæœ€å¿«ï¼Œç›´æ¥å‚ä¸è®¡ç®—

**CPU DRAM**
- å­˜å‚¨ï¼šçƒ­ç¼“å­˜ï¼Œæœ€è¿‘ä½¿ç”¨çš„ KV
- å®¹é‡ï¼šæ•°ç™¾ GB
- æŠ€æœ¯ï¼š
  - Pinned Memoryï¼ˆé”é¡µå†…å­˜ï¼‰
  - NUMA-aware Allocation
  - LRU é©±é€ç­–ç•¥

**Local Storage**
- å­˜å‚¨ï¼šé•¿æœŸç¼“å­˜ã€å¤§æ–‡æ¡£
- å®¹é‡ï¼šæ•° TB
- æ”¯æŒï¼š
  - æ™®é€šæ–‡ä»¶ç³»ç»Ÿï¼ˆfile://ï¼‰
  - NVMe + GDSï¼ˆGPU Direct Storageï¼‰
  - å‹ç¼©å­˜å‚¨

**Remote Storage**
- å­˜å‚¨ï¼šæŒä¹…åŒ–ã€è·¨å®ä¾‹å…±äº«
- æ”¯æŒåç«¯ï¼š
  - Redisï¼ˆredis://ï¼‰
  - Mooncake
  - InfiniStore
  - S3-compatible
  - è‡ªå®šä¹‰ Connector

#### 3. Async Pipelineï¼ˆå¼‚æ­¥æµæ°´çº¿ï¼‰

```python
# ç®€åŒ–çš„å¼‚æ­¥æµç¨‹
async def offload_pipeline():
    # 1. GPU â†’ CPU (sync, fast)
    cpu_buffer = await gpu_to_cpu_async(kv_cache)
    
    # 2. CPU â†’ Disk (async, non-blocking)
    asyncio.create_task(cpu_to_disk_async(cpu_buffer))
    
    # 3. Disk â†’ Remote (async, optional)
    asyncio.create_task(disk_to_remote_async(key))

async def prefetch_pipeline():
    # 1. Remote â†’ Disk (prefetch, async)
    await remote_to_disk_async(key)
    
    # 2. Disk â†’ CPU (load, async)
    cpu_buffer = await disk_to_cpu_async(key)
    
    # 3. CPU â†’ GPU (when needed, sync)
    kv_cache = await cpu_to_gpu_async(cpu_buffer)
```

**å…³é”®ä¼˜åŒ–**ï¼š
- **åŒç¼“å†²**ï¼šè¯»å†™åˆ†ç¦»
- **æ‰¹å¤„ç†**ï¼šåˆå¹¶å°è¯·æ±‚
- **é¢„å–**ï¼šé¢„æµ‹æ€§åŠ è½½

#### 4. Compression Moduleï¼ˆCacheGenï¼‰

**ç®—æœ¯ç¼–ç å‹ç¼©**ï¼š

```
åŸå§‹ KV Cache (FP16)
    â†“
é‡åŒ– (INT8/INT4)
    â†“
CDF è®¡ç®—
    â†“
ç®—æœ¯ç¼–ç 
    â†“
å‹ç¼©æ•°æ® (2-4x å‹ç¼©æ¯”)
```

**ç‰¹ç‚¹**ï¼š
- æ— æŸ/æœ‰æŸå¯é…ç½®
- æµå¼å‹ç¼©/è§£å‹
- CUDA åŠ é€Ÿ
- å‹ç¼©æ¯”ï¼š2-4x

#### 5. P2P Sharingï¼ˆç‚¹å¯¹ç‚¹å…±äº«ï¼‰

**åœºæ™¯**ï¼šå¤šä¸ªå®ä¾‹é—´å…±äº« KV Cache

```
Instance A (Prefill)          Instance B (Decode)
     â”‚                              â”‚
     â”‚  1. Compute KV Cache         â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
     â”‚  2. Send via NIXL/TCP        â”‚
     â”‚                              â”‚
     â”‚                          3. Reuse KV
     â”‚                              â”‚
```

**æŠ€æœ¯æ ˆ**ï¼š
- **NIXL**ï¼šé«˜æ€§èƒ½ç½‘ç»œä¼ è¾“åº“
- **TCP/RDMA**ï¼šå¯é€‰ä¼ è¾“åè®®
- **P2P Discovery**ï¼šå®ä¾‹å‘ç°æœºåˆ¶

#### 6. Disaggregated Prefill

**æ¶æ„**ï¼š

```
       Prefill Cluster            Decode Cluster
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  GPU 1, 2, ..., Nâ”‚        â”‚ GPU 1', 2', ...â”‚
    â”‚                  â”‚        â”‚                â”‚
    â”‚  - Prefill       â”‚â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ - Decode       â”‚
    â”‚  - KV Generation â”‚  KV    â”‚ - Generation   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  Cache  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä¼˜åŠ¿**ï¼š
- **ä¸“ä¸šåŒ–**ï¼šPrefill å’Œ Decode ç‹¬ç«‹ä¼˜åŒ–
- **å¼¹æ€§ä¼¸ç¼©**ï¼šæ ¹æ®è´Ÿè½½åŠ¨æ€è°ƒæ•´
- **æˆæœ¬ä¼˜åŒ–**ï¼šä¸åŒç®—åŠ›éœ€æ±‚é…ç½®ä¸åŒç¡¬ä»¶

### 4.3 ä¸¤ç§å·¥ä½œæ¨¡å¼

#### Mode 1: Storage Modeï¼ˆå­˜å‚¨æ¨¡å¼ï¼‰

**ç”¨é€”**ï¼šKV Cache æŒä¹…åŒ–å’Œå¤ç”¨

**æµç¨‹**ï¼š
1. é¦–æ¬¡è¯·æ±‚ï¼šè®¡ç®— KV Cache â†’ å­˜å‚¨åˆ°å„çº§å­˜å‚¨
2. åç»­è¯·æ±‚ï¼šæŸ¥è¯¢ Cache Index â†’ å‘½ä¸­åˆ™å¤ç”¨
3. ç¼“å­˜ç®¡ç†ï¼šLRU é©±é€ã€å®šæœŸå‹ç¼©

**å…¸å‹åœºæ™¯**ï¼š
- å¤šè½®å¯¹è¯ï¼ˆSystem Prompt å¤ç”¨ï¼‰
- RAGï¼ˆæ–‡æ¡£ç¼“å­˜ï¼‰
- é•¿æ–‡æ¡£ QA

#### Mode 2: Transport Modeï¼ˆä¼ è¾“æ¨¡å¼ï¼‰

**ç”¨é€”**ï¼šDisaggregated Prefill

**æµç¨‹**ï¼š
1. Prefill Instanceï¼šè®¡ç®— KV Cache
2. ä¼ è¾“ï¼šé€šè¿‡ NIXL/TCP å‘é€åˆ° Decode Instance
3. Decode Instanceï¼šç›´æ¥ä½¿ç”¨ KV Cache ç”Ÿæˆ

**å…¸å‹åœºæ™¯**ï¼š
- é«˜ååæ¨ç†æœåŠ¡
- Prefill-Decode åˆ†ç¦»éƒ¨ç½²
- å¼¹æ€§ä¼¸ç¼©åœºæ™¯

---

## äºŒæ¬¡å¼€å‘æŒ‡å—

### 5.1 å¼€å‘ç¯å¢ƒæ­å»º

#### ä»æºç å®‰è£…

```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/LMCache/LMCache.git
cd LMCache

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
uv venv --python 3.12
source .venv/bin/activate

# 3. å®‰è£…æ„å»ºä¾èµ–
uv pip install -r requirements/build.txt

# 4. å®‰è£… PyTorchï¼ˆåŒ¹é…æ¨ç†å¼•æ“ç‰ˆæœ¬ï¼‰
uv pip install torch==2.7.1  # ç¤ºä¾‹ï¼švLLM 0.10.0 ä½¿ç”¨æ­¤ç‰ˆæœ¬

# 5. å®‰è£… LMCacheï¼ˆå¼€å‘æ¨¡å¼ï¼‰
uv pip install -e . --no-build-isolation

# 6. éªŒè¯å®‰è£…
python3 -c "import lmcache.c_ops"  # æµ‹è¯• CUDA æ‰©å±•
```

#### é¡¹ç›®ç»“æ„

```
LMCache/
â”œâ”€â”€ lmcache/                 # Python ä¸»åŒ…
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py           # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ cache_engine.py     # ç¼“å­˜å¼•æ“æ ¸å¿ƒ
â”‚   â”œâ”€â”€ integration/        # æ¡†æ¶é›†æˆ
â”‚   â”‚   â”œâ”€â”€ vllm/          # vLLM Connector
â”‚   â”‚   â””â”€â”€ sglang/        # SGLang Adapter
â”‚   â”œâ”€â”€ storage_backend/    # å­˜å‚¨åç«¯
â”‚   â”‚   â”œâ”€â”€ connector/     # è¿œç¨‹è¿æ¥å™¨
â”‚   â”‚   â”œâ”€â”€ evictor/       # é©±é€ç­–ç•¥
â”‚   â”‚   â””â”€â”€ serde/         # åºåˆ—åŒ–
â”‚   â””â”€â”€ v1/                # æ–°ç‰ˆ API
â”‚       â”œâ”€â”€ cache_engine.py
â”‚       â”œâ”€â”€ storage_backend/
â”‚       â””â”€â”€ internal_api_server/
â”‚
â”œâ”€â”€ csrc/                    # C++/CUDA æºç 
â”‚   â”œâ”€â”€ ac_enc.cu           # ç®—æœ¯ç¼–ç 
â”‚   â”œâ”€â”€ ac_dec.cu           # ç®—æœ¯è§£ç 
â”‚   â”œâ”€â”€ mem_kernels.cu      # å†…å­˜æ ¸å‡½æ•°
â”‚   â”œâ”€â”€ pos_kernels.cu      # ä½ç½®ç¼–ç 
â”‚   â””â”€â”€ *.h/*.cpp           # å¤´æ–‡ä»¶å’Œè¾…åŠ©
â”‚
â”œâ”€â”€ examples/                # ç¤ºä¾‹ä»£ç 
â”‚   â”œâ”€â”€ basic_check/
â”‚   â”œâ”€â”€ cache_controller/
â”‚   â”œâ”€â”€ disagg_prefill/
â”‚   â””â”€â”€ kv_cache_reuse/
â”‚
â”œâ”€â”€ tests/                   # æµ‹è¯•
â”‚   â”œâ”€â”€ v1/
â”‚   â””â”€â”€ benchmarks/
â”‚
â”œâ”€â”€ docs/                    # æ–‡æ¡£ï¼ˆSphinxï¼‰
â”œâ”€â”€ benchmarks/              # æ€§èƒ½æµ‹è¯•
â””â”€â”€ setup.py                 # æ„å»ºè„šæœ¬
```

### 5.2 æ‰©å±•å­˜å‚¨åç«¯

#### æ­¥éª¤ 1ï¼šå®ç° Connector æ¥å£

```python
# lmcache/storage_backend/connector/my_storage_connector.py

from lmcache.storage_backend.connector.base_connector import RemoteBytesConnector
from typing import Optional, List

class MyStorageConnector(RemoteBytesConnector):
    """è‡ªå®šä¹‰å­˜å‚¨åç«¯è¿æ¥å™¨"""
    
    def __init__(self, host: str, port: int, **kwargs):
        self.host = host
        self.port = port
        # åˆå§‹åŒ–ä½ çš„å®¢æˆ·ç«¯
        self.client = MyStorageClient(host, port)
    
    def exists(self, key: str) -> bool:
        """æ£€æŸ¥ key æ˜¯å¦å­˜åœ¨"""
        return self.client.has(key)
    
    def set(self, key: str, obj: bytes) -> None:
        """å­˜å‚¨æ•°æ®"""
        self.client.put(key, obj)
    
    def get(self, key: str) -> Optional[bytes]:
        """è·å–æ•°æ®"""
        try:
            return self.client.get(key)
        except KeyError:
            return None
    
    def list(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰ key"""
        return self.client.keys()
    
    def close(self) -> None:
        """å…³é—­è¿æ¥"""
        self.client.disconnect()
```

#### æ­¥éª¤ 2ï¼šæ³¨å†Œ Connector

```python
# lmcache/storage_backend/connector/__init__.py

from .my_storage_connector import MyStorageConnector

CONNECTOR_REGISTRY = {
    "redis": RedisConnector,
    "lmcache_server": LMCServerConnector,
    "my_storage": MyStorageConnector,  # æ³¨å†Œä½ çš„ Connector
}

def get_connector(backend_type: str, **kwargs):
    connector_cls = CONNECTOR_REGISTRY.get(backend_type)
    if not connector_cls:
        raise ValueError(f"Unknown backend: {backend_type}")
    return connector_cls(**kwargs)
```

#### æ­¥éª¤ 3ï¼šé…ç½®ä½¿ç”¨

```yaml
# my_config.yaml
chunk_size: 256
local_device: "cpu"
max_local_cache_size: 10
remote_url: "mystorage://myhost:9000"
remote_serde: "torch"
```

```python
# ä½¿ç”¨è‡ªå®šä¹‰å­˜å‚¨
from lmcache import LMCacheEngine
from lmcache.config import LMCacheEngineConfig

config = LMCacheEngineConfig.from_file("my_config.yaml")
cache_engine = LMCacheEngine(config, metadata)
```

### 5.3 è‡ªå®šä¹‰é©±é€ç­–ç•¥

#### å®ç° Evictor

```python
# lmcache/storage_backend/evictor/lfu_evictor.py

from lmcache.storage_backend.evictor.base_evictor import Evictor
from collections import defaultdict
import heapq

class LFUEvictor(Evictor):
    """LFUï¼ˆLeast Frequently Usedï¼‰é©±é€ç­–ç•¥"""
    
    def __init__(self):
        self.freq = defaultdict(int)  # key -> è®¿é—®é¢‘ç‡
        self.time = defaultdict(int)  # key -> æœ€åè®¿é—®æ—¶é—´
        self.clock = 0
    
    def update(self, key: str) -> None:
        """æ›´æ–°è®¿é—®ä¿¡æ¯"""
        self.freq[key] += 1
        self.time[key] = self.clock
        self.clock += 1
    
    def evict(self) -> str:
        """é€‰æ‹©è¦é©±é€çš„ key"""
        if not self.freq:
            raise ValueError("No keys to evict")
        
        # é€‰æ‹©é¢‘ç‡æœ€ä½çš„ï¼Œå¦‚æœé¢‘ç‡ç›¸åŒåˆ™é€‰æ‹©æœ€æ—§çš„
        min_key = min(
            self.freq.keys(),
            key=lambda k: (self.freq[k], self.time[k])
        )
        
        del self.freq[min_key]
        del self.time[min_key]
        return min_key
```

### 5.4 é›†æˆæ–°çš„æ¨ç†æ¡†æ¶

#### æ­¥éª¤ 1ï¼šå®ç° Adapter

```python
# lmcache/integration/myframework/myframework_adapter.py

class MyFrameworkLMCacheAdapter:
    """MyFramework çš„ LMCache é€‚é…å™¨"""
    
    def __init__(self, config):
        self.cache_engine = LMCacheEngine(config)
        self.chunk_size = config.chunk_size
    
    def prefill_hook(self, token_ids, kv_cache):
        """Prefill é˜¶æ®µçš„ Hook"""
        # 1. æŸ¥è¯¢ç¼“å­˜
        cached_kv, hit_tokens = self.cache_engine.retrieve(token_ids)
        
        if cached_kv is not None:
            # ç¼“å­˜å‘½ä¸­ï¼Œè¿”å›å·²æœ‰çš„ KV Cache
            return cached_kv, hit_tokens
        
        # 2. ç¼“å­˜æœªå‘½ä¸­ï¼Œè¿”å› None è®©æ¡†æ¶è®¡ç®—
        return None, 0
    
    def post_prefill_hook(self, token_ids, kv_cache):
        """Prefill åçš„ Hookï¼Œå­˜å‚¨æ–°ç”Ÿæˆçš„ KV Cache"""
        # å¼‚æ­¥å­˜å‚¨åˆ° LMCache
        self.cache_engine.store(token_ids, kv_cache)
    
    def decode_hook(self, token_ids, kv_cache):
        """Decode é˜¶æ®µçš„ Hookï¼ˆå¯é€‰ï¼‰"""
        if self.config.save_decode_cache:
            self.cache_engine.store(token_ids, kv_cache)
```

#### æ­¥éª¤ 2ï¼šä¿®æ”¹æ¨ç†å¼•æ“

```python
# åœ¨æ¨ç†å¼•æ“çš„ Prefill é˜¶æ®µæ’å…¥
class MyFrameworkEngine:
    def __init__(self, model, lmcache_config=None):
        self.model = model
        self.lmcache = None
        
        if lmcache_config:
            self.lmcache = MyFrameworkLMCacheAdapter(lmcache_config)
    
    def prefill(self, token_ids):
        # 1. å°è¯•ä» LMCache è·å–
        if self.lmcache:
            cached_kv, hit_tokens = self.lmcache.prefill_hook(token_ids)
            if cached_kv is not None:
                return cached_kv  # ç¼“å­˜å‘½ä¸­ï¼Œç›´æ¥è¿”å›
        
        # 2. ç¼“å­˜æœªå‘½ä¸­ï¼Œæ­£å¸¸è®¡ç®—
        kv_cache = self.model.forward_prefill(token_ids)
        
        # 3. å­˜å‚¨åˆ° LMCache
        if self.lmcache:
            self.lmcache.post_prefill_hook(token_ids, kv_cache)
        
        return kv_cache
```

### 5.5 æ·»åŠ  CUDA Kernel

#### ç¤ºä¾‹ï¼šè‡ªå®šä¹‰å†…å­˜æ‹·è´ Kernel

```cuda
// csrc/my_kernel.cu

#include <cuda_runtime.h>
#include <torch/extension.h>

__global__ void my_copy_kernel(
    const float* src,
    float* dst,
    int size
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        dst[idx] = src[idx];
    }
}

torch::Tensor my_copy_cuda(torch::Tensor src) {
    auto dst = torch::empty_like(src);
    
    int size = src.numel();
    int threads = 256;
    int blocks = (size + threads - 1) / threads;
    
    my_copy_kernel<<<blocks, threads>>>(
        src.data_ptr<float>(),
        dst.data_ptr<float>(),
        size
    );
    
    return dst;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("my_copy", &my_copy_cuda, "My custom copy (CUDA)");
}
```

#### æ·»åŠ åˆ° setup.py

```python
# setup.py
cuda_sources = [
    "csrc/pybind.cpp",
    "csrc/mem_kernels.cu",
    # ... å…¶ä»–æ–‡ä»¶
    "csrc/my_kernel.cu",  # æ·»åŠ ä½ çš„ kernel
]
```

### 5.6 æ€§èƒ½åˆ†æå’Œè°ƒè¯•

#### å¯ç”¨ NVTX æ ‡æ³¨

```python
# LMCache å†…ç½® NVTX æ”¯æŒ
from lmcache.utils import _lmcache_nvtx_annotate

@_lmcache_nvtx_annotate
def my_function():
    # ä½ çš„ä»£ç 
    pass
```

#### ä½¿ç”¨ Nsight Systems åˆ†æ

```bash
# ä½¿ç”¨ Nsight Systems åˆ†ææ€§èƒ½
nsys profile -o lmcache_profile python my_script.py

# æŸ¥çœ‹ç»“æœ
nsys-ui lmcache_profile.nsys-rep
```

#### è°ƒè¯•é…ç½®

```python
# å¯ç”¨è°ƒè¯•æ¨¡å¼
from lmcache.config import GlobalConfig
GlobalConfig.set_debug(True)

# æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)
```

### 5.7 æµ‹è¯•ä½ çš„æ‰©å±•

```python
# tests/test_my_extension.py

import pytest
import torch
from lmcache.storage_backend.connector import get_connector

def test_my_storage_connector():
    """æµ‹è¯•è‡ªå®šä¹‰å­˜å‚¨ Connector"""
    connector = get_connector(
        "my_storage",
        host="localhost",
        port=9000
    )
    
    # æµ‹è¯•å­˜å‚¨å’Œè¯»å–
    test_data = b"hello lmcache"
    connector.set("test_key", test_data)
    
    assert connector.exists("test_key")
    retrieved = connector.get("test_key")
    assert retrieved == test_data
    
    connector.close()

def test_integration_with_vllm():
    """æµ‹è¯•ä¸ vLLM çš„é›†æˆ"""
    from vllm import LLM, SamplingParams
    
    llm = LLM(
        model="facebook/opt-125m",
        kv_transfer_config={
            "kv_connector": "LMCacheConnector",
            "lmcache_config_file": "my_config.yaml"
        }
    )
    
    # é¦–æ¬¡æ¨ç†
    output1 = llm.generate("Hello, how are you?")
    
    # ç¬¬äºŒæ¬¡æ¨ç†ï¼Œåº”è¯¥å‘½ä¸­ç¼“å­˜
    output2 = llm.generate("Hello, how are you?")
    
    # éªŒè¯è¾“å‡ºä¸€è‡´
    assert output1 == output2
```

---

## æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹å›é¡¾

#### 1. LMCache æ˜¯ä»€ä¹ˆ
- **LLM æ¨ç†åŠ é€Ÿå¼•æ“æ‰©å±•**ï¼Œé€šè¿‡ KV Cache å¤ç”¨å‡å°‘ TTFT å’Œæå‡ååé‡
- æ”¯æŒ**ä»»æ„æ–‡æœ¬ç‰‡æ®µ**çš„ KV Cache å¤ç”¨ï¼Œä¸ä»…é™äºå‰ç¼€
- åœ¨å¤šè½®å¯¹è¯ã€RAG ç­‰åœºæ™¯ä¸‹å¯å®ç° **3-10x æ€§èƒ½æå‡**

#### 2. å·¥ç¨‹ vs ç®—æ³•
- **ä¸»è¦æ˜¯å·¥ç¨‹é¡¹ç›®**ï¼šå¤šå±‚çº§å­˜å‚¨ã€å¼‚æ­¥æµæ°´çº¿ã€åˆ†å¸ƒå¼åè°ƒ
- **èåˆç®—æ³•ä¼˜åŒ–**ï¼šCacheGen å‹ç¼©ã€CacheBlend èåˆ
- **ä¸ GPU æ·±åº¦ç›¸å…³**ï¼šæ”¯æŒ NVIDIAï¼ˆCUDAï¼‰å’Œ AMDï¼ˆROCmï¼‰
- **å¤šçº§å­˜å‚¨è®¾è®¡**ï¼šGPU â†’ CPU â†’ Disk â†’ Remoteï¼Œéœ€è¦åŒºåˆ†è®¾å¤‡

#### 3. æ¡†æ¶é€‚é…
- **éå¼€ç®±å³ç”¨**ï¼šéœ€è¦é€šè¿‡ Connector é›†æˆ
- **å®˜æ–¹æ”¯æŒ**ï¼švLLMï¼ˆv1 ä¸»æ¨ï¼‰ã€SGLang
- **ä¼ä¸šçº§æ”¯æŒ**ï¼švLLM Production Stackã€llm-dã€KServe
- **æ‰©å±•æ€§å¼º**ï¼šå¯ä»¥é€šè¿‡å®ç° Connector æ¥å£æ”¯æŒæ–°æ¡†æ¶

#### 4. äºŒæ¬¡å¼€å‘æ–¹å‘

**å­˜å‚¨å±‚æ‰©å±•**ï¼š
- è‡ªå®šä¹‰å­˜å‚¨åç«¯ï¼ˆå®ç° RemoteBytesConnectorï¼‰
- æ–°çš„é©±é€ç­–ç•¥ï¼ˆç»§æ‰¿ Evictorï¼‰
- ä¼˜åŒ–åºåˆ—åŒ–/ååºåˆ—åŒ–ï¼ˆserde æ¨¡å—ï¼‰

**æ¨ç†å¼•æ“é›†æˆ**ï¼š
- å®ç° Adapter ç±»
- Hook Prefill/Decode é˜¶æ®µ
- å¤„ç†åˆ†å¸ƒå¼åœºæ™¯

**æ€§èƒ½ä¼˜åŒ–**ï¼š
- æ·»åŠ  CUDA Kernel
- ä¼˜åŒ–å¼‚æ­¥æµæ°´çº¿
- æ”¹è¿›ç¼“å­˜ç­–ç•¥

**åŠŸèƒ½å¢å¼º**ï¼š
- æ–°çš„å‹ç¼©ç®—æ³•
- æ™ºèƒ½é¢„å–ç­–ç•¥
- å¯è§‚æµ‹æ€§å¢å¼º

### æ¨èå­¦ä¹ è·¯å¾„

1. **åŸºç¡€ç†è§£**ï¼ˆ1-2 å¤©ï¼‰
   - é˜…è¯» README å’Œæ–‡æ¡£
   - è¿è¡Œ `examples/` ä¸­çš„ç¤ºä¾‹
   - ç†è§£ KV Cache åŸç†

2. **æ·±å…¥æ¶æ„**ï¼ˆ3-5 å¤©ï¼‰
   - é˜…è¯»æ ¸å¿ƒä»£ç ï¼š`lmcache/cache_engine.py`
   - ç†è§£å­˜å‚¨åç«¯ï¼š`lmcache/storage_backend/`
   - ç ”ç©¶ vLLM é›†æˆï¼š`lmcache/integration/vllm/`

3. **åŠ¨æ‰‹å®è·µ**ï¼ˆ1-2 å‘¨ï¼‰
   - å®ç°ä¸€ä¸ªç®€å•çš„å­˜å‚¨ Connector
   - å°è¯•ä¿®æ”¹é©±é€ç­–ç•¥
   - è¿è¡Œ benchmarks å¹¶åˆ†ææ€§èƒ½

4. **é«˜çº§å¼€å‘**ï¼ˆæŒç»­ï¼‰
   - å‚ä¸ç¤¾åŒºè®¨è®ºï¼ˆSlackã€GitHubï¼‰
   - è´¡çŒ®ä»£ç å’Œæ–‡æ¡£
   - æ¢ç´¢å‰æ²¿ä¼˜åŒ–æ–¹å‘

### å‚è€ƒèµ„æº

#### å®˜æ–¹èµ„æº
- **æ–‡æ¡£**ï¼šhttps://docs.lmcache.ai/
- **åšå®¢**ï¼šhttps://blog.lmcache.ai/
- **Slack**ï¼šhttps://join.slack.com/t/lmcacheworkspace/...
- **GitHub**ï¼šhttps://github.com/LMCache/LMCache

#### å­¦æœ¯è®ºæ–‡
1. CacheGen (SIGCOMM 2024)
2. CacheBlend (EuroSys 2025)
3. LLM Content Delivery Network (arXiv 2024)
4. LMCache Architecture (arXiv 2025)

#### ç›¸å…³é¡¹ç›®
- **vLLM**ï¼šhttps://github.com/vllm-project/vllm
- **SGLang**ï¼šhttps://github.com/sgl-project/sglang
- **NIXL**ï¼šhttps://github.com/ai-dynamo/nixl

---


