# LMCache å®ç°ç»†èŠ‚ä¸æ•°æ®æµè½¬å®Œå…¨è§£æ

## ç›®å½•
1. [æ•´ä½“æ¶æ„å›é¡¾](#æ•´ä½“æ¶æ„å›é¡¾)
2. [æ ¸å¿ƒå®ç°ç»†èŠ‚](#æ ¸å¿ƒå®ç°ç»†èŠ‚)
3. [æ•°æ®æµè½¬å…¨è¿‡ç¨‹](#æ•°æ®æµè½¬å…¨è¿‡ç¨‹)
4. [ä¸Šå±‚ä½¿ç”¨æ–¹å¼](#ä¸Šå±‚ä½¿ç”¨æ–¹å¼)
5. [å®Œæ•´ç¤ºä¾‹](#å®Œæ•´ç¤ºä¾‹)
6. [æ€§èƒ½åˆ†æ](#æ€§èƒ½åˆ†æ)

---

## æ•´ä½“æ¶æ„å›é¡¾

### ç³»ç»Ÿåˆ†å±‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ä¸Šå±‚ï¼šæ¨ç†æ¡†æ¶                             â”‚
â”‚            (vLLM / SGLang / å…¶ä»– LLM Engine)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                    é›†æˆå±‚ (Connector)
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  LMCache Core                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Cache Engineâ”‚  â”‚ GPU Connectorâ”‚  â”‚Storage Managerâ”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Storage Backends                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Local CPU  â”‚ Local Disk   â”‚ Remote KV  â”‚ P2P Backend  â”‚ â”‚
â”‚  â”‚ Backend    â”‚ Backend      â”‚ Store      â”‚              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## æ ¸å¿ƒå®ç°ç»†èŠ‚

### 1. Cache Engineï¼ˆæ ¸å¿ƒå¼•æ“ï¼‰

**æ–‡ä»¶ä½ç½®**ï¼š`lmcache/v1/cache_engine.py`

#### æ ¸å¿ƒæ•°æ®ç»“æ„

```python
class LMCacheEngine:
    """LMCache çš„æ ¸å¿ƒå¼•æ“"""
    
    def __init__(self, config: LMCacheEngineConfig, metadata: LMCacheEngineMetadata):
        # 1. é…ç½®
        self.config = config
        self.metadata = metadata
        
        # 2. GPU Connectorï¼šå¤„ç† GPU å†…å­˜æ“ä½œ
        self.gpu_connector = self._create_gpu_connector()
        
        # 3. Storage Managerï¼šç®¡ç†å¤šçº§å­˜å‚¨
        self.storage_manager = StorageManager(config, metadata, self.loop)
        
        # 4. ç¼“å­˜ç´¢å¼•ï¼šToken Hash â†’ CacheEngineKey
        self.cache_index = {}  # å†…å­˜ä¸­çš„å“ˆå¸Œè¡¨
        
        # 5. Lookup Pinsï¼šç”¨äº P2P ä¼ è¾“æ—¶çš„ä¸´æ—¶å›ºå®š
        self.lookup_pins = {}  # lookup_id â†’ [keys]
        
        # 6. äº‹ä»¶å¾ªç¯ï¼šç”¨äºå¼‚æ­¥æ“ä½œ
        self.loop = asyncio.new_event_loop()
        self.worker_thread = threading.Thread(target=self._run_loop, daemon=True)
        self.worker_thread.start()
```

#### å…³é”®æ–¹æ³•

##### **1. store() - å­˜å‚¨ KV Cache**

```python
def store(
    self,
    tokens: Union[torch.Tensor, List[int]],
    kv_caches,  # vLLM/SGLang çš„ KV Cache
    skip_query: bool = False
):
    """
    å°† KV Cache å­˜å‚¨åˆ° LMCache
    
    æµç¨‹ï¼š
    1. Token åºåˆ— â†’ å“ˆå¸Œï¼ˆåˆ†å—ï¼‰
    2. GPU Paged Memory â†’ CPU Contiguous Memory
    3. CPU â†’ Local Storage (CPU/Disk)
    4. Local â†’ Remote Storage (å¼‚æ­¥)
    """
    
    # Step 1: å¯¹ tokens è¿›è¡Œå“ˆå¸Œå’Œåˆ†å—
    chunk_size = self.config.chunk_size  # é»˜è®¤ 256
    token_chunks = self._chunk_tokens(tokens, chunk_size)
    
    for chunk_tokens in token_chunks:
        # è®¡ç®—å“ˆå¸Œ
        chunk_hash = self._compute_hash(chunk_tokens)
        
        # Step 2: æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        key = CacheEngineKey(
            fmt=self.metadata.fmt,
            model=self.metadata.model_name,
            worker_id=self.metadata.worker_id,
            chunk_hash=chunk_hash
        )
        
        if not skip_query and self.storage_manager.contains(key):
            continue  # å·²å­˜åœ¨ï¼Œè·³è¿‡
        
        # Step 3: ä» GPU æå–åˆ° CPU
        memory_obj = self._extract_from_gpu(kv_caches, chunk_tokens)
        
        # Step 4: æäº¤åˆ° Storage Managerï¼ˆå¼‚æ­¥å­˜å‚¨ï¼‰
        self.storage_manager.submit_put_task(key, memory_obj)
```

**å…³é”®ç‚¹**ï¼š
- Token åºåˆ—æŒ‰ `chunk_size`ï¼ˆé»˜è®¤ 256ï¼‰åˆ†å—
- æ¯ä¸ªå—è®¡ç®—å“ˆå¸Œä½œä¸º key
- GPU â†’ CPU åŒæ­¥ï¼ˆå¿«ï¼‰
- CPU â†’ Disk/Remote å¼‚æ­¥ï¼ˆä¸é˜»å¡ï¼‰

##### **2. retrieve() - æ£€ç´¢ KV Cache**

```python
def retrieve(
    self,
    tokens: Union[torch.Tensor, List[int]],
    kv_caches  # ç›®æ ‡ï¼švLLM/SGLang çš„ KV Cacheï¼ˆå¾…å¡«å……ï¼‰
) -> int:
    """
    ä» LMCache æ£€ç´¢ KV Cache
    
    è¿”å›ï¼šå‘½ä¸­çš„ token æ•°é‡
    """
    
    # Step 1: Token åºåˆ—åˆ†å—å’Œå“ˆå¸Œ
    chunk_size = self.config.chunk_size
    token_chunks = self._chunk_tokens(tokens, chunk_size)
    
    hit_tokens = 0
    
    for i, chunk_tokens in enumerate(token_chunks):
        chunk_hash = self._compute_hash(chunk_tokens)
        key = CacheEngineKey(...)
        
        # Step 2: ä» Storage Manager è·å– MemoryObj
        memory_obj = self.storage_manager.get(key)
        
        if memory_obj is None:
            break  # æœªå‘½ä¸­ï¼Œåœæ­¢ï¼ˆå‰ç¼€åŒ¹é…ï¼‰
        
        # Step 3: ä» CPU åŠ è½½åˆ° GPU
        self._load_to_gpu(memory_obj, kv_caches, chunk_tokens)
        
        hit_tokens += len(chunk_tokens)
    
    return hit_tokens
```

**å…³é”®ç‚¹**ï¼š
- å‰ç¼€åŒ¹é…ï¼šä¸€æ—¦æŸä¸ªå—æœªå‘½ä¸­ï¼Œåç»­å—ä¹Ÿä¸æ£€ç´¢
- CPU â†’ GPU åŒæ­¥åŠ è½½
- æ”¯æŒéƒ¨åˆ†å‘½ä¸­ï¼ˆå‰ N ä¸ªå—å‘½ä¸­ï¼‰

---

### 2. GPU Connectorï¼ˆGPU æ¥å£ï¼‰

**æ–‡ä»¶ä½ç½®**ï¼š`lmcache/v1/gpu_connector.py`

#### ä½œç”¨

GPU Connector è´Ÿè´£ï¼š
1. **ä» GPU æå–**ï¼švLLM Paged Memory â†’ LMCache Contiguous Memory
2. **åŠ è½½åˆ° GPU**ï¼šLMCache Contiguous Memory â†’ vLLM Paged Memory

#### ä¸ºä»€ä¹ˆéœ€è¦ï¼Ÿ

vLLM çš„ KV Cache æ˜¯ **åˆ†é¡µå­˜å‚¨**ï¼ˆPagedAttentionï¼‰ï¼š

```
vLLM Paged Memoryï¼ˆä¸è¿ç»­ï¼‰ï¼š
Page 0: [Token 0, Token 1, Token 3]
Page 1: [Token 5, Token 6]
Page 2: [Token 2, Token 4]
         â†‘ ä¹±åºã€åˆ†æ•£

éœ€è¦è½¬æ¢ä¸ºï¼š

LMCache Contiguous Memoryï¼ˆè¿ç»­ï¼‰ï¼š
[Token 0][Token 1][Token 2][Token 3][Token 4][Token 5][Token 6]
    â†‘ æœ‰åºã€è¿ç»­
```

#### æ ¸å¿ƒå®ç°

```python
class VLLMPagedMemGPUConnectorV2(GPUConnectorInterface):
    """vLLM çš„ GPU Connector"""
    
    def __init__(self, kvcaches, metadata, ...):
        self.kvcaches = kvcaches  # vLLM çš„ paged KV cache
        self.num_layers = metadata.kv_shape[0]
        self.page_buffer_size = ...
        
        # åˆ›å»ºä¸“ç”¨çš„ CUDA Stream
        self.store_stream = torch.cuda.Stream()  # ç”¨äºå­˜å‚¨ï¼ˆGPUâ†’CPUï¼‰
        self.load_stream = torch.cuda.Stream()   # ç”¨äºåŠ è½½ï¼ˆCPUâ†’GPUï¼‰
    
    def from_gpu(
        self,
        memory_obj: MemoryObj,  # ç›®æ ‡ï¼šCPU ä¸Šçš„è¿ç»­å†…å­˜
        start: int,             # èµ·å§‹ token ç´¢å¼•
        end: int,               # ç»“æŸ token ç´¢å¼•
        slot_mapping: torch.Tensor  # Token â†’ Page çš„æ˜ å°„
    ):
        """
        ä» vLLM Paged Memory æå–åˆ° LMCache Contiguous Memory
        """
        
        # è·å–æ‰€æœ‰å±‚çš„ KV cache æŒ‡é’ˆ
        kv_cache_pointers = self._initialize_pointers(self.kvcaches)
        
        # ä½¿ç”¨ä¸“ç”¨ Streamï¼ˆä¸é˜»å¡ä¸» Streamï¼‰
        with torch.cuda.stream(self.store_stream):
            # è°ƒç”¨ CUDA Kernel
            lmc_ops.multi_layer_kv_transfer(
                memory_obj.tensor,          # ç›®æ ‡ï¼ˆCPU pinned memoryï¼‰
                kv_cache_pointers,          # æºï¼ˆGPU paged memoryï¼‰
                slot_mapping[start:end],    # æ˜ å°„å…³ç³»
                self.kvcaches[0].device,    # GPU è®¾å¤‡
                self.page_buffer_size,
                True,                       # æ–¹å‘ï¼špaged â†’ contiguous
                self.use_mla
            )
        
        # å¦‚æœç›®æ ‡ä¸åœ¨ GPU ä¸Šï¼ŒåŒæ­¥ç­‰å¾…
        if not memory_obj.tensor.is_cuda:
            self.store_stream.synchronize()
    
    def to_gpu(
        self,
        memory_obj: MemoryObj,  # æºï¼šCPU ä¸Šçš„è¿ç»­å†…å­˜
        start: int,
        end: int,
        slot_mapping: torch.Tensor
    ):
        """
        ä» LMCache Contiguous Memory åŠ è½½åˆ° vLLM Paged Memory
        """
        
        kv_cache_pointers = self._initialize_pointers(self.kvcaches)
        
        with torch.cuda.stream(self.load_stream):
            lmc_ops.multi_layer_kv_transfer(
                memory_obj.tensor,
                kv_cache_pointers,
                slot_mapping[start:end],
                self.kvcaches[0].device,
                self.page_buffer_size,
                False,  # æ–¹å‘ï¼šcontiguous â†’ paged
                self.use_mla
            )
        
        self.load_stream.synchronize()
```

**å…³é”®ä¼˜åŒ–**ï¼š
- **Pinned Memory**ï¼šCPU ä¾§ä½¿ç”¨é”é¡µå†…å­˜ï¼ŒåŠ é€Ÿ GPU-CPU ä¼ è¾“
- **ä¸“ç”¨ Stream**ï¼šé¿å…é˜»å¡ä¸» Stream
- **æ‰¹é‡å¤„ç†**ï¼šä¸€æ¬¡å¤„ç†æ‰€æœ‰å±‚

---

### 3. Storage Managerï¼ˆå­˜å‚¨ç®¡ç†å™¨ï¼‰

**æ–‡ä»¶ä½ç½®**ï¼š`lmcache/v1/storage_backend/storage_manager.py`

#### æ ¸å¿ƒèŒè´£

```python
class StorageManager:
    """
    ç®¡ç†å¤šçº§å­˜å‚¨åç«¯
    
    å±‚çº§ï¼š
    1. Local CPU Backendï¼ˆçƒ­ç¼“å­˜ï¼Œå¿«ï¼‰
    2. Local Disk Backendï¼ˆå†·ç¼“å­˜ï¼Œä¸­é€Ÿï¼‰
    3. Remote Backendï¼ˆæŒä¹…åŒ–ï¼Œæ…¢ï¼‰
    4. P2P Backendï¼ˆè·¨å®ä¾‹å…±äº«ï¼‰
    """
    
    def __init__(self, config, metadata, loop):
        self.storage_backends = {}
        
        # 1. Local CPU Backendï¼ˆå¿…é¡»ï¼‰
        self.storage_backends["LocalCPUBackend"] = LocalCPUBackend(
            config, metadata, loop
        )
        
        # 2. Local Disk Backendï¼ˆå¯é€‰ï¼‰
        if config.local_device and config.local_device.startswith("/"):
            self.storage_backends["LocalDiskBackend"] = LocalDiskBackend(
                config, metadata, loop
            )
        
        # 3. Remote Backendï¼ˆå¯é€‰ï¼‰
        if config.remote_url:
            self.storage_backends["RemoteBackend"] = RemoteBackend(
                config, metadata, loop
            )
        
        # 4. P2P Backendï¼ˆå¯é€‰ï¼‰
        if config.enable_p2p:
            self.storage_backends["P2PBackend"] = P2PBackend(
                config, metadata, loop, ...
            )
        
        # å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—
        self.loop = loop
    
    def submit_put_task(self, key: CacheEngineKey, memory_obj: MemoryObj):
        """
        æäº¤å­˜å‚¨ä»»åŠ¡ï¼ˆå¼‚æ­¥ï¼‰
        
        æµç¨‹ï¼š
        1. CPU Backendï¼ˆåŒæ­¥ï¼‰
        2. Disk Backendï¼ˆå¼‚æ­¥ï¼‰â† å¦‚æœ CPU å·²æ»¡ï¼ŒLRU é©±é€åˆ° Disk
        3. Remote Backendï¼ˆå¼‚æ­¥ï¼‰
        """
        
        # æäº¤åˆ° CPU Backend
        future = asyncio.run_coroutine_threadsafe(
            self.storage_backends["LocalCPUBackend"].async_submit_put_task(
                key, memory_obj
            ),
            self.loop
        )
        
        # ä¸ç­‰å¾…å®Œæˆï¼Œç»§ç»­æ‰§è¡Œï¼ˆå¼‚æ­¥ï¼‰
    
    def get(self, key: CacheEngineKey) -> Optional[MemoryObj]:
        """
        è·å–ç¼“å­˜ï¼ˆåŒæ­¥ï¼‰
        
        æŸ¥æ‰¾é¡ºåºï¼š
        1. Local CPUï¼ˆæœ€å¿«ï¼‰
        2. Local Diskï¼ˆä¸­é€Ÿï¼‰
        3. Remoteï¼ˆæœ€æ…¢ï¼‰
        4. P2Pï¼ˆè·¨å®ä¾‹ï¼‰
        """
        
        # 1. å…ˆæŸ¥ CPU
        mem_obj = self.storage_backends["LocalCPUBackend"].get(key)
        if mem_obj:
            return mem_obj
        
        # 2. å†æŸ¥ Disk
        if "LocalDiskBackend" in self.storage_backends:
            mem_obj = self.storage_backends["LocalDiskBackend"].get(key)
            if mem_obj:
                # æå‡åˆ° CPUï¼ˆçƒ­åº¦æå‡ï¼‰
                self.storage_backends["LocalCPUBackend"].put(key, mem_obj)
                return mem_obj
        
        # 3. æœ€åæŸ¥ Remote
        if "RemoteBackend" in self.storage_backends:
            mem_obj = self.storage_backends["RemoteBackend"].get(key)
            if mem_obj:
                # æå‡åˆ° CPU
                self.storage_backends["LocalCPUBackend"].put(key, mem_obj)
                return mem_obj
        
        return None
```

---

### 4. Local CPU Backendï¼ˆæœ¬åœ° CPU åç«¯ï¼‰

**æ–‡ä»¶ä½ç½®**ï¼š`lmcache/v1/storage_backend/local_cpu_backend.py`

#### æ ¸å¿ƒå®ç°

```python
class LocalCPUBackend(StorageBackendInterface):
    """
    æœ¬åœ° CPU å†…å­˜åç«¯
    
    ç‰¹ç‚¹ï¼š
    - ä½¿ç”¨ Pinned Memoryï¼ˆé”é¡µå†…å­˜ï¼‰
    - LRU é©±é€ç­–ç•¥
    - é«˜é€Ÿè®¿é—®
    """
    
    def __init__(self, config, metadata, loop):
        # å†…å­˜åˆ†é…å™¨
        self.memory_allocator = PagedCpuGpuMemoryAllocator(
            buffer_size=config.max_local_cache_size * 1024**3,  # GB â†’ Bytes
            page_size=config.chunk_size * hidden_size * 2 * dtype_size,
            device="cpu",
            pin_memory=True  # å…³é”®ï¼ä½¿ç”¨é”é¡µå†…å­˜
        )
        
        # ç¼“å­˜ç´¢å¼•ï¼škey â†’ memory_obj
        self.cache = {}
        
        # LRU é©±é€å™¨
        self.evictor = LRUEvictor()
        
        # å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—
        self.put_queue = asyncio.Queue()
        self.loop = loop
    
    def allocate(self, shape, dtype, fmt) -> MemoryObj:
        """
        åˆ†é… CPU å†…å­˜ï¼ˆPinned Memoryï¼‰
        """
        
        # å¦‚æœå†…å­˜ä¸è¶³ï¼Œé©±é€æ—§æ¡ç›®
        while not self.memory_allocator.has_space(shape):
            evict_key = self.evictor.evict()
            self._evict_to_disk(evict_key)  # é©±é€åˆ° Disk
        
        # ä»å†…å­˜æ± åˆ†é…
        tensor = self.memory_allocator.allocate(shape, dtype)
        
        # åˆ›å»º MemoryObj
        memory_obj = MemoryObj(
            tensor=tensor,
            metadata=MemoryMetadata(shape=shape, dtype=dtype, fmt=fmt)
        )
        
        return memory_obj
    
    def put(self, key: CacheEngineKey, memory_obj: MemoryObj):
        """
        å­˜å‚¨åˆ° CPU ç¼“å­˜
        """
        self.cache[key] = memory_obj
        self.evictor.update(key)  # æ›´æ–° LRU
    
    def get(self, key: CacheEngineKey) -> Optional[MemoryObj]:
        """
        ä» CPU ç¼“å­˜è¯»å–
        """
        if key in self.cache:
            self.evictor.update(key)  # æ›´æ–° LRU
            return self.cache[key]
        return None
```

**å…³é”®ä¼˜åŒ–**ï¼š
- **Pinned Memory**ï¼šGPU å¯ä»¥ç›´æ¥ DMA è®¿é—®
- **LRU é©±é€**ï¼šå†…å­˜æ»¡æ—¶è‡ªåŠ¨é©±é€æœ€å°‘ä½¿ç”¨çš„
- **å¼‚æ­¥å†™å…¥**ï¼šä¸é˜»å¡ä¸»çº¿ç¨‹

---

## æ•°æ®æµè½¬å…¨è¿‡ç¨‹

### åœºæ™¯ 1ï¼šé¦–æ¬¡å­˜å‚¨ KV Cache

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    vLLM Inference                            â”‚
â”‚  1. æ¨¡å‹æ¨ç†ï¼Œç”Ÿæˆ KV Cacheï¼ˆGPU Paged Memoryï¼‰              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LMCache.store(tokens, kv_caches)                â”‚
â”‚                                                              â”‚
â”‚  Step 1: Token åºåˆ—åˆ†å—å’Œå“ˆå¸Œ                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ tokens: [1, 2, 3, ..., 512]                           â”‚ â”‚
â”‚  â”‚    â†“ æŒ‰ chunk_size=256 åˆ†å—                            â”‚ â”‚
â”‚  â”‚ chunks: [[1...256], [257...512]]                      â”‚ â”‚
â”‚  â”‚    â†“ è®¡ç®—å“ˆå¸Œ                                          â”‚ â”‚
â”‚  â”‚ hashes: [hash1, hash2]                                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                              â”‚
â”‚  Step 2: ä» GPU æå–ï¼ˆå¯¹æ¯ä¸ª chunkï¼‰                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ GPU Connector.from_gpu()                              â”‚ â”‚
â”‚  â”‚   â”œâ”€ vLLM Paged Memory (GPU, ä¸è¿ç»­)                  â”‚ â”‚
â”‚  â”‚   â”‚     [Page 0][Page 2][Page 5]...                   â”‚ â”‚
â”‚  â”‚   â†“ CUDA Kernel: multi_layer_kv_transfer             â”‚ â”‚
â”‚  â”‚   â”œâ”€ LMCache Contiguous Memory (CPU Pinned, è¿ç»­)     â”‚ â”‚
â”‚  â”‚   â”‚     [Token 0][Token 1][Token 2]...                â”‚ â”‚
â”‚  â”‚   â””â”€ MemoryObj {tensor, metadata}                     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                                                    â”‚
â”‚         â–¼                                                    â”‚
â”‚  Step 3: æäº¤åˆ° Storage Manager                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ storage_manager.submit_put_task(key, memory_obj)      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼ å¼‚æ­¥å¤„ç†
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Storage Manager å¼‚æ­¥ä»»åŠ¡                     â”‚
â”‚                                                              â”‚
â”‚  Step 4a: å­˜å‚¨åˆ° Local CPU Backend                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ LocalCPUBackend.put(key, memory_obj)                  â”‚ â”‚
â”‚  â”‚   â”œâ”€ æ£€æŸ¥å†…å­˜ç©ºé—´                                      â”‚ â”‚
â”‚  â”‚   â”œâ”€ å¦‚æœæ»¡äº†ï¼ŒLRU é©±é€                                 â”‚ â”‚
â”‚  â”‚   â””â”€ å­˜å…¥ cache[key] = memory_obj                     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                                                    â”‚
â”‚         â–¼                                                    â”‚
â”‚  Step 4b: å¼‚æ­¥ Offload åˆ° Diskï¼ˆå¦‚æœé…ç½®ï¼‰                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ LocalDiskBackend.async_put(key, memory_obj)           â”‚ â”‚
â”‚  â”‚   â”œâ”€ åºåˆ—åŒ– memory_obj                                 â”‚ â”‚
â”‚  â”‚   â”œâ”€ å‹ç¼©ï¼ˆå¯é€‰ï¼‰                                       â”‚ â”‚
â”‚  â”‚   â””â”€ å†™å…¥ç£ç›˜æ–‡ä»¶                                       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                                                    â”‚
â”‚         â–¼                                                    â”‚
â”‚  Step 4c: å¼‚æ­¥ä¸Šä¼ åˆ° Remoteï¼ˆå¦‚æœé…ç½®ï¼‰                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ RemoteBackend.async_put(key, memory_obj)              â”‚ â”‚
â”‚  â”‚   â”œâ”€ åºåˆ—åŒ–                                            â”‚ â”‚
â”‚  â”‚   â””â”€ å‘é€åˆ° Redis/S3/...                               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ—¶é—´çº¿**ï¼š
- Step 1-3ï¼š**åŒæ­¥**ï¼ˆå¿«ï¼Œ< 1msï¼‰
- Step 4aï¼š**åŒæ­¥**ï¼ˆå¿«ï¼Œå†…å­˜æ“ä½œï¼‰
- Step 4b-cï¼š**å¼‚æ­¥**ï¼ˆæ…¢ï¼Œä¸é˜»å¡æ¨ç†ï¼‰

---

### åœºæ™¯ 2ï¼šæ£€ç´¢å’Œå¤ç”¨ KV Cache

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    vLLM Inference                            â”‚
â”‚  æ”¶åˆ°æ–°è¯·æ±‚ï¼Œtokens å‰ç¼€å¯èƒ½å·²ç¼“å­˜                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           hit_tokens = LMCache.retrieve(tokens)              â”‚
â”‚                                                              â”‚
â”‚  Step 1: Token åºåˆ—åˆ†å—å’Œå“ˆå¸Œ                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ tokens: [1, 2, 3, ..., 512]                           â”‚ â”‚
â”‚  â”‚    â†“ åˆ†å—                                              â”‚ â”‚
â”‚  â”‚ chunks: [[1...256], [257...512]]                      â”‚ â”‚
â”‚  â”‚    â†“ å“ˆå¸Œ                                              â”‚ â”‚
â”‚  â”‚ hashes: [hash1, hash2]                                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                              â”‚
â”‚  Step 2: æŸ¥è¯¢ Storage Managerï¼ˆå¯¹æ¯ä¸ª chunkï¼‰                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ For chunk in chunks:                                  â”‚ â”‚
â”‚  â”‚   memory_obj = storage_manager.get(key)               â”‚ â”‚
â”‚  â”‚                                                       â”‚ â”‚
â”‚  â”‚   æŸ¥æ‰¾é¡ºåºï¼š                                           â”‚ â”‚
â”‚  â”‚   1. Local CPU Backend (å‘½ä¸­ç‡ 80-90%)                â”‚ â”‚
â”‚  â”‚      â””â”€ ç›´æ¥è¿”å› memory_obj                           â”‚ â”‚
â”‚  â”‚                                                       â”‚ â”‚
â”‚  â”‚   2. Local Disk Backend (å‘½ä¸­ç‡ 10-20%)               â”‚ â”‚
â”‚  â”‚      â”œâ”€ ä»ç£ç›˜è¯»å–                                     â”‚ â”‚
â”‚  â”‚      â”œâ”€ è§£å‹ç¼©                                         â”‚ â”‚
â”‚  â”‚      â””â”€ æå‡åˆ° CPU Backend                            â”‚ â”‚
â”‚  â”‚                                                       â”‚ â”‚
â”‚  â”‚   3. Remote Backend (å‘½ä¸­ç‡ < 5%)                     â”‚ â”‚
â”‚  â”‚      â”œâ”€ ä» Redis/S3 è·å–                               â”‚ â”‚
â”‚  â”‚      â””â”€ æå‡åˆ° CPU Backend                            â”‚ â”‚
â”‚  â”‚                                                       â”‚ â”‚
â”‚  â”‚   4. æœªå‘½ä¸­                                            â”‚ â”‚
â”‚  â”‚      â””â”€ åœæ­¢æŸ¥è¯¢ï¼ˆå‰ç¼€åŒ¹é…ï¼‰                           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                                                    â”‚
â”‚         â–¼                                                    â”‚
â”‚  Step 3: åŠ è½½åˆ° GPUï¼ˆå¯¹å‘½ä¸­çš„ chunkï¼‰                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ GPU Connector.to_gpu()                                â”‚ â”‚
â”‚  â”‚   â”œâ”€ LMCache Contiguous Memory (CPU Pinned)           â”‚ â”‚
â”‚  â”‚   â”‚     [Token 0][Token 1][Token 2]...                â”‚ â”‚
â”‚  â”‚   â†“ CUDA Kernel: multi_layer_kv_transfer             â”‚ â”‚
â”‚  â”‚   â””â”€ vLLM Paged Memory (GPU)                          â”‚ â”‚
â”‚  â”‚         [Page 0][Page 2][Page 5]...                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                              â”‚
â”‚  Return: hit_tokens (å·²å¤ç”¨çš„ token æ•°é‡)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    vLLM Inference                            â”‚
â”‚  è·³è¿‡å·²å¤ç”¨çš„ tokensï¼Œåªè®¡ç®—å‰©ä½™éƒ¨åˆ†                          â”‚
â”‚  â”œâ”€ å·²å¤ç”¨ï¼štokens[0:hit_tokens]  â† ç›´æ¥ä½¿ç”¨ç¼“å­˜             â”‚
â”‚  â””â”€ éœ€è®¡ç®—ï¼štokens[hit_tokens:]   â† æ¨¡å‹æ¨ç†                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ€§èƒ½å…³é”®ç‚¹**ï¼š
- **CPU å‘½ä¸­**ï¼š< 1msï¼ˆå†…å­˜è¯»å–ï¼‰
- **Disk å‘½ä¸­**ï¼š10-50msï¼ˆç£ç›˜ I/Oï¼‰
- **Remote å‘½ä¸­**ï¼š50-200msï¼ˆç½‘ç»œ I/Oï¼‰

---

### åœºæ™¯ 3ï¼šè·¨å®ä¾‹ P2P å…±äº«

```
Worker A (æœ‰ç¼“å­˜)                        Worker B (éœ€è¦ç¼“å­˜)
    â”‚                                         â”‚
    â”‚ 1. Worker B å‘èµ·æŸ¥è¯¢                     â”‚
    â”‚â—„â”€â”€â”€â”€â”€â”€â”€ P2P Lookup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   { keys: [hash1, hash2] }               â”‚
    â”‚                                         â”‚
    â”‚ 2. Worker A æ£€æŸ¥æœ¬åœ°ç¼“å­˜                  â”‚
    â”œâ”€ LocalCPUBackend.contains(hash1) âœ“     â”‚
    â”œâ”€ LocalCPUBackend.contains(hash2) âœ“     â”‚
    â”‚                                         â”‚
    â”‚ 3. Worker A å“åº”                         â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€ P2P Lookup Response â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
    â”‚   { hits: [hash1, hash2], location }     â”‚
    â”‚                                         â”‚
    â”‚ 4. Worker B å‘èµ· P2P ä¼ è¾“                â”‚
    â”‚â—„â”€â”€â”€â”€â”€â”€â”€ BatchedLookupAndPutMsg â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   { sender_id: A, mem_indexes }          â”‚
    â”‚                                         â”‚
    â”‚ 5. Worker B åˆ†é…æœ¬åœ°å†…å­˜                  â”‚
    â”‚                                         â”œâ”€ allocate()
    â”‚                                         â”‚
    â”‚ 6. NIXL ç›´æ¥ä¼ è¾“ï¼ˆGPU â†’ GPUï¼‰             â”‚
    â”œâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â–ºâ”‚
    â”‚   GPU Direct Transfer (RDMA)             â”‚
    â”œâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â–ºâ”‚
    â”‚                                         â”‚
    â”‚                                         â”‚ 7. Worker B å­˜å…¥æœ¬åœ°ç¼“å­˜
    â”‚                                         â”œâ”€ LocalCPUBackend.put()
    â”‚                                         â”‚
    â”‚ 8. Worker B å“åº”å®Œæˆ                     â”‚
    â”‚â—„â”€â”€â”€â”€â”€â”€â”€ BatchedLookupAndPutRetMsg â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   { num_read_chunks: 2 }                 â”‚
    â”‚                                         â”‚
    â–¼                                         â–¼
```

**æ€§èƒ½**ï¼š
- å»¶è¿Ÿï¼š2-5 msï¼ˆGPU Direct Transferï¼‰
- å¸¦å®½ï¼š10-25 GB/sï¼ˆæ¥è¿‘ NVLink/PCIe å¸¦å®½ï¼‰

---

## ä¸Šå±‚ä½¿ç”¨æ–¹å¼

### 1. vLLM é›†æˆ

**é›†æˆç‚¹**ï¼š`vllm/worker/model_runner.py`

#### åˆå§‹åŒ–

```python
# vLLM Worker å¯åŠ¨æ—¶
class ModelRunner:
    def __init__(self, ...):
        # å¦‚æœå¯ç”¨ LMCache
        if kv_transfer_config and kv_transfer_config["kv_connector"] == "LMCacheConnector":
            # åˆ›å»º LMCache Connector
            self.lmcache_connector = LMCacheConnector(
                config_file=kv_transfer_config.get("lmcache_config_file"),
                kvcaches=self.kv_caches,  # vLLM çš„ KV Cache
                metadata=...
            )
```

#### Prefill é˜¶æ®µï¼ˆç”Ÿæˆ KV Cacheï¼‰

```python
def prefill(self, seq_group_metadata_list):
    """
    Prefill é˜¶æ®µï¼šå¤„ç†è¾“å…¥ tokensï¼Œç”Ÿæˆ KV Cache
    """
    
    # Step 1: å°è¯•ä» LMCache æ£€ç´¢
    if self.lmcache_connector:
        for seq_group in seq_group_metadata_list:
            tokens = seq_group.token_ids
            
            # æ£€ç´¢ç¼“å­˜
            hit_tokens = self.lmcache_connector.retrieve(tokens)
            
            if hit_tokens > 0:
                # æ›´æ–° seq_groupï¼Œæ ‡è®°å“ªäº› tokens å·²ç¼“å­˜
                seq_group.cached_tokens = hit_tokens
    
    # Step 2: åªè®¡ç®—æœªç¼“å­˜çš„éƒ¨åˆ†
    for seq_group in seq_group_metadata_list:
        if seq_group.cached_tokens < len(seq_group.token_ids):
            # ä» cached_tokens å¼€å§‹è®¡ç®—
            new_tokens = seq_group.token_ids[seq_group.cached_tokens:]
            
            # æ¨¡å‹æ¨ç†ï¼ˆç”Ÿæˆæ–°çš„ KV Cacheï¼‰
            logits, new_kv = self.model.forward(new_tokens, ...)
    
    # Step 3: å­˜å‚¨æ–°ç”Ÿæˆçš„ KV Cache
    if self.lmcache_connector:
        for seq_group in seq_group_metadata_list:
            if seq_group.cached_tokens < len(seq_group.token_ids):
                # å­˜å‚¨å®Œæ•´çš„ token åºåˆ—
                self.lmcache_connector.store(
                    seq_group.token_ids,
                    self.kv_caches
                )
```

**å…³é”®ç‚¹**ï¼š
- **å…ˆæ£€ç´¢**ï¼šæ¯ä¸ªè¯·æ±‚å…ˆå°è¯•ä» LMCache è·å–
- **éƒ¨åˆ†è®¡ç®—**ï¼šåªè®¡ç®—æœªç¼“å­˜çš„ tokens
- **åå­˜å‚¨**ï¼šæ–°ç”Ÿæˆçš„ KV Cache å­˜å…¥ LMCache

#### Decode é˜¶æ®µï¼ˆç”Ÿæˆ tokenï¼‰

```python
def decode(self, seq_group_metadata_list):
    """
    Decode é˜¶æ®µï¼šç”Ÿæˆä¸‹ä¸€ä¸ª token
    """
    
    # Decode é˜¶æ®µé€šå¸¸ä¸ä½¿ç”¨ LMCache
    # ï¼ˆé™¤é config.save_decode_cache = Trueï¼‰
    
    for seq_group in seq_group_metadata_list:
        # æ­£å¸¸ decode
        next_token = self.model.forward_decode(...)
```

---

### 2. é…ç½®æ–¹å¼

#### æ–¹å¼ 1ï¼šå‘½ä»¤è¡Œå‚æ•°

```bash
vllm serve meta-llama/Llama-2-7b-hf \
  --kv-transfer-config '{
    "kv_connector": "LMCacheConnector",
    "kv_buffer_size": 1e9
  }'
```

#### æ–¹å¼ 2ï¼šé…ç½®æ–‡ä»¶

```yaml
# lmcache_config.yaml
chunk_size: 256
local_device: "cpu"
max_local_cache_size: 10  # GB

# å¯é€‰ï¼šRemote Storage
remote_url: "redis://localhost:6379"
remote_serde: "torch"

# å¯é€‰ï¼šP2P
enable_p2p: true
p2p_host: "localhost"
p2p_init_ports: [8200, 8202]
transfer_channel: "nixl"
```

```bash
export LMCACHE_CONFIG_FILE=lmcache_config.yaml

vllm serve meta-llama/Llama-2-7b-hf \
  --kv-transfer-config '{
    "kv_connector": "LMCacheConnector"
  }'
```

#### æ–¹å¼ 3ï¼šPython API

```python
from vllm import LLM, SamplingParams

llm = LLM(
    model="meta-llama/Llama-2-7b-hf",
    kv_transfer_config={
        "kv_connector": "LMCacheConnector",
        "kv_buffer_size": 1e9,
    },
    # æˆ–é€šè¿‡ç¯å¢ƒå˜é‡æŒ‡å®šé…ç½®æ–‡ä»¶
)

# æ­£å¸¸ä½¿ç”¨
outputs = llm.generate("Hello, how are you?", sampling_params)
```

---

### 3. SGLang é›†æˆ

```python
import sglang as sgl

# è®¾ç½® LMCache é…ç½®
lmcache_config = {
    "local_device": "cpu",
    "max_local_cache_size": 10,
}

# å¯åŠ¨ SGLang Runtime
runtime = sgl.Runtime(
    model_path="meta-llama/Llama-2-7b-hf",
    lmcache_config=lmcache_config
)

# ä½¿ç”¨
@sgl.function
def chat(s, question):
    s += sgl.system("You are a helpful assistant.")
    s += sgl.user(question)
    s += sgl.assistant(sgl.gen("answer"))

state = chat.run(question="What is AI?")
```

---

## å®Œæ•´ç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šå¤šè½®å¯¹è¯ï¼ˆSystem Prompt å¤ç”¨ï¼‰

```python
from vllm import LLM, SamplingParams

# åˆå§‹åŒ– LMCache
llm = LLM(
    model="meta-llama/Llama-2-7b-hf",
    kv_transfer_config={
        "kv_connector": "LMCacheConnector",
    }
)

sampling_params = SamplingParams(max_tokens=100)

# System Promptï¼ˆå›ºå®šï¼‰
system_prompt = "You are a helpful AI assistant. " * 100  # å‡è®¾å¾ˆé•¿

# Round 1ï¼šé¦–æ¬¡å¯¹è¯
prompt1 = system_prompt + "\nUser: What is AI?\nAssistant:"
output1 = llm.generate(prompt1, sampling_params)
# â†‘ LMCache å­˜å‚¨äº† system_prompt çš„ KV Cache

# Round 2ï¼šç¬¬äºŒæ¬¡å¯¹è¯
prompt2 = system_prompt + "\nUser: Tell me about ML.\nAssistant:"
output2 = llm.generate(prompt2, sampling_params)
# â†‘ LMCache å¤ç”¨äº† system_prompt çš„ KV Cacheï¼ˆå‘½ä¸­ï¼ï¼‰
#   åªéœ€è¦è®¡ç®— "User: Tell me about ML." çš„éƒ¨åˆ†

# Round 3ï¼šç¬¬ä¸‰æ¬¡å¯¹è¯
prompt3 = system_prompt + "\nUser: Explain deep learning.\nAssistant:"
output3 = llm.generate(prompt3, sampling_params)
# â†‘ å†æ¬¡å¤ç”¨ system_promptï¼ˆå‘½ä¸­ï¼ï¼‰
```

**æ€§èƒ½æå‡**ï¼š
- Round 1ï¼š100% è®¡ç®—ï¼ˆæ— ç¼“å­˜ï¼‰
- Round 2ï¼šåªè®¡ç®— 10%ï¼ˆsystem_prompt å  90%ï¼‰
- Round 3ï¼šåªè®¡ç®— 10%

**TTFT é™ä½**ï¼š5-10x

---

### ç¤ºä¾‹ 2ï¼šRAGï¼ˆæ–‡æ¡£ç¼“å­˜ï¼‰

```python
from vllm import LLM

llm = LLM(model="...", kv_transfer_config={"kv_connector": "LMCacheConnector"})

# é•¿æ–‡æ¡£ï¼ˆæ¯æ¬¡ RAG éƒ½æ£€ç´¢åˆ°ï¼‰
long_document = "..." * 10000  # 10k tokens

# Query 1
query1 = f"Document: {long_document}\n\nQuestion: Who is the author?\nAnswer:"
answer1 = llm.generate(query1)
# â†‘ å­˜å‚¨ long_document çš„ KV Cache

# Query 2ï¼ˆä¸åŒé—®é¢˜ï¼Œä½†åŒä¸€æ–‡æ¡£ï¼‰
query2 = f"Document: {long_document}\n\nQuestion: What is the main idea?\nAnswer:"
answer2 = llm.generate(query2)
# â†‘ å¤ç”¨ long_document çš„ KV Cacheï¼ˆå‘½ä¸­ï¼ï¼‰

# Query 3
query3 = f"Document: {long_document}\n\nQuestion: Summarize it.\nAnswer:"
answer3 = llm.generate(query3)
# â†‘ å†æ¬¡å¤ç”¨ï¼ˆå‘½ä¸­ï¼ï¼‰
```

**æ€§èƒ½æå‡**ï¼š
- Query 1ï¼šè®¡ç®—æ•´ä¸ªæ–‡æ¡£ï¼ˆ10k tokensï¼‰
- Query 2-3ï¼šåªè®¡ç®—é—®é¢˜éƒ¨åˆ†ï¼ˆ< 20 tokensï¼‰

**GPU è®¡ç®—èŠ‚çœ**ï¼š500x

---

## æ€§èƒ½åˆ†æ

### å…³é”®æ€§èƒ½æŒ‡æ ‡

#### 1. ç¼“å­˜å‘½ä¸­ç‡

```
ç¼“å­˜å‘½ä¸­ç‡ = å¤ç”¨çš„ tokens / æ€» tokens
```

**å…¸å‹å€¼**ï¼š
- å¤šè½®å¯¹è¯ï¼š80-95%ï¼ˆSystem Prompt å å¤§éƒ¨åˆ†ï¼‰
- RAGï¼š70-90%ï¼ˆæ–‡æ¡£å å¤§éƒ¨åˆ†ï¼‰
- é•¿æ–‡æ¡£ QAï¼š90-95%

#### 2. TTFTï¼ˆTime To First Tokenï¼‰

```
TTFT = Prefill æ—¶é—´ + Decode é¦– token æ—¶é—´
```

**LMCache ä¼˜åŒ–**ï¼š

```
æ—  LMCacheï¼š
TTFT = T_prefill(all_tokens) + T_decode
     â‰ˆ 1000ms + 50ms = 1050ms

æœ‰ LMCacheï¼ˆ90% å‘½ä¸­ï¼‰ï¼š
TTFT = T_retrieve + T_prefill(10% tokens) + T_decode
     â‰ˆ 1ms + 100ms + 50ms = 151ms

æå‡ï¼š1050ms / 151ms â‰ˆ 7x
```

#### 3. ååé‡

```
ååé‡ = è¯·æ±‚æ•° / æ—¶é—´
```

**æå‡åŸå› **ï¼š
- Prefill æ—¶é—´å‡å°‘ â†’ GPU æ›´å¿«å¤„ç†ä¸‹ä¸€ä¸ªè¯·æ±‚
- GPU åˆ©ç”¨ç‡æé«˜

**å…¸å‹æå‡**ï¼š3-5x

---

### æ€§èƒ½ç“¶é¢ˆåˆ†æ

#### ç“¶é¢ˆ 1ï¼šCPU-GPU ä¼ è¾“

```python
# ä¼˜åŒ–å‰ï¼ˆæ…¢ï¼‰
kv_cache = gpu_tensor.cpu()  # GPU â†’ CPUï¼ˆé˜»å¡ï¼‰

# ä¼˜åŒ–åï¼ˆå¿«ï¼‰
kv_cache = torch.empty(..., pin_memory=True)  # Pinned Memory
gpu_tensor.copy_(kv_cache, non_blocking=True)  # å¼‚æ­¥æ‹·è´
```

**ä¼˜åŒ–æ•ˆæœ**ï¼š2-3x

#### ç“¶é¢ˆ 2ï¼šDisk I/O

```python
# ä¼˜åŒ–å‰ï¼ˆæ…¢ï¼‰
with open(path, 'wb') as f:
    torch.save(kv_cache, f)  # åŒæ­¥å†™å…¥

# ä¼˜åŒ–åï¼ˆå¿«ï¼‰
async def async_write():
    # å¼‚æ­¥å†™å…¥ï¼Œä¸é˜»å¡ä¸»çº¿ç¨‹
    await asyncio.to_thread(torch.save, kv_cache, path)
```

**ä¼˜åŒ–æ•ˆæœ**ï¼šä¸é˜»å¡æ¨ç†

#### ç“¶é¢ˆ 3ï¼šRemote ç½‘ç»œå»¶è¿Ÿ

```python
# ä¼˜åŒ–ï¼šLazy Upload
# - CPU Backendï¼šç«‹å³å¯ç”¨ï¼ˆ< 1msï¼‰
# - Remote Backendï¼šå¼‚æ­¥ä¸Šä¼ ï¼ˆä¸é˜»å¡ï¼‰

storage_manager.submit_put_task(key, memory_obj)
# â†‘ ç«‹å³è¿”å›ï¼Œåå°ä¸Šä¼ 
```

---

## æ€»ç»“

### æ ¸å¿ƒå®ç°è¦ç‚¹

1. **åˆ†å±‚è®¾è®¡**ï¼š
   - Cache Engineï¼šä¸šåŠ¡é€»è¾‘
   - GPU Connectorï¼šGPU æ¥å£
   - Storage Managerï¼šå­˜å‚¨åè°ƒ
   - Storage Backendsï¼šå¤šçº§å­˜å‚¨

2. **å¼‚æ­¥å¤„ç†**ï¼š
   - Prefillï¼šåŒæ­¥æ£€ç´¢ + å¼‚æ­¥å­˜å‚¨
   - ä¸é˜»å¡æ¨ç†ä¸»çº¿ç¨‹

3. **å¤šçº§ç¼“å­˜**ï¼š
   - CPUï¼ˆçƒ­ï¼‰â†’ Diskï¼ˆæ¸©ï¼‰â†’ Remoteï¼ˆå†·ï¼‰
   - è‡ªåŠ¨æå‡çƒ­åº¦

### æ•°æ®æµè½¬å…³é”®è·¯å¾„

```
å­˜å‚¨ï¼šGPU Paged â†’ CPU Contiguous â†’ Storage Backends
æ£€ç´¢ï¼šStorage Backends â†’ CPU Contiguous â†’ GPU Paged
```

### ä¸Šå±‚ä½¿ç”¨æ¨¡å¼

```python
# 1. é…ç½® LMCache
kv_transfer_config = {"kv_connector": "LMCacheConnector"}

# 2. åˆ›å»º LLM
llm = LLM(model="...", kv_transfer_config=kv_transfer_config)

# 3. æ­£å¸¸æ¨ç†ï¼ˆè‡ªåŠ¨ç¼“å­˜å¤ç”¨ï¼‰
outputs = llm.generate(prompt)
```

### æ€§èƒ½å…³é”®å› ç´ 

| å› ç´  | å½±å“ | ä¼˜åŒ–æ–¹æ³• |
|-----|------|---------|
| ç¼“å­˜å‘½ä¸­ç‡ | ç›´æ¥å½±å“ TTFT | å¢åŠ  cache_sizeã€è°ƒæ•´ chunk_size |
| CPU-GPU ä¼ è¾“ | Prefill å»¶è¿Ÿ | Pinned Memoryã€å¼‚æ­¥æ‹·è´ |
| å­˜å‚¨å±‚çº§ | å‘½ä¸­å»¶è¿Ÿ | ä¼˜å…ˆ CPUã€Lazy Upload |

---

**å®Œæ•´çš„ LMCache å®ç°ç»†èŠ‚å’Œæ•°æ®æµè½¬è§£æå®Œæˆï¼** ğŸ‰

å¸Œæœ›è¿™ç¯‡æ–‡æ¡£èƒ½å¸®ä½ æ·±å…¥ç†è§£ LMCache çš„å·¥ä½œåŸç†ï¼

