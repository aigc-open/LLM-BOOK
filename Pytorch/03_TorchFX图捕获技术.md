# ç¬¬ä¸‰ç« ï¼šTorchFX å›¾æ•è·æŠ€æœ¯

## ğŸ¯ æœ¬ç« ç›®æ ‡

- ç†è§£ TorchFX çš„è®¾è®¡ç†å¿µå’Œä¼˜åŠ¿
- æŒæ¡ç¬¦å·è¿½è¸ªï¼ˆSymbolic Tracingï¼‰æŠ€æœ¯
- å­¦ä¹  TorchFX IR çš„ç»“æ„å’Œæ“ä½œ
- èƒ½å¤Ÿç¼–å†™å›¾å˜æ¢å’Œä¼˜åŒ–

## 1. ä»€ä¹ˆæ˜¯ TorchFXï¼Ÿ

### 1.1 ç®€å•ç†è§£

**TorchScript çš„é—®é¢˜ï¼š**
```python
# TorchScript æŠŠä»£ç å˜æˆ"é»‘ç›’"
scripted_model = torch.jit.script(model)
# éš¾ä»¥ä¿®æ”¹ã€éš¾ä»¥åˆ†æå†…éƒ¨ç»“æ„
```

**TorchFX çš„è§£å†³æ–¹æ¡ˆï¼š**
```python
# TorchFX æä¾›"é€æ˜çš„"ã€å¯ç¼–è¾‘çš„å›¾è¡¨ç¤º
traced_graph = torch.fx.symbolic_trace(model)
# å¯ä»¥æŸ¥çœ‹ã€ä¿®æ”¹æ¯ä¸ªæ“ä½œï¼
```

**ç±»æ¯”ï¼š**
- TorchScript = ç¼–è¯‘åçš„äºŒè¿›åˆ¶æ–‡ä»¶ï¼ˆéš¾ä»¥ä¿®æ”¹ï¼‰
- TorchFX = æºä»£ç ï¼ˆå¯ä»¥è‡ªç”±ç¼–è¾‘ï¼‰

### 1.2 æ ¸å¿ƒç‰¹ç‚¹

1. **Python-First**ï¼šå®Œå…¨æ˜¯ Python å¯¹è±¡ï¼Œæ˜“äºæ“ä½œ
2. **å¯ç¼–ç¨‹**ï¼šå¯ä»¥è½»æ¾ä¿®æ”¹è®¡ç®—å›¾
3. **ä¿ç•™åŠ¨æ€æ€§**ï¼šä¸ Python ç”Ÿæ€å…¼å®¹
4. **è½»é‡çº§**ï¼šæ— éœ€é‡é‡çº§ç¼–è¯‘å™¨

## 2. ç¬¦å·è¿½è¸ªï¼ˆSymbolic Tracingï¼‰

### 2.1 åŸºæœ¬ç”¨æ³•

```python
import torch
import torch.nn as nn
import torch.fx as fx

# å®šä¹‰ä¸€ä¸ªç®€å•æ¨¡å‹
class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear1 = nn.Linear(10, 20)
        self.linear2 = nn.Linear(20, 5)
    
    def forward(self, x):
        x = self.linear1(x)
        x = torch.relu(x)
        x = self.linear2(x)
        return x

# ç¬¦å·è¿½è¸ª
model = SimpleModel()
traced_graph = fx.symbolic_trace(model)

# æŸ¥çœ‹ç”Ÿæˆçš„ä»£ç 
print(traced_graph.code)
```

**è¾“å‡ºï¼š**
```python
def forward(self, x):
    linear1 = self.linear1(x)
    relu = torch.relu(linear1)
    linear2 = self.linear2(relu)
    return linear2
```

### 2.2 ä¸ torch.jit.trace çš„åŒºåˆ«

| ç‰¹æ€§ | torch.jit.trace | torch.fx.symbolic_trace |
|------|----------------|------------------------|
| **æ‰§è¡Œæ–¹å¼** | è¿è¡Œå®é™…è®¡ç®— | ç¬¦å·æ‰§è¡Œï¼ˆä¸è®¡ç®—ï¼‰ |
| **è¿”å›ç±»å‹** | ScriptModule | GraphModule |
| **å¯ä¿®æ”¹æ€§** | âŒ å›°éš¾ | âœ… å®¹æ˜“ |
| **æ€§èƒ½å¼€é”€** | éœ€è¦å®é™…è®¡ç®— | å‡ ä¹æ— å¼€é”€ |

```python
# jit.trace éœ€è¦å®é™…æ•°æ®
import torch.jit as jit
traced_jit = jit.trace(model, torch.randn(1, 10))  # éœ€è¦ç¤ºä¾‹è¾“å…¥

# fx.symbolic_trace ä¸éœ€è¦
traced_fx = fx.symbolic_trace(model)  # æ— éœ€ç¤ºä¾‹è¾“å…¥ï¼
```

## 3. TorchFX IR è¯¦è§£

### 3.1 IR çš„ç»„æˆ

TorchFX çš„ IR ç”±ä»¥ä¸‹å‡ éƒ¨åˆ†ç»„æˆï¼š

```python
traced = fx.symbolic_trace(model)

# 1. Graphï¼šè®¡ç®—å›¾
print(traced.graph)

# 2. Nodesï¼šå›¾ä¸­çš„èŠ‚ç‚¹
for node in traced.graph.nodes:
    print(node)

# 3. Codeï¼šç”Ÿæˆçš„ Python ä»£ç 
print(traced.code)
```

### 3.2 Node ç±»å‹

TorchFX ä¸­æœ‰ 6 ç§èŠ‚ç‚¹ç±»å‹ï¼š

```python
import torch.fx as fx

traced = fx.symbolic_trace(SimpleModel())

for node in traced.graph.nodes:
    print(f"{node.name}: {node.op}")
```

**èŠ‚ç‚¹ç±»å‹è¯¦è§£ï¼š**

| ç±»å‹ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| **placeholder** | è¾“å…¥å‚æ•° | `x` (å‡½æ•°å‚æ•°) |
| **get_attr** | è·å–æ¨¡å—å±æ€§ | `self.linear1` |
| **call_function** | è°ƒç”¨å‡½æ•° | `torch.relu(x)` |
| **call_method** | è°ƒç”¨æ–¹æ³• | `x.view(...)` |
| **call_module** | è°ƒç”¨å­æ¨¡å— | `self.linear1(x)` |
| **output** | è¾“å‡º | `return x` |

### 3.3 è¯¦ç»†ç¤ºä¾‹

```python
class ExampleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(5, 5))
        self.linear = nn.Linear(5, 3)
    
    def forward(self, x):
        # å„ç§ä¸åŒç±»å‹çš„æ“ä½œ
        w = self.weight              # get_attr
        x = torch.matmul(x, w)       # call_function
        x = x.relu()                 # call_method
        x = self.linear(x)           # call_module
        return x

traced = fx.symbolic_trace(ExampleModel())

print("=" * 50)
print("å›¾ç»“æ„ï¼š")
print(traced.graph)

print("\n" + "=" * 50)
print("è¯¦ç»†èŠ‚ç‚¹ä¿¡æ¯ï¼š")
for node in traced.graph.nodes:
    print(f"åç§°: {node.name:15} | ç±»å‹: {node.op:15} | ç›®æ ‡: {node.target}")
```

**è¾“å‡ºï¼š**
```
==================================================
å›¾ç»“æ„ï¼š
graph():
    %x : [#users=1] = placeholder[target=x]
    %weight : [#users=1] = get_attr[target=weight]
    %matmul : [#users=1] = call_function[target=torch.matmul](args = (%x, %weight), kwargs = {})
    %relu : [#users=1] = call_method[target=relu](args = (%matmul,), kwargs = {})
    %linear : [#users=1] = call_module[target=linear](args = (%relu,), kwargs = {})
    return linear

==================================================
è¯¦ç»†èŠ‚ç‚¹ä¿¡æ¯ï¼š
åç§°: x               | ç±»å‹: placeholder     | ç›®æ ‡: x
åç§°: weight          | ç±»å‹: get_attr        | ç›®æ ‡: weight
åç§°: matmul          | ç±»å‹: call_function   | ç›®æ ‡: <built-in function matmul>
åç§°: relu            | ç±»å‹: call_method     | ç›®æ ‡: relu
åç§°: linear          | ç±»å‹: call_module     | ç›®æ ‡: linear
åç§°: output          | ç±»å‹: output          | ç›®æ ‡: output
```

### 3.4 Node çš„å±æ€§

æ¯ä¸ª Node éƒ½æœ‰ä¸°å¯Œçš„å±æ€§ï¼š

```python
traced = fx.symbolic_trace(model)

for node in traced.graph.nodes:
    if node.op == 'call_function':
        print(f"èŠ‚ç‚¹å: {node.name}")
        print(f"  æ“ä½œç±»å‹: {node.op}")
        print(f"  ç›®æ ‡å‡½æ•°: {node.target}")
        print(f"  å‚æ•°: {node.args}")
        print(f"  å…³é”®å­—å‚æ•°: {node.kwargs}")
        print(f"  ä½¿ç”¨è€…: {[u.name for u in node.users]}")
        print(f"  ä½¿ç”¨æ¬¡æ•°: {len(node.users)}")
        print()
```

## 4. å›¾å˜æ¢ï¼ˆGraph Transformationï¼‰

### 4.1 åŸºæœ¬æ“ä½œï¼šéå†å’ŒæŸ¥çœ‹

```python
def analyze_graph(traced_model):
    """åˆ†æå›¾çš„åŸºæœ¬ä¿¡æ¯"""
    print("=" * 50)
    print("å›¾åˆ†ææŠ¥å‘Š")
    print("=" * 50)
    
    # ç»Ÿè®¡èŠ‚ç‚¹ç±»å‹
    op_counts = {}
    for node in traced_model.graph.nodes:
        op_counts[node.op] = op_counts.get(node.op, 0) + 1
    
    print("\nèŠ‚ç‚¹ç±»å‹ç»Ÿè®¡:")
    for op, count in op_counts.items():
        print(f"  {op}: {count}")
    
    # æ‰¾åˆ°æ‰€æœ‰å‡½æ•°è°ƒç”¨
    print("\nå‡½æ•°è°ƒç”¨:")
    for node in traced_model.graph.nodes:
        if node.op == 'call_function':
            print(f"  {node.name}: {node.target.__name__}")
    
    # æ‰¾åˆ°æ‰€æœ‰æ¨¡å—è°ƒç”¨
    print("\næ¨¡å—è°ƒç”¨:")
    for node in traced_model.graph.nodes:
        if node.op == 'call_module':
            print(f"  {node.name}: {type(traced_model.get_submodule(node.target)).__name__}")

# ä½¿ç”¨
traced = fx.symbolic_trace(SimpleModel())
analyze_graph(traced)
```

### 4.2 ä¿®æ”¹å›¾ï¼šæ›¿æ¢æ“ä½œ

**ç¤ºä¾‹ 1ï¼šå°† ReLU æ›¿æ¢ä¸º GELU**

```python
import torch.nn.functional as F

def replace_relu_with_gelu(traced_model):
    """å°†æ‰€æœ‰ ReLU æ›¿æ¢ä¸º GELU"""
    graph = traced_model.graph
    
    # éå†æ‰€æœ‰èŠ‚ç‚¹
    for node in graph.nodes:
        # æ‰¾åˆ° relu è°ƒç”¨
        if node.op == 'call_function' and node.target == torch.relu:
            # æ›¿æ¢ç›®æ ‡å‡½æ•°
            node.target = F.gelu
            print(f"æ›¿æ¢èŠ‚ç‚¹ {node.name}: relu â†’ gelu")
    
    # é‡æ–°ç¼–è¯‘
    traced_model.recompile()
    return traced_model

# ä½¿ç”¨
traced = fx.symbolic_trace(SimpleModel())
print("åŸå§‹ä»£ç :")
print(traced.code)

modified = replace_relu_with_gelu(traced)
print("\nä¿®æ”¹åçš„ä»£ç :")
print(modified.code)
```

**è¾“å‡ºï¼š**
```python
# åŸå§‹ä»£ç :
def forward(self, x):
    linear1 = self.linear1(x)
    relu = torch.relu(linear1)
    linear2 = self.linear2(relu)
    return linear2

# ä¿®æ”¹åçš„ä»£ç :
def forward(self, x):
    linear1 = self.linear1(x)
    relu = torch.nn.functional.gelu(linear1)  # å·²æ›¿æ¢ï¼
    linear2 = self.linear2(relu)
    return linear2
```

### 4.3 æ’å…¥æ–°èŠ‚ç‚¹

**ç¤ºä¾‹ 2ï¼šåœ¨æ¯ä¸ªæ“ä½œåæ’å…¥æ‰“å°**

```python
def insert_print_statements(traced_model):
    """åœ¨æ¯ä¸ªæ“ä½œåæ’å…¥æ‰“å°è¯­å¥ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
    graph = traced_model.graph
    
    # éœ€è¦åœ¨è¿­ä»£æ—¶ä¿®æ”¹ï¼Œæ‰€ä»¥å…ˆæ”¶é›†èŠ‚ç‚¹
    nodes_to_instrument = []
    for node in graph.nodes:
        if node.op in ['call_function', 'call_module', 'call_method']:
            nodes_to_instrument.append(node)
    
    # æ’å…¥æ‰“å°èŠ‚ç‚¹
    for node in nodes_to_instrument:
        with graph.inserting_after(node):
            # æ’å…¥ print è°ƒç”¨
            print_node = graph.call_function(
                print,
                args=(f"[DEBUG] {node.name} shape:", node,)
            )
    
    graph.lint()  # éªŒè¯å›¾çš„æ­£ç¡®æ€§
    traced_model.recompile()
    return traced_model

# ä½¿ç”¨
traced = fx.symbolic_trace(SimpleModel())
instrumented = insert_print_statements(traced)
print(instrumented.code)
```

### 4.4 åˆ é™¤èŠ‚ç‚¹

**ç¤ºä¾‹ 3ï¼šç§»é™¤æ‰€æœ‰ Dropout**

```python
def remove_dropout(traced_model):
    """ç§»é™¤æ‰€æœ‰ Dropout å±‚"""
    graph = traced_model.graph
    
    # æ‰¾åˆ°æ‰€æœ‰ Dropout èŠ‚ç‚¹
    nodes_to_remove = []
    for node in graph.nodes:
        if node.op == 'call_module':
            module = traced_model.get_submodule(node.target)
            if isinstance(module, nn.Dropout):
                nodes_to_remove.append(node)
    
    # åˆ é™¤èŠ‚ç‚¹
    for node in nodes_to_remove:
        # Dropout çš„è¾“å‡ºå°±æ˜¯è¾“å…¥ï¼Œç›´æ¥æ›¿æ¢
        node.replace_all_uses_with(node.args[0])
        graph.erase_node(node)
        print(f"ç§»é™¤èŠ‚ç‚¹: {node.name}")
    
    graph.lint()
    traced_model.recompile()
    return traced_model
```

## 5. å®é™…åº”ç”¨ï¼šç®—å­èåˆ

### 5.1 Conv-BN èåˆ

å·ç§¯å’Œ BatchNorm å¯ä»¥èåˆä¸ºä¸€ä¸ªæ“ä½œï¼Œæå‡æ€§èƒ½ï¼š

```python
def fuse_conv_bn(traced_model):
    """
    èåˆ Conv2d + BatchNorm2d
    åŸç†ï¼šy = BN(Conv(x)) å¯ä»¥åˆå¹¶ä¸ºä¸€ä¸ª Conv
    """
    graph = traced_model.graph
    modules = dict(traced_model.named_modules())
    
    for node in graph.nodes:
        # æŸ¥æ‰¾ BatchNorm èŠ‚ç‚¹
        if node.op == 'call_module':
            bn = modules.get(node.target)
            if not isinstance(bn, nn.BatchNorm2d):
                continue
            
            # æ£€æŸ¥å‰ä¸€ä¸ªèŠ‚ç‚¹æ˜¯å¦æ˜¯ Conv2d
            if len(node.args) > 0:
                conv_node = node.args[0]
                if conv_node.op == 'call_module':
                    conv = modules.get(conv_node.target)
                    if isinstance(conv, nn.Conv2d):
                        # æ‰§è¡Œèåˆ
                        fused_conv = fuse_conv_bn_eval(conv, bn)
                        
                        # æ›¿æ¢ conv
                        setattr(traced_model, conv_node.target, fused_conv)
                        
                        # ç§»é™¤ BN
                        node.replace_all_uses_with(conv_node)
                        graph.erase_node(node)
                        
                        print(f"èåˆ: {conv_node.target} + {node.target}")
    
    graph.lint()
    traced_model.recompile()
    return traced_model

def fuse_conv_bn_eval(conv, bn):
    """
    æ•°å­¦ä¸Šèåˆ Conv å’Œ BN
    """
    # è·å–å‚æ•°
    conv_weight = conv.weight
    conv_bias = conv.bias if conv.bias is not None else torch.zeros(conv.out_channels)
    
    bn_weight = bn.weight
    bn_bias = bn.bias
    bn_mean = bn.running_mean
    bn_var = bn.running_var
    bn_eps = bn.eps
    
    # è®¡ç®—èåˆåçš„æƒé‡å’Œåç½®
    bn_std = torch.sqrt(bn_var + bn_eps)
    fused_weight = conv_weight * (bn_weight / bn_std).reshape(-1, 1, 1, 1)
    fused_bias = (conv_bias - bn_mean) * bn_weight / bn_std + bn_bias
    
    # åˆ›å»ºæ–°çš„ Conv
    fused_conv = nn.Conv2d(
        conv.in_channels,
        conv.out_channels,
        conv.kernel_size,
        conv.stride,
        conv.padding,
        bias=True
    )
    fused_conv.weight.data = fused_weight
    fused_conv.bias.data = fused_bias
    
    return fused_conv
```

### 5.2 ä½¿ç”¨èåˆ

```python
class ConvBNModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(3, 64, 3, padding=1)
        self.bn = nn.BatchNorm2d(64)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        x = self.relu(x)
        return x

# åŸå§‹æ¨¡å‹
model = ConvBNModel()
model.eval()  # å¿…é¡»æ˜¯ eval æ¨¡å¼

# Trace
traced = fx.symbolic_trace(model)
print("åŸå§‹ä»£ç :")
print(traced.code)

# èåˆ
fused = fuse_conv_bn(traced)
print("\nèåˆåä»£ç :")
print(fused.code)

# éªŒè¯æ­£ç¡®æ€§
input_data = torch.randn(1, 3, 224, 224)
with torch.no_grad():
    output_original = model(input_data)
    output_fused = fused(input_data)
    print(f"\nè¾“å‡ºå·®å¼‚: {(output_original - output_fused).abs().max().item()}")
```

## 6. é«˜çº§ç‰¹æ€§

### 6.1 Proxy å¯¹è±¡

åœ¨ symbolic_trace è¿‡ç¨‹ä¸­ï¼Œå®é™…æ“ä½œçš„æ˜¯ Proxy å¯¹è±¡ï¼š

```python
import torch.fx as fx

class MyTracer(fx.Tracer):
    def trace(self, root, concrete_args=None):
        print("å¼€å§‹è¿½è¸ª...")
        graph = super().trace(root, concrete_args)
        print("è¿½è¸ªå®Œæˆï¼")
        return graph

tracer = MyTracer()
traced = tracer.trace(SimpleModel())
```

### 6.2 è‡ªå®šä¹‰è¿½è¸ªè§„åˆ™

```python
class CustomTracer(fx.Tracer):
    def is_leaf_module(self, m, module_qualified_name):
        """
        å†³å®šå“ªäº›æ¨¡å—åº”è¯¥ä½œä¸º"å¶å­"ï¼ˆä¸è¿›ä¸€æ­¥è¿½è¸ªå†…éƒ¨ï¼‰
        """
        # é»˜è®¤ nn.Linear ä¼šè¢«è¿½è¸ªï¼Œè¿™é‡Œæˆ‘ä»¬è®©å®ƒæˆä¸ºå¶å­
        if isinstance(m, nn.Linear):
            return True
        return super().is_leaf_module(m, module_qualified_name)

tracer = CustomTracer()
traced = tracer.trace(SimpleModel())
```

### 6.3 Interpreter æ¨¡å¼

æ‰‹åŠ¨æ‰§è¡Œå›¾ï¼š

```python
class CustomInterpreter(fx.Interpreter):
    def call_function(self, target, args, kwargs):
        """æ‹¦æˆªæ‰€æœ‰å‡½æ•°è°ƒç”¨"""
        print(f"è°ƒç”¨å‡½æ•°: {target.__name__}")
        result = super().call_function(target, args, kwargs)
        print(f"  ç»“æœå½¢çŠ¶: {result.shape if isinstance(result, torch.Tensor) else type(result)}")
        return result

traced = fx.symbolic_trace(SimpleModel())
interpreter = CustomInterpreter(traced)

input_data = torch.randn(2, 10)
output = interpreter.run(input_data)
```

## 7. TorchFX åœ¨ PyTorch 2.0 ä¸­çš„è§’è‰²

### 7.1 ä½œä¸º IR å±‚

```
torch.compile æµç¨‹:
    â†“
Dynamo (æ•è·å­—èŠ‚ç )
    â†“
TorchFX Graph (IR è¡¨ç¤º) â† æˆ‘ä»¬åœ¨è¿™é‡Œï¼
    â†“
AOTAutograd (ç”Ÿæˆå‰å‘/åå‘å›¾)
    â†“
Backend Compiler (TorchInductor/...)
```

### 7.2 æŸ¥çœ‹ torch.compile ç”Ÿæˆçš„å›¾

```python
import torch
import torch._dynamo as dynamo

model = SimpleModel()
input_data = torch.randn(1, 10)

# å¯¼å‡º torch.compile æ•è·çš„å›¾
exported, _ = dynamo.export(model, input_data)

# è¿™å°±æ˜¯ä¸€ä¸ª TorchFX GraphModuleï¼
print(type(exported))  # <class 'torch.fx.graph_module.GraphModule'>
print(exported.code)
```

## 8. æºç ä½ç½®

åœ¨ `pytorch/torch/fx/` ç›®å½•ä¸‹ï¼š

```
torch/fx/
â”œâ”€â”€ __init__.py           # ä¸»è¦ API
â”œâ”€â”€ graph.py              # Graph å’Œ Node å®šä¹‰
â”œâ”€â”€ graph_module.py       # GraphModule å®ç°
â”œâ”€â”€ interpreter.py        # Interpreter æ¨¡å¼
â”œâ”€â”€ proxy.py              # Proxy å¯¹è±¡
â”œâ”€â”€ symbolic_trace.py     # ç¬¦å·è¿½è¸ªæ ¸å¿ƒ
â”œâ”€â”€ node.py               # Node è¯¦ç»†å®ç°
â””â”€â”€ passes/               # å„ç§å›¾ä¼˜åŒ– pass
    â”œâ”€â”€ shape_prop.py     # å½¢çŠ¶æ¨å¯¼
    â”œâ”€â”€ split_module.py   # å›¾åˆ†å‰²
    â””â”€â”€ ...
```

**å…³é”®æ–‡ä»¶ï¼š**
- `torch/fx/symbolic_trace.py`ï¼šç¬¦å·è¿½è¸ªçš„ä¸»è¦é€»è¾‘
- `torch/fx/graph.py`ï¼šGraph çš„æ•°æ®ç»“æ„
- `torch/fx/graph_module.py`ï¼šGraphModule å®ç°

## 9. å°ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **TorchFX = å¯ç¼–ç¨‹çš„è®¡ç®—å›¾è¡¨ç¤º**
2. **ç¬¦å·è¿½è¸ª**ï¼šä¸æ‰§è¡Œå®é™…è®¡ç®—ï¼Œåªè®°å½•æ“ä½œ
3. **6 ç§èŠ‚ç‚¹ç±»å‹**ï¼šplaceholder, get_attr, call_function, call_method, call_module, output
4. **å›¾å˜æ¢**ï¼šå¯ä»¥è½»æ¾ä¿®æ”¹ã€ä¼˜åŒ–è®¡ç®—å›¾
5. **åœ¨ PyTorch 2.0 ä¸­ä½œä¸º IR å±‚**

### å…³é”® API

```python
# 1. è¿½è¸ª
traced = torch.fx.symbolic_trace(model)

# 2. æŸ¥çœ‹
print(traced.code)
print(traced.graph)

# 3. éå†
for node in traced.graph.nodes:
    print(node)

# 4. ä¿®æ”¹
node.target = new_function
graph.erase_node(node)

# 5. é‡æ–°ç¼–è¯‘
traced.recompile()
```

### å…¸å‹åº”ç”¨

- âœ… ç®—å­èåˆ
- âœ… é‡åŒ–
- âœ… è‡ªå®šä¹‰ä¼˜åŒ–
- âœ… å›¾åˆ†å‰²ï¼ˆæ¨¡å‹å¹¶è¡Œï¼‰
- âœ… è°ƒè¯•å’Œå¯è§†åŒ–

### ä¸‹ä¸€æ­¥

ç°åœ¨ä½ æŒæ¡äº† TorchFX è¿™ä¸ªå¼ºå¤§çš„å›¾æ“ä½œå·¥å…·ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å­¦ä¹ ï¼š

ğŸ“– [ç¬¬å››ç« ï¼šLazy Tensor å»¶è¿Ÿæ‰§è¡Œæœºåˆ¶](./04_lazy_tensor.md) - ç†è§£å»¶è¿Ÿæ‰§è¡Œçš„åŸç†

---

## ğŸ’¡ ç»ƒä¹ é¢˜

1. ç”¨ TorchFX è¿½è¸ªä¸€ä¸ªè‡ªå®šä¹‰æ¨¡å‹ï¼Œæ‰“å°æ‰€æœ‰èŠ‚ç‚¹ä¿¡æ¯
2. ç¼–å†™ä¸€ä¸ªå›¾å˜æ¢ï¼šå°†æ‰€æœ‰ `sigmoid` æ›¿æ¢ä¸º `tanh`
3. å®ç°ä¸€ä¸ª Interpreterï¼Œç»Ÿè®¡æ¯ä¸ªæ“ä½œçš„æ‰§è¡Œæ—¶é—´

**ç»§ç»­å­¦ä¹ ** â†’ [Lazy Tensor å»¶è¿Ÿæ‰§è¡Œæœºåˆ¶](./04_lazy_tensor.md)

