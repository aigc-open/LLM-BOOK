#!/usr/bin/env python3
"""
GCU Triton 1D ç®—å­æµ‹è¯•

ä½¿ç”¨ pytest æµ‹è¯•ä¸åŒç±»å‹çš„ 1D Pointwise æ“ä½œï¼š
- ç®€å• Pointwise: åŸºç¡€çš„é€å…ƒç´ æ“ä½œ
- å¤æ‚ Pointwise: å¤šå±‚èåˆæ“ä½œ
- Mixed: Linear + Pointwise æ··åˆ

ä½¿ç”¨æ–¹æ³•ï¼š
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    pytest gcu_triton_1d.py -v
    
    # åªè¿è¡Œ pointwise æµ‹è¯•
    pytest gcu_triton_1d.py -v -k "pointwise"
    
    # åªè¿è¡Œ mixed æµ‹è¯•
    pytest gcu_triton_1d.py -v -k "mixed"
    
    # è¿è¡Œå¹¶æ˜¾ç¤ºæ€§èƒ½æ•°æ®
    pytest gcu_triton_1d.py -v -s
    
    # è¿è¡Œç‰¹å®šæ¨¡å‹
    pytest gcu_triton_1d.py -v -k "SimplePointwise"
"""

# =============================================================================
# åº”ç”¨ GCU è¡¥ä¸ï¼ˆå¿…é¡»åœ¨ torch.compile ä¹‹å‰ï¼‰
# =============================================================================
import os
os.environ['TRITON_NUM_WARPS'] = '4'
os.environ['TRITON_MAX_BLOCK_SIZE'] = '256'

import gcu_patches

# =============================================================================
# æ ‡å‡†å¯¼å…¥
# =============================================================================

import torch
import torch.nn as nn
import torch.nn.functional as F
import pytest
import time
from typing import Tuple
from dataclasses import dataclass


# =============================================================================
# æµ‹è¯•é…ç½®
# =============================================================================

@dataclass
class TestConfig:
    """æµ‹è¯•é…ç½®"""
    batch_size: int = 128
    input_dim: int = 1024
    warmup: int = 20
    runs: int = 50
    rtol: float = 1e-4
    atol: float = 1e-4


CONFIG = TestConfig()


# =============================================================================
# Fixtures
# =============================================================================

@pytest.fixture(scope="module")
def device():
    """æ£€æµ‹å¹¶è¿”å›å¯ç”¨è®¾å¤‡"""
    try:
        import torch_gcu
        return "gcu"
    except ImportError:
        if torch.cuda.is_available():
            return "cuda"
        return "cpu"


@pytest.fixture(scope="module")
def config():
    """è¿”å›æµ‹è¯•é…ç½®"""
    return CONFIG


# =============================================================================
# 1D Pointwise æ¨¡å‹
# =============================================================================

class SimplePointwise(nn.Module):
    """ç®€å•çš„ Pointwise æ“ä½œ"""
    
    def __init__(self, dim: int = 1024):
        super().__init__()
        self.scale = nn.Parameter(torch.randn(dim))
        self.bias = nn.Parameter(torch.randn(dim))
    
    def forward(self, x):
        x = x * self.scale + self.bias
        x = torch.relu(x)
        return x


class ComplexPointwise(nn.Module):
    """å¤æ‚çš„ Pointwise æ“ä½œï¼ˆå¤šå±‚èåˆï¼‰"""
    
    def __init__(self, dim: int = 1024):
        super().__init__()
        self.scale1 = nn.Parameter(torch.randn(dim))
        self.bias1 = nn.Parameter(torch.randn(dim))
        self.scale2 = nn.Parameter(torch.randn(dim))
        self.bias2 = nn.Parameter(torch.randn(dim))
        self.scale3 = nn.Parameter(torch.randn(dim))
        self.bias3 = nn.Parameter(torch.randn(dim))
    
    def forward(self, x):
        # å¤šä¸ª pointwise æ“ä½œ - ä¼šè¢«èåˆæˆä¸€ä¸ª Triton kernel
        x = x * self.scale1 + self.bias1
        x = torch.relu(x)
        x = x * self.scale2 + self.bias2
        x = torch.sigmoid(x)
        x = x * self.scale3 + self.bias3
        x = torch.tanh(x)
        # æ›´å¤šæ“ä½œ
        x = x * 2.0 + 1.0
        x = torch.relu(x)
        x = x.pow(2)
        x = torch.sqrt(x + 1e-6)
        return x


class GatedPointwise(nn.Module):
    """é—¨æ§ Pointwise æ“ä½œï¼ˆGLU é£æ ¼ï¼‰"""
    
    def __init__(self, dim: int = 1024):
        super().__init__()
        self.scale = nn.Parameter(torch.ones(dim))
        self.bias = nn.Parameter(torch.zeros(dim))
        self.gate_scale = nn.Parameter(torch.ones(dim))
        self.gate_bias = nn.Parameter(torch.zeros(dim))
    
    def forward(self, x):
        # ä¸»è·¯å¾„
        main = x * self.scale + self.bias
        main = F.gelu(main)
        
        # é—¨æ§è·¯å¾„
        gate = torch.sigmoid(x * self.gate_scale + self.gate_bias)
        
        # é—¨æ§è¾“å‡º
        return main * gate


class SiLUPointwise(nn.Module):
    """SiLU (Swish) Pointwise æ“ä½œ"""
    
    def __init__(self, dim: int = 1024):
        super().__init__()
        self.scale = nn.Parameter(torch.randn(dim))
        self.bias = nn.Parameter(torch.randn(dim))
    
    def forward(self, x):
        x = x * self.scale + self.bias
        x = F.silu(x)  # x * sigmoid(x)
        x = x * 2.0 - 1.0
        return x


# =============================================================================
# Mixed æ¨¡å‹ (Linear + Pointwise)
# =============================================================================

class MixedModel(nn.Module):
    """æ··åˆæ¨¡å‹ - Linear ç”¨ extern kernel, Pointwise ç”¨ Triton"""
    
    def __init__(self, dim: int = 512):
        super().__init__()
        self.linear1 = nn.Linear(dim, dim)
        self.linear2 = nn.Linear(dim, dim)
        self.scale = nn.Parameter(torch.randn(dim))
        self.bias = nn.Parameter(torch.randn(dim))
    
    def forward(self, x):
        # Linear (extern kernel - cuBLAS)
        x = self.linear1(x)
        # Pointwise ops (Triton kernel)
        x = x * self.scale + self.bias
        x = torch.relu(x)
        x = x * 0.5 + 0.5
        x = torch.gelu(x)
        # Another Linear
        x = self.linear2(x)
        # More pointwise
        x = torch.sigmoid(x)
        x = x * 2.0 - 1.0
        return x


class LargeLinearModel(nn.Module):
    """è¾ƒå¤§çš„ Linear æ¨¡å‹ï¼ˆç”¨äºå¯¹æ¯”ï¼‰"""
    
    def __init__(self, input_dim: int = 1024):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, 1024),
            nn.ReLU(),
            nn.LayerNorm(1024),
            nn.Linear(1024, 1024),
            nn.ReLU(),
            nn.LayerNorm(1024),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.LayerNorm(512),
            nn.Linear(512, 10),
        )
    
    def forward(self, x):
        return self.network(x)


# =============================================================================
# æµ‹è¯•å·¥å…·å‡½æ•°
# =============================================================================

def sync_device(device: str):
    """åŒæ­¥è®¾å¤‡"""
    if device == "gcu":
        torch.gcu.synchronize()
    elif device == "cuda":
        torch.cuda.synchronize()


def benchmark_model(model, inputs, device: str, warmup: int = 20, runs: int = 50) -> Tuple[float, float]:
    """
    æ€§èƒ½æµ‹è¯•
    
    Returns:
        (eager_time_ms, compiled_time_ms)
    """
    compiled_model = torch.compile(model, backend="inductor")
    
    if not isinstance(inputs, tuple):
        inputs = (inputs,)
    
    # é¢„çƒ­
    with torch.no_grad():
        for _ in range(warmup):
            _ = model(*inputs)
            _ = compiled_model(*inputs)
    sync_device(device)
    
    # æµ‹è¯• eager
    times_eager = []
    with torch.no_grad():
        for _ in range(3):
            sync_device(device)
            start = time.time()
            for _ in range(runs):
                _ = model(*inputs)
            sync_device(device)
            times_eager.append((time.time() - start) / runs * 1000)
    
    # æµ‹è¯• compiled
    times_compiled = []
    with torch.no_grad():
        for _ in range(3):
            sync_device(device)
            start = time.time()
            for _ in range(runs):
                _ = compiled_model(*inputs)
            sync_device(device)
            times_compiled.append((time.time() - start) / runs * 1000)
    
    return min(times_eager), min(times_compiled)


def verify_correctness(model, inputs, device: str, rtol: float = 1e-4, atol: float = 1e-4) -> Tuple[bool, float]:
    """éªŒè¯æ­£ç¡®æ€§"""
    compiled_model = torch.compile(model, backend="inductor")
    
    if not isinstance(inputs, tuple):
        inputs = (inputs,)
    
    with torch.no_grad():
        eager_out = model(*inputs)
        compiled_out = compiled_model(*inputs)
        
        max_diff = (eager_out - compiled_out).abs().max().item()
        is_close = torch.allclose(eager_out, compiled_out, rtol=rtol, atol=atol)
    
    return is_close, max_diff


# =============================================================================
# Pytest æµ‹è¯•ç±»
# =============================================================================

class TestSimplePointwise:
    """ç®€å• Pointwise æ“ä½œæµ‹è¯•"""
    
    @pytest.mark.parametrize("model_class,name", [
        (SimplePointwise, "SimplePointwise"),
        (SiLUPointwise, "SiLUPointwise"),
    ])
    def test_simple_correctness(self, device, config, model_class, name):
        """æµ‹è¯•ç®€å• Pointwise æ¨¡å‹æ­£ç¡®æ€§"""
        model = model_class(config.input_dim).to(device).eval()
        x = torch.randn(config.batch_size, config.input_dim, device=device)
        
        is_correct, max_diff = verify_correctness(model, x, device, config.rtol, config.atol)
        
        print(f"\n[{name}] max_diff={max_diff:.2e}")
        assert is_correct, f"{name} correctness failed: max_diff={max_diff:.2e}"
    
    @pytest.mark.parametrize("model_class,name", [
        (SimplePointwise, "SimplePointwise"),
        (SiLUPointwise, "SiLUPointwise"),
    ])
    def test_simple_performance(self, device, config, model_class, name):
        """æµ‹è¯•ç®€å• Pointwise æ¨¡å‹æ€§èƒ½"""
        model = model_class(config.input_dim).to(device).eval()
        x = torch.randn(config.batch_size, config.input_dim, device=device)
        
        eager_time, compiled_time = benchmark_model(model, x, device, config.warmup, config.runs)
        speedup = eager_time / compiled_time
        
        print(f"\n[{name}] eager={eager_time:.3f}ms, compiled={compiled_time:.3f}ms, speedup={speedup:.2f}x")


class TestComplexPointwise:
    """å¤æ‚ Pointwise æ“ä½œæµ‹è¯•"""
    
    @pytest.mark.parametrize("model_class,name", [
        (ComplexPointwise, "ComplexPointwise"),
        (GatedPointwise, "GatedPointwise"),
    ])
    def test_complex_correctness(self, device, config, model_class, name):
        """æµ‹è¯•å¤æ‚ Pointwise æ¨¡å‹æ­£ç¡®æ€§"""
        model = model_class(config.input_dim).to(device).eval()
        x = torch.randn(config.batch_size, config.input_dim, device=device)
        
        is_correct, max_diff = verify_correctness(model, x, device, config.rtol, config.atol)
        
        print(f"\n[{name}] max_diff={max_diff:.2e}")
        assert is_correct, f"{name} correctness failed: max_diff={max_diff:.2e}"
    
    @pytest.mark.parametrize("model_class,name", [
        (ComplexPointwise, "ComplexPointwise"),
        (GatedPointwise, "GatedPointwise"),
    ])
    def test_complex_performance(self, device, config, model_class, name):
        """æµ‹è¯•å¤æ‚ Pointwise æ¨¡å‹æ€§èƒ½"""
        model = model_class(config.input_dim).to(device).eval()
        x = torch.randn(config.batch_size, config.input_dim, device=device)
        
        eager_time, compiled_time = benchmark_model(model, x, device, config.warmup, config.runs)
        speedup = eager_time / compiled_time
        
        print(f"\n[{name}] eager={eager_time:.3f}ms, compiled={compiled_time:.3f}ms, speedup={speedup:.2f}x")
        
        # å¤æ‚ Pointwise åº”è¯¥æœ‰è¾ƒå¥½çš„åŠ é€Ÿ
        assert speedup > 0.5, f"{name} performance regression: speedup={speedup:.2f}x"


class TestMixedModels:
    """æ··åˆæ¨¡å‹æµ‹è¯•"""
    
    def test_mixed_correctness(self, device, config):
        """æµ‹è¯• MixedModel æ­£ç¡®æ€§"""
        model = MixedModel(512).to(device).eval()
        x = torch.randn(config.batch_size, 512, device=device)
        
        is_correct, max_diff = verify_correctness(model, x, device, config.rtol, config.atol)
        
        print(f"\n[MixedModel] max_diff={max_diff:.2e}")
        assert is_correct, f"MixedModel correctness failed: max_diff={max_diff:.2e}"
    
    def test_mixed_performance(self, device, config):
        """æµ‹è¯• MixedModel æ€§èƒ½"""
        model = MixedModel(512).to(device).eval()
        x = torch.randn(config.batch_size, 512, device=device)
        
        eager_time, compiled_time = benchmark_model(model, x, device, config.warmup, config.runs)
        speedup = eager_time / compiled_time
        
        print(f"\n[MixedModel] eager={eager_time:.3f}ms, compiled={compiled_time:.3f}ms, speedup={speedup:.2f}x")
    
    def test_large_linear_correctness(self, device, config):
        """æµ‹è¯• LargeLinearModel æ­£ç¡®æ€§"""
        model = LargeLinearModel(config.input_dim).to(device).eval()
        x = torch.randn(config.batch_size, config.input_dim, device=device)
        
        is_correct, max_diff = verify_correctness(model, x, device, rtol=1e-3, atol=1e-3)
        
        print(f"\n[LargeLinearModel] max_diff={max_diff:.2e}")
        assert is_correct, f"LargeLinearModel correctness failed: max_diff={max_diff:.2e}"
    
    def test_large_linear_performance(self, device, config):
        """æµ‹è¯• LargeLinearModel æ€§èƒ½"""
        model = LargeLinearModel(config.input_dim).to(device).eval()
        x = torch.randn(config.batch_size, config.input_dim, device=device)
        
        eager_time, compiled_time = benchmark_model(model, x, device, config.warmup, config.runs)
        speedup = eager_time / compiled_time
        
        print(f"\n[LargeLinearModel] eager={eager_time:.3f}ms, compiled={compiled_time:.3f}ms, speedup={speedup:.2f}x")


# =============================================================================
# å‚æ•°åŒ–æµ‹è¯•
# =============================================================================

class TestBatchSizes:
    """æµ‹è¯•ä¸åŒ batch size"""
    
    @pytest.mark.parametrize("batch_size", [1, 8, 32, 64, 128, 256, 512])
    def test_pointwise_batch_sizes(self, device, config, batch_size):
        """æµ‹è¯•ä¸åŒ batch size ä¸‹ Pointwise çš„æ€§èƒ½"""
        model = ComplexPointwise(config.input_dim).to(device).eval()
        x = torch.randn(batch_size, config.input_dim, device=device)
        
        eager_time, compiled_time = benchmark_model(model, x, device, config.warmup, config.runs)
        speedup = eager_time / compiled_time
        
        print(f"\n[batch_size={batch_size}] eager={eager_time:.3f}ms, compiled={compiled_time:.3f}ms, speedup={speedup:.2f}x")


class TestInputDims:
    """æµ‹è¯•ä¸åŒ input dim"""
    
    @pytest.mark.parametrize("input_dim", [128, 256, 512, 1024, 2048, 4096])
    def test_pointwise_input_dims(self, device, config, input_dim):
        """æµ‹è¯•ä¸åŒ input_dim ä¸‹ Pointwise çš„æ€§èƒ½"""
        model = ComplexPointwise(input_dim).to(device).eval()
        x = torch.randn(config.batch_size, input_dim, device=device)
        
        eager_time, compiled_time = benchmark_model(model, x, device, config.warmup, config.runs)
        speedup = eager_time / compiled_time
        
        print(f"\n[input_dim={input_dim}] eager={eager_time:.3f}ms, compiled={compiled_time:.3f}ms, speedup={speedup:.2f}x")


# =============================================================================
# å…¨æ¨¡å‹å¯¹æ¯”æµ‹è¯•
# =============================================================================

class TestAllModels:
    """æ‰€æœ‰æ¨¡å‹å¯¹æ¯”æµ‹è¯•"""
    
    @pytest.mark.parametrize("model_class,name,dim", [
        (SimplePointwise, "SimplePointwise", 1024),
        (ComplexPointwise, "ComplexPointwise", 1024),
        (GatedPointwise, "GatedPointwise", 1024),
        (SiLUPointwise, "SiLUPointwise", 1024),
        (MixedModel, "MixedModel", 512),
        (LargeLinearModel, "LargeLinearModel", 1024),
    ])
    def test_all_models_comparison(self, device, config, model_class, name, dim):
        """å¯¹æ¯”æ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½"""
        model = model_class(dim).to(device).eval()
        x = torch.randn(config.batch_size, dim, device=device)
        
        is_correct, max_diff = verify_correctness(model, x, device, rtol=1e-3, atol=1e-3)
        eager_time, compiled_time = benchmark_model(model, x, device, config.warmup, config.runs)
        speedup = eager_time / compiled_time
        
        status = "âœ“" if is_correct else "âœ—"
        indicator = "ğŸš€" if speedup > 2.0 else "âœ“" if speedup > 1.0 else "âš "
        
        print(f"\n[{name}] {status} correct, speedup={speedup:.2f}x {indicator}")


# =============================================================================
# å‘½ä»¤è¡Œå…¥å£
# =============================================================================

def main():
    """ç›´æ¥è¿è¡Œæ—¶çš„å…¥å£"""
    print("=" * 70)
    print(" GCU Triton 1D ç®—å­æµ‹è¯•")
    print(" ä½¿ç”¨ pytest è¿è¡Œ: pytest gcu_triton_1d.py -v -s")
    print("=" * 70)
    
    # æ£€æµ‹è®¾å¤‡
    try:
        import torch_gcu
        device = "gcu"
        print(f"\n[Device] Using GCU")
    except ImportError:
        if torch.cuda.is_available():
            device = "cuda"
            print(f"\n[Device] Using CUDA")
        else:
            device = "cpu"
            print(f"\n[Device] Using CPU")
    
    print("\nè¿è¡Œæµ‹è¯•å‘½ä»¤:")
    print("  pytest gcu_triton_1d.py -v                    # è¿è¡Œæ‰€æœ‰æµ‹è¯•")
    print("  pytest gcu_triton_1d.py -v -k 'pointwise'     # åªæµ‹è¯• pointwise")
    print("  pytest gcu_triton_1d.py -v -k 'mixed'         # åªæµ‹è¯• mixed")
    print("  pytest gcu_triton_1d.py -v -k 'performance'   # åªæµ‹è¯•æ€§èƒ½")
    print("  pytest gcu_triton_1d.py -v -k 'correctness'   # åªæµ‹è¯•æ­£ç¡®æ€§")
    print("  pytest gcu_triton_1d.py -v -s                 # æ˜¾ç¤ºè¯¦ç»†è¾“å‡º")
    
    # å¿«é€Ÿæ¼”ç¤º
    print("\n" + "=" * 70)
    print(" å¿«é€Ÿæ¼”ç¤ºæµ‹è¯•")
    print("=" * 70)
    
    config = CONFIG
    
    models = [
        (SimplePointwise, "SimplePointwise", config.input_dim),
        (ComplexPointwise, "ComplexPointwise", config.input_dim),
        (GatedPointwise, "GatedPointwise", config.input_dim),
        (MixedModel, "MixedModel", 512),
    ]
    
    print(f"\n{'Model':<20} {'Correct':<10} {'Eager(ms)':<12} {'Compiled(ms)':<14} {'Speedup':<10}")
    print("-" * 70)
    
    for model_class, name, dim in models:
        model = model_class(dim).to(device).eval()
        x = torch.randn(config.batch_size, dim, device=device)
        
        is_correct, max_diff = verify_correctness(model, x, device)
        eager_time, compiled_time = benchmark_model(model, x, device)
        speedup = eager_time / compiled_time
        
        status = "âœ“" if is_correct else "âœ—"
        print(f"{name:<20} {status:<10} {eager_time:<12.3f} {compiled_time:<14.3f} {speedup:<10.2f}x")
    
    print("-" * 70)


if __name__ == "__main__":
    main()
