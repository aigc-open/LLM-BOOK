# ç¬¬åç« ï¼šä»£ç ç”Ÿæˆæœºåˆ¶ä¸å¯å‘å¼ä¼˜åŒ–

## ğŸ“– æœ¬ç« æ¦‚è¦

æœ¬ç« æ·±å…¥è®²è§£ TorchInductor å¦‚ä½•ç”Ÿæˆé«˜æ•ˆçš„ Triton/C++ ä»£ç ï¼Œä»¥åŠå¦‚ä½•é€šè¿‡å¯å‘å¼ç­–ç•¥ï¼ˆHeuristicsï¼‰è¿›è¡Œæ€§èƒ½ä¼˜åŒ–ã€‚æ‚¨å°†äº†è§£ï¼š
- TorchInductor çš„ä»£ç ç”Ÿæˆæµç¨‹
- Triton Kernel çš„å‚æ•°é…ç½®ç­–ç•¥ï¼ˆgrid sizeã€block sizeã€num_warpsï¼‰
- AutoTuning è‡ªåŠ¨è°ƒä¼˜æœºåˆ¶
- å¦‚ä½•ä¸ºç‰¹å®šç¡¬ä»¶å®šåˆ¶ä»£ç ç”Ÿæˆç­–ç•¥

## ç›®å½•

1. [ä»£ç ç”Ÿæˆå…¨æµç¨‹](#1-ä»£ç ç”Ÿæˆå…¨æµç¨‹)
2. [æ·±å…¥ï¼šçœŸå®çš„Kernelä»£ç ç”Ÿæˆ](#2-æ·±å…¥çœŸå®çš„kernelä»£ç ç”Ÿæˆ) â­ **æ ¸å¿ƒ**
   - [å®Œæ•´ç¤ºä¾‹ï¼šé€æ­¥è¿½è¸ª](#22-å®Œæ•´ç¤ºä¾‹é€æ­¥è¿½è¸ª)
   - [è¯¦ç»†æ­¥éª¤æ‹†è§£](#23-è¯¦ç»†æ­¥éª¤æ‹†è§£)
   - [å®Œæ•´æ—¶é—´çº¿æ€»ç»“](#24-å®Œæ•´æ—¶é—´çº¿æ€»ç»“)
   - [å…³é”®æ•°æ®æµ](#25-å…³é”®æ•°æ®æµ)
   - [ä¸åŒæ“ä½œç±»å‹çš„ä»£ç ç”Ÿæˆ](#26-ä¸åŒæ“ä½œç±»å‹çš„ä»£ç ç”Ÿæˆ)
3. [Triton Kernel å‚æ•°è¯¦è§£](#3-triton-kernel-å‚æ•°è¯¦è§£)
4. [å¯å‘å¼ä¼˜åŒ–ç­–ç•¥](#4-å¯å‘å¼ä¼˜åŒ–ç­–ç•¥)
5. [Grid Size ä¸å¾ªç¯ç­–ç•¥](#5-grid-size-ä¸å¾ªç¯ç­–ç•¥)
6. [Block Size ä¸ Num_Warps é…ç½®](#6-block-size-ä¸-num_warps-é…ç½®)
7. [AutoTuning æœºåˆ¶](#7-autotuning-æœºåˆ¶)
8. [å®æˆ˜ï¼šè‡ªå®šä¹‰ä»£ç ç”Ÿæˆç­–ç•¥](#8-å®æˆ˜è‡ªå®šä¹‰ä»£ç ç”Ÿæˆç­–ç•¥)
9. [æ€§èƒ½åˆ†æä¸è°ƒä¼˜](#9-æ€§èƒ½åˆ†æä¸è°ƒä¼˜)
10. [å¸¸è§é—®é¢˜](#10-å¸¸è§é—®é¢˜)
11. [æ€»ç»“](#11-æ€»ç»“)

---

## 1. ä»£ç ç”Ÿæˆå…¨æµç¨‹

### 1.1 ä» FX Graph åˆ°å¯æ‰§è¡Œä»£ç 

```
ç”¨æˆ·æ¨¡å‹
    â†“
torch.compile
    â†“
[1] TorchDynamo: æ•è·è®¡ç®—å›¾
    â†“
[2] AOTAutograd: å‰å‘/åå‘åˆ†ç¦»
    â†“
[3] TorchInductor: ä»£ç ç”Ÿæˆ
    â”œâ”€â†’ [3.1] Lowering: é«˜å±‚ IR â†’ ä½å±‚ IR
    â”œâ”€â†’ [3.2] Fusion: ç®—å­èåˆå†³ç­–
    â”œâ”€â†’ [3.3] Scheduling: ç”Ÿæˆè°ƒåº¦ä»£ç 
    â”‚         â”œâ”€â†’ TritonScheduling
    â”‚         â”œâ”€â†’ CppScheduling
    â”‚         â””â”€â†’ ExternKernel (è°ƒç”¨å¤–éƒ¨åº“)
    â”œâ”€â†’ [3.4] Code Emission: ç”Ÿæˆå†…æ ¸ä»£ç 
    â”‚         â”œâ”€â†’ Triton Code
    â”‚         â”œâ”€â†’ C++ Code
    â”‚         â””â”€â†’ CUDA Code
    â””â”€â†’ [3.5] Wrapper Generation: ç”ŸæˆåŒ…è£…ä»£ç 
              â””â”€â†’ Python/C++ Wrapper
    â†“
[4] ç¼–è¯‘ä¸åŠ è½½
    â”œâ”€â†’ Triton Compiler â†’ PTX/LLVM IR
    â”œâ”€â†’ C++ Compiler â†’ .so
    â””â”€â†’ åŠ¨æ€åŠ è½½åˆ° Python è¿›ç¨‹
    â†“
æ‰§è¡Œ
```

### 1.2 æ ¸å¿ƒæ•°æ®ç»“æ„

```python
# torch/_inductor/ir.py
class IRNode:
    """IR èŠ‚ç‚¹åŸºç±»"""
    def get_size(self) -> List[sympy.Expr]:
        """è¿”å›å¼ é‡å½¢çŠ¶"""
        pass
    
    def get_dtype(self) -> torch.dtype:
        """è¿”å›æ•°æ®ç±»å‹"""
        pass
    
    def get_device(self) -> torch.device:
        """è¿”å›è®¾å¤‡ç±»å‹"""
        pass

class SchedulerNode:
    """è°ƒåº¦èŠ‚ç‚¹ï¼šåŒ…å« IR + è°ƒåº¦ä¿¡æ¯"""
    def __init__(self, scheduler, node: IRNode):
        self.scheduler = scheduler
        self.node = node
        self.users = []  # ä½¿ç”¨è¯¥èŠ‚ç‚¹çš„èŠ‚ç‚¹åˆ—è¡¨
        self.group = None  # èåˆç»„
        
    def can_fuse(self, other: 'SchedulerNode') -> bool:
        """åˆ¤æ–­æ˜¯å¦å¯ä»¥ä¸å…¶ä»–èŠ‚ç‚¹èåˆ"""
        pass
```

### 1.3 ä»£ç ç”Ÿæˆå…¥å£

```python
# torch/_inductor/compile_fx.py
def compile_fx_inner(gm: torch.fx.GraphModule, example_inputs):
    """ç¼–è¯‘ FX å›¾"""
    with V.set_graph_handler(GraphLowering(gm)):
        # [1] Lowering: å°† FX Graph è½¬æ¢ä¸º IR
        graph = V.graph
        graph.run(*example_inputs)
        
        # [2] Scheduling: ç”Ÿæˆè°ƒåº¦è®¡åˆ’
        compiled_graph = graph.compile_to_fn()
        
    return compiled_graph

# torch/_inductor/graph.py
class GraphLowering:
    def compile_to_fn(self):
        """ç”Ÿæˆå¯æ‰§è¡Œå‡½æ•°"""
        # [1] åˆ›å»ºè°ƒåº¦å™¨
        from torch._inductor.scheduler import Scheduler
        self.scheduler = Scheduler(self.buffers)
        
        # [2] èåˆå†³ç­–
        self.scheduler.codegen()
        
        # [3] ç”Ÿæˆ Python wrapper
        return self.compile_to_module().call
```

---

## 2. æ·±å…¥ï¼šçœŸå®çš„Kernelä»£ç ç”Ÿæˆ

è¿™ä¸€èŠ‚æˆ‘ä»¬æ·±å…¥åˆ°æºç çº§åˆ«ï¼Œçœ‹çœ‹TorchInductoræ˜¯å¦‚ä½•çœŸæ­£ç”ŸæˆTriton kernelä»£ç çš„ã€‚

### 2.1 ä»IRèŠ‚ç‚¹åˆ°Tritonä»£ç çš„å®Œæ•´æµç¨‹

```python
# å®Œæ•´çš„ä»£ç ç”Ÿæˆæµç¨‹
FX Graph Node
    â†“
[1] Lowering (torch/_inductor/lowering.py)
    â†“
IRNode (torch/_inductor/ir.py)
    â”œâ”€â†’ Pointwise (é€ç‚¹æ“ä½œ)
    â”œâ”€â†’ Reduction (å½’çº¦æ“ä½œ)
    â”œâ”€â†’ TensorBox (å¼ é‡å¼•ç”¨)
    â””â”€â†’ ComputedBuffer (è®¡ç®—ç¼“å†²åŒº)
    â†“
[2] Scheduling (torch/_inductor/scheduler.py)
    â†“
FusedSchedulerNode (èåˆèŠ‚ç‚¹ç»„)
    â”œâ”€â†’ snodes: List[SchedulerNode]
    â””â”€â†’ åŒ…å«å¤šä¸ªå¯èåˆçš„æ“ä½œ
    â†“
[3] Code Generation (torch/_inductor/codegen/triton.py)
    â†“
TritonKernel
    â”œâ”€â†’ args: å‚æ•°åˆ—è¡¨
    â”œâ”€â†’ loads: å†…å­˜åŠ è½½æ“ä½œ
    â”œâ”€â†’ stores: å†…å­˜å­˜å‚¨æ“ä½œ
    â”œâ”€â†’ compute: è®¡ç®—é€»è¾‘
    â””â”€â†’ indexing: ç´¢å¼•è®¡ç®—
    â†“
[4] Code Emission
    â†“
Triton Source Code (å­—ç¬¦ä¸²)
    â†“
[5] Triton Compilation
    â†“
PTX / LLVM IR
    â†“
GPU Executable
```

### 2.2 å®Œæ•´ç¤ºä¾‹ï¼šé€æ­¥è¿½è¸ªä»£ç ç”Ÿæˆ

è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå…·ä½“ä¾‹å­ï¼Œ**å®Œæ•´è¿½è¸ª**ä»Pythonä»£ç åˆ°GPUæ‰§è¡Œçš„æ¯ä¸€æ­¥ï¼š

```python
# ç”¨æˆ·ä»£ç 
import torch

@torch.compile
def model(x, y):
    z = x + y
    return z.relu()

# è¿è¡Œ
x = torch.randn(1024, device='cuda')
y = torch.randn(1024, device='cuda')
result = model(x, y)
```

#### ğŸ” æ‰§è¡Œæµç¨‹æ—¶é—´çº¿

```
t=0ms:  ç”¨æˆ·è°ƒç”¨ model(x, y)
        â†“
t=1ms:  TorchDynamo æ‹¦æˆªå‡½æ•°è°ƒç”¨
        â†“
t=2ms:  å­—èŠ‚ç åˆ†æ + FX Graph æ•è·
        â†“
t=50ms: AOTAutograd å¤„ç†
        â†“
t=51ms: TorchInductor ä»£ç ç”Ÿæˆ
        â”œâ”€â†’ Lowering (51-52ms)
        â”œâ”€â†’ Fusion (52-53ms)
        â”œâ”€â†’ Code Generation (53-55ms)
        â””â”€â†’ Triton Compilation (55-150ms)
        â†“
t=150ms: åŠ¨æ€åŠ è½½ç¼–è¯‘ç»“æœ
        â†“
t=151ms: é¦–æ¬¡æ‰§è¡Œï¼ˆç¼–è¯‘å®Œæˆï¼‰
        â†“
t=152ms: ç¬¬äºŒæ¬¡è°ƒç”¨ model(x, y)
        â””â”€â†’ ç›´æ¥æ‰§è¡Œç¼–è¯‘å¥½çš„ä»£ç ï¼ˆ~0.1msï¼‰
```

### 2.3 è¯¦ç»†æ­¥éª¤æ‹†è§£

#### æ­¥éª¤0ï¼šè£…é¥°å™¨æ³¨å†Œ

```python
# å½“ä½ å†™ @torch.compile æ—¶
@torch.compile
def model(x, y):
    z = x + y
    return z.relu()

# ç­‰ä»·äº
model = torch.compile(model)

# torch.compile åšäº†ä»€ä¹ˆï¼Ÿ
def torch_compile(fn):
    """
    è¿”å›ä¸€ä¸ªåŒ…è£…å‡½æ•°ï¼Œåœ¨é¦–æ¬¡è°ƒç”¨æ—¶è§¦å‘ç¼–è¯‘
    """
    compiled_fn = None
    
    def wrapper(*args, **kwargs):
        nonlocal compiled_fn
        
        # [å…³é”®] é¦–æ¬¡è°ƒç”¨æ—¶ç¼–è¯‘
        if compiled_fn is None:
            print("[Compile] å¼€å§‹ç¼–è¯‘...")
            compiled_fn = _compile_impl(fn, args, kwargs)
        
        # æ‰§è¡Œç¼–è¯‘åçš„å‡½æ•°
        return compiled_fn(*args, **kwargs)
    
    return wrapper
```

#### æ­¥éª¤1ï¼šé¦–æ¬¡è°ƒç”¨ - TorchDynamoæ‹¦æˆªï¼ˆt=1msï¼‰

```python
# å½“æ‰§è¡Œ result = model(x, y) æ—¶

# [1.1] TorchDynamo æ‹¦æˆª
# torch/_dynamo/eval_frame.py
def _compile(
    fn,
    args,
    kwargs,
    compiler_fn,
):
    """
    TorchDynamo çš„æ ¸å¿ƒï¼šæ‹¦æˆª Python å­—èŠ‚ç 
    """
    # [1] è·å–å‡½æ•°çš„å­—èŠ‚ç 
    code = fn.__code__
    print(f"[Dynamo] æ‹¦æˆªå‡½æ•°: {fn.__name__}")
    print(f"[Dynamo] å­—èŠ‚ç æŒ‡ä»¤æ•°: {len(code.co_code)}")
    
    # [2] åˆ›å»ºå­—èŠ‚ç åˆ†æå™¨
    from torch._dynamo.symbolic_convert import InstructionTranslator
    tracer = InstructionTranslator(
        instructions=dis.get_instructions(code),
        f_locals=fn.__globals__,
    )
    
    # [3] é€æ¡æ‰§è¡Œå­—èŠ‚ç ï¼Œæ„å»ºè®¡ç®—å›¾
    graph = tracer.run()
    
    return graph
```

**å®é™…çš„å­—èŠ‚ç ï¼ˆä½¿ç”¨ `dis.dis(model)` æŸ¥çœ‹ï¼‰**ï¼š

```python
import dis
dis.dis(model)

# è¾“å‡ºï¼š
#   2           0 LOAD_FAST                0 (x)
#               2 LOAD_FAST                1 (y)
#               4 BINARY_ADD
#               6 STORE_FAST               2 (z)
#
#   3           8 LOAD_FAST                2 (z)
#              10 LOAD_ATTR                0 (relu)
#              12 CALL_FUNCTION            0
#              14 RETURN_VALUE
```

**TorchDynamo å¦‚ä½•å¤„ç†è¿™äº›å­—èŠ‚ç **ï¼š

```python
# torch/_dynamo/symbolic_convert.py
class InstructionTranslator:
    def run(self):
        """æ‰§è¡Œå­—èŠ‚ç ï¼Œæ„å»ºFX Graph"""
        for inst in self.instructions:
            handler = getattr(self, inst.opname, None)
            if handler:
                handler(inst)
        
        return self.output.graph
    
    def LOAD_FAST(self, inst):
        """å¤„ç† LOAD_FAST æŒ‡ä»¤ï¼ˆåŠ è½½å±€éƒ¨å˜é‡ï¼‰"""
        var_name = inst.argval  # 'x' æˆ– 'y'
        # åˆ›å»ºç¬¦å·å˜é‡
        var = VariableTracker.build(
            self.f_locals[var_name]
        )
        self.stack.append(var)
        print(f"[Dynamo] LOAD_FAST: {var_name}")
    
    def BINARY_ADD(self, inst):
        """å¤„ç† BINARY_ADD æŒ‡ä»¤ï¼ˆåŠ æ³•ï¼‰"""
        # ä»æ ˆä¸­å¼¹å‡ºä¸¤ä¸ªæ“ä½œæ•°
        right = self.stack.pop()  # y
        left = self.stack.pop()   # x
        
        # è®°å½•æ“ä½œåˆ°å›¾ä¸­
        result = self.output.create_node(
            'call_function',
            torch.ops.aten.add.Tensor,
            args=(left, right),
        )
        self.stack.append(result)
        print(f"[Dynamo] BINARY_ADD: {left} + {right}")
    
    def LOAD_ATTR(self, inst):
        """å¤„ç† LOAD_ATTR æŒ‡ä»¤ï¼ˆå±æ€§è®¿é—®ï¼‰"""
        obj = self.stack.pop()  # z
        attr_name = inst.argval  # 'relu'
        
        # è®°å½•å±æ€§è®¿é—®
        method = getattr(obj, attr_name)
        self.stack.append(method)
        print(f"[Dynamo] LOAD_ATTR: {obj}.{attr_name}")
    
    def CALL_FUNCTION(self, inst):
        """å¤„ç† CALL_FUNCTION æŒ‡ä»¤ï¼ˆå‡½æ•°è°ƒç”¨ï¼‰"""
        num_args = inst.argval  # 0ï¼ˆreluæ²¡æœ‰å‚æ•°ï¼‰
        
        # å¼¹å‡ºå‡½æ•°å’Œå‚æ•°
        fn = self.stack.pop()  # relu method
        args = [self.stack.pop() for _ in range(num_args)]
        
        # è®°å½•å‡½æ•°è°ƒç”¨
        result = self.output.create_node(
            'call_method',
            'relu',
            args=(fn.__self__,),  # selfæ˜¯z
        )
        self.stack.append(result)
        print(f"[Dynamo] CALL_FUNCTION: {fn}({args})")
```

#### æ­¥éª¤2ï¼šFX Graphç”Ÿæˆï¼ˆt=2msï¼‰

```python
# ç”Ÿæˆçš„FX Graph
# torch.fx.GraphModule
class GraphModule(torch.nn.Module):
    def forward(self, x, y):
        # èŠ‚ç‚¹1: add
        add_tensor = torch.ops.aten.add.Tensor(x, y)
        
        # èŠ‚ç‚¹2: relu
        relu_default = torch.ops.aten.relu.default(add_tensor)
        
        return relu_default

# æ‰“å°å›¾ç»“æ„
print(graph.graph)
# è¾“å‡ºï¼š
# graph():
#     %x : [num_users=1] = placeholder[target=x]
#     %y : [num_users=1] = placeholder[target=y]
#     %add_tensor : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%x, %y), kwargs = {})
#     %relu_default : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%add_tensor,), kwargs = {})
#     return relu_default
```

**å›¾çš„å¯è§†åŒ–**ï¼š

```
è¾“å…¥: x [1024]    è¾“å…¥: y [1024]
    â†“                  â†“
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
    aten.add.Tensor
           â†“
      add_tensor [1024]
           â†“
    aten.relu.default
           â†“
      relu_default [1024]
           â†“
         è¾“å‡º
```

#### æ­¥éª¤3ï¼šAOTAutogradå¤„ç†ï¼ˆt=50msï¼‰

```python
# torch/_functorch/aot_autograd.py
def aot_function(fn, fw_compiler):
    """
    Ahead-Of-Time Autograd
    åˆ†ç¦»å‰å‘å’Œåå‘å›¾
    """
    def compiled_fn(*args):
        # [1] è¿è¡Œå‰å‘ï¼Œè®°å½•æ“ä½œ
        with torch.enable_grad():
            # æ ‡è®°è¾“å…¥éœ€è¦æ¢¯åº¦
            args_with_grad = [
                arg.requires_grad_(True) if isinstance(arg, torch.Tensor) 
                else arg for arg in args
            ]
            
            # è¿è¡Œå‰å‘
            out = fn(*args_with_grad)
            
            # [2] æ„å»ºåå‘å›¾
            # ï¼ˆæœ¬ä¾‹ä¸éœ€è¦åå‘ï¼Œç®€åŒ–å¤„ç†ï¼‰
            
            # [3] ç¼–è¯‘å‰å‘å›¾
            compiled_fw = fw_compiler(
                forward_graph,
                args_with_grad,
            )
        
        return compiled_fw(*args)
    
    return compiled_fn

# å¯¹äºæˆ‘ä»¬çš„ä¾‹å­ï¼ŒAOTAutogradä¸»è¦åšï¼š
# 1. ç¡®è®¤ä¸éœ€è¦æ¢¯åº¦ï¼ˆx, y æ²¡æœ‰ requires_grad=Trueï¼‰
# 2. å°†FX Graphä¼ é€’ç»™TorchInductor
```

#### æ­¥éª¤4ï¼šTorchInductor Loweringï¼ˆt=51-52msï¼‰

è¿™æ˜¯**å…³é”®æ­¥éª¤**ï¼å°†é«˜å±‚çš„ `aten.add` å’Œ `aten.relu` è½¬æ¢ä¸ºä½å±‚IRã€‚

```python
# torch/_inductor/lowering.py

# [4.1] æ³¨å†Œ lowering è§„åˆ™
@register_lowering(torch.ops.aten.add)
def add_tensor(x, y):
    """
    å°† aten.add è½¬æ¢ä¸º Pointwise IR
    
    è¾“å…¥: x, y æ˜¯ TensorBoxï¼ˆåŒ…è£…çš„å¼ é‡å¼•ç”¨ï¼‰
    è¾“å‡º: Pointwise IRèŠ‚ç‚¹
    """
    print(f"[Lowering] add: x.shape={x.get_size()}, y.shape={y.get_size()}")
    
    def inner_fn(idx):
        """
        æ ¸å¿ƒè®¡ç®—é€»è¾‘
        idx: ç¬¦å·ç´¢å¼•ï¼Œä¾‹å¦‚ (i,) å¯¹äº1Då¼ é‡
        """
        # åŠ è½½ x[idx]
        x_val = ops.load(x, idx)
        # åŠ è½½ y[idx]
        y_val = ops.load(y, idx)
        # è®¡ç®— x[idx] + y[idx]
        result = ops.add(x_val, y_val)
        return result
    
    # åˆ›å»º Pointwise IR èŠ‚ç‚¹
    return Pointwise.create(
        device=x.get_device(),     # 'cuda'
        dtype=x.get_dtype(),       # torch.float32
        inner_fn=inner_fn,         # ä¸Šé¢å®šä¹‰çš„lambda
        ranges=list(x.get_size()), # [1024]
    )

@register_lowering(torch.ops.aten.relu)
def relu(x):
    """
    å°† aten.relu è½¬æ¢ä¸º Pointwise IR
    """
    print(f"[Lowering] relu: x.shape={x.get_size()}")
    
    def inner_fn(idx):
        # åŠ è½½ x[idx]
        x_val = ops.load(x, idx)
        # è®¡ç®— max(x[idx], 0)
        zero = ops.constant(0.0, x.get_dtype())
        result = ops.maximum(x_val, zero)
        return result
    
    return Pointwise.create(
        device=x.get_device(),
        dtype=x.get_dtype(),
        inner_fn=inner_fn,
        ranges=list(x.get_size()),
    )

# [4.2] æ‰§è¡Œ Lowering
# torch/_inductor/graph.py
class GraphLowering:
    def run(self, *args):
        """éå†FX Graphï¼Œè°ƒç”¨å¯¹åº”çš„loweringå‡½æ•°"""
        for node in self.graph.nodes:
            if node.op == 'call_function':
                # æŸ¥æ‰¾å¯¹åº”çš„loweringå‡½æ•°
                lowering_fn = lowerings.get(node.target)
                
                # è°ƒç”¨lowering
                ir_result = lowering_fn(*node.args, **node.kwargs)
                
                # ä¿å­˜IRèŠ‚ç‚¹
                self.env[node.name] = ir_result
                
                print(f"[Lowering] {node.name} -> {type(ir_result).__name__}")
```

**Loweringåçš„IRç»“æ„**ï¼š

```python
# IR Node 1: add
ir_add = Pointwise(
    device='cuda',
    dtype=torch.float32,
    inner_fn=lambda idx: ops.add(
        ops.load(x, idx),
        ops.load(y, idx)
    ),
    ranges=[1024],
    name='buf0',
)

# IR Node 2: relu
ir_relu = Pointwise(
    device='cuda',
    dtype=torch.float32,
    inner_fn=lambda idx: ops.maximum(
        ops.load(buf0, idx),  # ä¾èµ– buf0ï¼ˆaddçš„è¾“å‡ºï¼‰
        ops.constant(0.0, torch.float32)
    ),
    ranges=[1024],
    name='buf1',
)
```

#### æ­¥éª¤5ï¼šFusionå†³ç­–ï¼ˆt=52-53msï¼‰

```python
# torch/_inductor/scheduler.py
class Scheduler:
    def fusion_pass(self):
        """ç®—å­èåˆå†³ç­–"""
        print("[Fusion] å¼€å§‹èåˆåˆ†æ...")
        
        # [5.1] ä¸ºæ¯ä¸ªIRèŠ‚ç‚¹åˆ›å»ºSchedulerNode
        self.nodes = []
        for buf_name, ir_node in self.buffers.items():
            snode = SchedulerNode(self, ir_node)
            snode.name = buf_name
            self.nodes.append(snode)
        
        print(f"[Fusion] å‘ç° {len(self.nodes)} ä¸ªèŠ‚ç‚¹")
        
        # [5.2] æ„å»ºä¾èµ–å…³ç³»
        for snode in self.nodes:
            # åˆ†æ inner_fnï¼Œæå–è¯»å–çš„ç¼“å†²åŒº
            reads = snode.node.get_reads()
            for read_buf in reads:
                # æ‰¾åˆ°ç”Ÿäº§è€…èŠ‚ç‚¹
                producer = self.name_to_node[read_buf.name]
                snode.read_from.add(producer)
                producer.users.append(snode)
        
        print("[Fusion] ä¾èµ–å…³ç³»:")
        print("  buf0 (add) -> buf1 (relu)")
        
        # [5.3] å°è¯•èåˆ
        for consumer in self.nodes:
            for producer in list(consumer.read_from):
                if self.can_fuse(producer, consumer):
                    print(f"[Fusion] èåˆ: {producer.name} + {consumer.name}")
                    self.fuse(producer, consumer)
    
    def can_fuse(self, producer, consumer):
        """åˆ¤æ–­æ˜¯å¦å¯ä»¥èåˆ"""
        # æ¡ä»¶1: éƒ½æ˜¯Pointwise
        if not (isinstance(producer.node, Pointwise) and 
                isinstance(consumer.node, Pointwise)):
            print(f"[Fusion] ä¸èƒ½èåˆ: ä¸éƒ½æ˜¯Pointwise")
            return False
        
        # æ¡ä»¶2: produceråªæœ‰ä¸€ä¸ªä½¿ç”¨è€…ï¼ˆconsumerï¼‰
        if len(producer.users) != 1:
            print(f"[Fusion] ä¸èƒ½èåˆ: produceræœ‰å¤šä¸ªä½¿ç”¨è€…")
            return False
        
        # æ¡ä»¶3: å½¢çŠ¶ç›¸åŒ
        if producer.node.get_size() != consumer.node.get_size():
            print(f"[Fusion] ä¸èƒ½èåˆ: å½¢çŠ¶ä¸åŒ¹é…")
            return False
        
        print(f"[Fusion] âœ“ å¯ä»¥èåˆ")
        return True
    
    def fuse(self, producer, consumer):
        """èåˆä¸¤ä¸ªèŠ‚ç‚¹"""
        # åˆ›å»ºèåˆåçš„inner_fn
        def fused_inner_fn(idx):
            # å†…è”producerçš„è®¡ç®—
            producer_result = producer.node.inner_fn(idx)
            
            # æ›¿æ¢consumerä¸­å¯¹producerçš„loadä¸ºç›´æ¥ä½¿ç”¨ç»“æœ
            # consumer.inner_fn ä¸­çš„ ops.load(producer, idx) 
            # æ›¿æ¢ä¸º producer_result
            
            # è¿™é‡Œç®€åŒ–è¡¨ç¤º
            return consumer.node.inner_fn_with_inline(
                producer_result, idx
            )
        
        # åˆ›å»ºèåˆèŠ‚ç‚¹
        fused_node = Pointwise.create(
            device=consumer.node.get_device(),
            dtype=consumer.node.get_dtype(),
            inner_fn=fused_inner_fn,
            ranges=consumer.node.get_size(),
        )
        
        # æ›´æ–°å›¾
        self.replace_node(consumer, fused_node)
        self.remove_node(producer)
```

**èåˆåçš„IR**ï¼š

```python
# èåˆååªæœ‰ä¸€ä¸ª Pointwise èŠ‚ç‚¹
fused_ir = Pointwise(
    device='cuda',
    dtype=torch.float32,
    inner_fn=lambda idx: ops.maximum(
        ops.add(
            ops.load(x, idx),    # æ¥è‡ªadd
            ops.load(y, idx)     # æ¥è‡ªadd
        ),                       # addçš„ç»“æœ
        ops.constant(0.0, torch.float32)  # æ¥è‡ªrelu
    ),
    ranges=[1024],
    name='buf_fused',
)
```

#### æ­¥éª¤6ï¼šç”ŸæˆTritonä»£ç ï¼ˆt=53-55msï¼‰

è¿™æ˜¯æœ€ç²¾å½©çš„éƒ¨åˆ†ï¼å°†IRè½¬æ¢ä¸ºå®é™…çš„Tritonæºä»£ç ã€‚

```python
# torch/_inductor/codegen/triton.py
class TritonScheduling:
    def codegen(self):
        """ä¸ºæ‰€æœ‰èåˆåçš„èŠ‚ç‚¹ç”Ÿæˆä»£ç """
        print("[Codegen] å¼€å§‹ç”ŸæˆTritonä»£ç ...")
        
        for node in self.scheduled_nodes:
            kernel = self.create_kernel(node)
            kernel_code = kernel.codegen()
            self.kernels.append(kernel_code)
    
    def create_kernel(self, node):
        """ä¸ºå•ä¸ªèŠ‚ç‚¹åˆ›å»ºTriton Kernel"""
        kernel = TritonKernel(node)
        return kernel

class TritonKernel:
    def __init__(self, node):
        self.node = node
        self.args = IndentedBuffer()
        self.indexing = IndentedBuffer()
        self.loads = IndentedBuffer()
        self.compute = IndentedBuffer()
        self.stores = IndentedBuffer()
        
        # ä¸´æ—¶å˜é‡è®¡æ•°å™¨
        self.tmp_counter = 0
    
    def new_tmp_var(self):
        """åˆ†é…æ–°çš„ä¸´æ—¶å˜é‡"""
        var = f"tmp{self.tmp_counter}"
        self.tmp_counter += 1
        return var
    
    def codegen(self):
        """ç”Ÿæˆå®Œæ•´çš„kernelä»£ç """
        print(f"[Codegen] å¤„ç†èŠ‚ç‚¹: {self.node.name}")
        
        # [1] ç”Ÿæˆå‚æ•°åˆ—è¡¨
        self.codegen_args()
        
        # [2] ç”Ÿæˆç´¢å¼•è®¡ç®—
        self.codegen_indexing()
        
        # [3] ç”Ÿæˆè®¡ç®—ä»£ç ï¼ˆéå†inner_fnï¼‰
        result_var = self.codegen_inner_fn()
        
        # [4] ç”Ÿæˆstore
        self.codegen_store(result_var)
        
        # [5] ç»„è£…å®Œæ•´ä»£ç 
        return self.assemble()
    
    def codegen_args(self):
        """ç”Ÿæˆå‚æ•°åˆ—è¡¨"""
        # è¾“å…¥ç¼“å†²åŒº
        for inp in self.node.get_inputs():
            self.args.writeline(f"in_ptr{inp.index},")
        
        # è¾“å‡ºç¼“å†²åŒº
        self.args.writeline("out_ptr0,")
        
        # å…ƒç´ æ•°é‡
        self.args.writeline("xnumel,")
        
        print(f"[Codegen] å‚æ•°: 2ä¸ªè¾“å…¥ + 1ä¸ªè¾“å‡º + 1ä¸ªsize")
    
    def codegen_indexing(self):
        """ç”Ÿæˆç´¢å¼•è®¡ç®—ä»£ç """
        self.indexing.writeline("# è®¡ç®—ç´¢å¼•")
        self.indexing.writeline("pid = tl.program_id(0)")
        self.indexing.writeline("xoffset = pid * XBLOCK")
        self.indexing.writeline("xindex = xoffset + tl.arange(0, XBLOCK)")
        self.indexing.writeline("xmask = xindex < xnumel")
        
        print(f"[Codegen] ç´¢å¼•: 1D, range=[{self.node.get_size()[0]}]")
    
    def codegen_inner_fn(self):
        """
        ç”Ÿæˆinner_fnçš„ä»£ç 
        è¿™æ˜¯æœ€æ ¸å¿ƒçš„éƒ¨åˆ†ï¼
        """
        # æˆ‘ä»¬çš„èåˆåçš„inner_fnæ˜¯:
        # lambda idx: ops.maximum(
        #     ops.add(
        #         ops.load(x, idx),
        #         ops.load(y, idx)
        #     ),
        #     ops.constant(0.0, torch.float32)
        # )
        
        # ä½¿ç”¨ç¬¦å·æ‰§è¡Œ
        symbolic_idx = sympy.Symbol('xindex')
        expr = self.node.inner_fn(symbolic_idx)
        
        # é€’å½’ç”Ÿæˆè¡¨è¾¾å¼æ ‘
        result_var = self.codegen_expr(expr)
        
        return result_var
    
    def codegen_expr(self, expr):
        """é€’å½’ç”Ÿæˆè¡¨è¾¾å¼çš„Tritonä»£ç """
        print(f"[Codegen] å¤„ç†è¡¨è¾¾å¼: {type(expr).__name__}")
        
        if isinstance(expr, ops.Load):
            # === Load æ“ä½œ ===
            buffer_name = expr.name  # 'in_ptr0' æˆ– 'in_ptr1'
            index = 'xindex'  # ç®€åŒ–ï¼Œå®é™…ä¼šè®¡ç®—å¤æ‚ç´¢å¼•
            
            tmp_var = self.new_tmp_var()  # tmp0
            self.loads.writeline(
                f"{tmp_var} = tl.load({buffer_name} + {index}, xmask)"
            )
            print(f"[Codegen]   Load: {buffer_name}[{index}] -> {tmp_var}")
            return tmp_var
        
        elif isinstance(expr, ops.Add):
            # === Add æ“ä½œ ===
            lhs_var = self.codegen_expr(expr.lhs)  # é€’å½’å¤„ç†å·¦æ“ä½œæ•°
            rhs_var = self.codegen_expr(expr.rhs)  # é€’å½’å¤„ç†å³æ“ä½œæ•°
            
            tmp_var = self.new_tmp_var()  # tmp2
            self.compute.writeline(f"{tmp_var} = {lhs_var} + {rhs_var}")
            print(f"[Codegen]   Add: {lhs_var} + {rhs_var} -> {tmp_var}")
            return tmp_var
        
        elif isinstance(expr, ops.Maximum):
            # === Maximum æ“ä½œï¼ˆç”¨äºreluï¼‰ ===
            lhs_var = self.codegen_expr(expr.lhs)
            rhs_var = self.codegen_expr(expr.rhs)
            
            tmp_var = self.new_tmp_var()  # tmp3
            self.compute.writeline(f"{tmp_var} = tl.maximum({lhs_var}, {rhs_var})")
            print(f"[Codegen]   Maximum: max({lhs_var}, {rhs_var}) -> {tmp_var}")
            return tmp_var
        
        elif isinstance(expr, ops.Constant):
            # === å¸¸é‡ ===
            return str(expr.value)  # "0.0"
        
        else:
            raise NotImplementedError(f"Unknown expr: {type(expr)}")
    
    def codegen_store(self, result_var):
        """ç”Ÿæˆstoreè¯­å¥"""
        self.stores.writeline(
            f"tl.store(out_ptr0 + xindex, {result_var}, xmask)"
        )
        print(f"[Codegen] Store: {result_var} -> out_ptr0[xindex]")
    
    def assemble(self):
        """ç»„è£…å®Œæ•´çš„kernelä»£ç """
        code = IndentedBuffer()
        
        # å‡½æ•°ç­¾å
        code.writeline("@triton.jit")
        code.writeline("def triton_poi_fused_add_relu_0(")
        code.indent()
        code.splice(self.args)
        code.writeline("XBLOCK: tl.constexpr,")
        code.dedent()
        code.writeline("):")
        
        # å‡½æ•°ä½“
        code.indent()
        code.splice(self.indexing)
        code.writeline("")
        code.splice(self.loads)
        code.writeline("")
        code.splice(self.compute)
        code.writeline("")
        code.splice(self.stores)
        code.dedent()
        
        return code.getvalue()
```

**ç”Ÿæˆçš„Tritonä»£ç ï¼ˆæœ€ç»ˆè¾“å‡ºï¼‰**ï¼š

```python
@triton.jit
def triton_poi_fused_add_relu_0(
    in_ptr0,  # x
    in_ptr1,  # y
    out_ptr0, # output
    xnumel,   # 1024
    XBLOCK: tl.constexpr,
):
    # è®¡ç®—ç´¢å¼•
    pid = tl.program_id(0)
    xoffset = pid * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)
    xmask = xindex < xnumel
    
    # Load
    tmp0 = tl.load(in_ptr0 + xindex, xmask)  # load x
    tmp1 = tl.load(in_ptr1 + xindex, xmask)  # load y
    
    # Compute
    tmp2 = tmp0 + tmp1           # add
    tmp3 = tl.maximum(tmp2, 0.0) # relu
    
    # Store
    tl.store(out_ptr0 + xindex, tmp3, xmask)
```

**ä»£ç ç”Ÿæˆè¿‡ç¨‹çš„æ‰§è¡Œæ—¥å¿—**ï¼š

```
[Codegen] å¤„ç†èŠ‚ç‚¹: buf_fused
[Codegen] å‚æ•°: 2ä¸ªè¾“å…¥ + 1ä¸ªè¾“å‡º + 1ä¸ªsize
[Codegen] ç´¢å¼•: 1D, range=[1024]
[Codegen] å¤„ç†è¡¨è¾¾å¼: Maximum
[Codegen]   å¤„ç†è¡¨è¾¾å¼: Add
[Codegen]     å¤„ç†è¡¨è¾¾å¼: Load
[Codegen]       Load: in_ptr0[xindex] -> tmp0
[Codegen]     å¤„ç†è¡¨è¾¾å¼: Load
[Codegen]       Load: in_ptr1[xindex] -> tmp1
[Codegen]   Add: tmp0 + tmp1 -> tmp2
[Codegen]   å¤„ç†è¡¨è¾¾å¼: Constant
[Codegen] Maximum: max(tmp2, 0.0) -> tmp3
[Codegen] Store: tmp3 -> out_ptr0[xindex]
```

#### æ­¥éª¤7ï¼šé…ç½®å‚æ•°ï¼ˆå¯å‘å¼ï¼‰ï¼ˆt=55msï¼‰

```python
# torch/_inductor/codegen/triton.py
class TritonScheduling:
    def select_config(self, node):
        """é€‰æ‹©kernelå‚æ•°é…ç½®"""
        numel = node.get_size()[0]  # 1024
        dtype = node.get_dtype()    # torch.float32
        
        # [1] é€‰æ‹©XBLOCK
        xblock = self.select_xblock(numel, dtype)
        print(f"[Config] XBLOCK = {xblock}")
        
        # [2] é€‰æ‹©num_warps
        num_warps = self.select_num_warps(xblock)
        print(f"[Config] num_warps = {num_warps}")
        
        # [3] è®¡ç®—grid
        grid_size = triton.cdiv(numel, xblock)
        print(f"[Config] grid = ({grid_size},)")
        
        return {
            'XBLOCK': xblock,
            'num_warps': num_warps,
            'grid': (grid_size,),
        }
    
    def select_xblock(self, numel, dtype):
        """å¯å‘å¼é€‰æ‹©XBLOCK"""
        # å¯¹äº1024ä¸ªå…ƒç´ ï¼Œå°è¯•ä¸åŒçš„block size
        candidates = [256, 512, 1024]
        
        for xblock in candidates:
            grid_size = triton.cdiv(numel, xblock)
            
            # æ¡ä»¶1: gridä¸èƒ½å¤ªå°
            if grid_size < 4:  # è‡³å°‘4ä¸ªblockä¿è¯å¹¶è¡Œ
                continue
            
            # æ¡ä»¶2: å†…å­˜å¯¹é½
            if xblock * dtype.itemsize % 128 == 0:
                return xblock
        
        return 256  # é»˜è®¤
    
    def select_num_warps(self, xblock):
        """æ ¹æ®XBLOCKé€‰æ‹©num_warps"""
        # æ¯ä¸ªwarp = 32 threads
        min_warps = (xblock + 31) // 32
        
        # å‘ä¸Šå–æ•´åˆ°2çš„å¹‚æ¬¡
        import math
        num_warps = 2 ** math.ceil(math.log2(min_warps))
        
        return min(num_warps, 32)

# å¯¹äºæˆ‘ä»¬çš„ä¾‹å­:
# numel = 1024
# XBLOCK = 256 (é€‰æ‹©256ï¼Œå› ä¸º 1024 / 256 = 4 blocks)
# num_warps = 8 (256 / 32 = 8 warps)
# grid = (4,)
```

#### æ­¥éª¤8ï¼šç”ŸæˆWrapperä»£ç ï¼ˆt=55msï¼‰

```python
# torch/_inductor/codegen/wrapper.py
class WrapperCodegen:
    def generate(self):
        """ç”ŸæˆPython wrapperä»£ç """
        code = IndentedBuffer()
        
        # [1] å¯¼å…¥
        code.writeline("import torch")
        code.writeline("import triton")
        code.writeline("import triton.language as tl")
        code.writeline("")
        
        # [2] Kernelå®šä¹‰
        code.writeline("# Kernelå®šä¹‰")
        code.splice(self.kernel_code)
        code.writeline("")
        
        # [3] è°ƒç”¨å‡½æ•°
        code.writeline("def call(args):")
        code.indent()
        
        # [3.1] è§£åŒ…å‚æ•°
        code.writeline("primals_1 = args[0]  # x")
        code.writeline("primals_2 = args[1]  # y")
        code.writeline("")
        
        # [3.2] åˆ†é…è¾“å‡ºç¼“å†²åŒº
        code.writeline("# åˆ†é…è¾“å‡º")
        code.writeline("buf0 = torch.empty_strided(")
        code.indent()
        code.writeline("(1024,),")
        code.writeline("(1,),")
        code.writeline("device='cuda',")
        code.writeline("dtype=torch.float32")
        code.dedent()
        code.writeline(")")
        code.writeline("")
        
        # [3.3] è°ƒç”¨kernel
        code.writeline("# å¯åŠ¨kernel")
        code.writeline("grid = lambda meta: (")
        code.indent()
        code.writeline("triton.cdiv(1024, meta['XBLOCK']),")
        code.dedent()
        code.writeline(")")
        code.writeline("triton_poi_fused_add_relu_0[grid](")
        code.indent()
        code.writeline("primals_1,")
        code.writeline("primals_2,")
        code.writeline("buf0,")
        code.writeline("1024,")
        code.writeline("XBLOCK=256,")
        code.writeline("num_warps=8,")
        code.dedent()
        code.writeline(")")
        code.writeline("")
        
        # [3.4] è¿”å›ç»“æœ
        code.writeline("return (buf0,)")
        code.dedent()
        
        return code.getvalue()
```

**ç”Ÿæˆçš„å®Œæ•´Pythonæ¨¡å—**ï¼š

```python
# /tmp/torchinductor_user/ab/cabcdef123.py
import torch
import triton
import triton.language as tl

# Kernelå®šä¹‰
@triton.jit
def triton_poi_fused_add_relu_0(
    in_ptr0,
    in_ptr1,
    out_ptr0,
    xnumel,
    XBLOCK: tl.constexpr,
):
    pid = tl.program_id(0)
    xoffset = pid * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)
    xmask = xindex < xnumel
    
    tmp0 = tl.load(in_ptr0 + xindex, xmask)
    tmp1 = tl.load(in_ptr1 + xindex, xmask)
    tmp2 = tmp0 + tmp1
    tmp3 = tl.maximum(tmp2, 0.0)
    
    tl.store(out_ptr0 + xindex, tmp3, xmask)

def call(args):
    """ä¸»è°ƒç”¨å‡½æ•°"""
    primals_1 = args[0]  # x
    primals_2 = args[1]  # y
    
    # åˆ†é…è¾“å‡º
    buf0 = torch.empty_strided(
        (1024,),
        (1,),
        device='cuda',
        dtype=torch.float32
    )
    
    # å¯åŠ¨kernel
    grid = lambda meta: (
        triton.cdiv(1024, meta['XBLOCK']),
    )
    triton_poi_fused_add_relu_0[grid](
        primals_1,
        primals_2,
        buf0,
        1024,
        XBLOCK=256,
        num_warps=8,
    )
    
    return (buf0,)
```

#### æ­¥éª¤9ï¼šTritonç¼–è¯‘ï¼ˆt=55-150msï¼‰

```python
# Tritonç¼–è¯‘å™¨å°†Triton DSLç¼–è¯‘ä¸ºPTX/LLVM IR

# [1] Triton JITç¼–è¯‘
# triton/compiler/compiler.py
class TritonCompiler:
    def compile(self, src, options):
        """ç¼–è¯‘Tritonä»£ç """
        print("[Triton] å¼€å§‹ç¼–è¯‘...")
        
        # [1] è§£æTritonä»£ç  -> AST
        ast = self.parse(src)
        print("[Triton] è§£æå®Œæˆ")
        
        # [2] ç±»å‹æ¨æ–­
        ast = self.type_inference(ast)
        print("[Triton] ç±»å‹æ¨æ–­å®Œæˆ")
        
        # [3] è½¬æ¢ä¸ºTriton IR
        ttir = self.lower_to_ttir(ast)
        print("[Triton] Triton IRç”Ÿæˆ")
        
        # [4] ä¼˜åŒ–Triton IR
        ttir = self.optimize_ttir(ttir)
        print("[Triton] IRä¼˜åŒ–å®Œæˆ")
        
        # [5] è½¬æ¢ä¸ºLLVM IR
        llvm_ir = self.lower_to_llvm(ttir, options)
        print("[Triton] LLVM IRç”Ÿæˆ")
        
        # [6] LLVMç¼–è¯‘ä¸ºPTX
        ptx = self.llvm_to_ptx(llvm_ir)
        print("[Triton] PTXç”Ÿæˆ")
        
        # [7] PTXç¼–è¯‘ä¸ºCUBIN
        cubin = self.ptx_to_cubin(ptx)
        print("[Triton] CUBINç”Ÿæˆ")
        
        return cubin

# ç¼–è¯‘æ—¥å¿—:
# [Triton] å¼€å§‹ç¼–è¯‘...
# [Triton] è§£æå®Œæˆ (10ms)
# [Triton] ç±»å‹æ¨æ–­å®Œæˆ (5ms)
# [Triton] Triton IRç”Ÿæˆ (15ms)
# [Triton] IRä¼˜åŒ–å®Œæˆ (20ms)
# [Triton] LLVM IRç”Ÿæˆ (25ms)
# [Triton] PTXç”Ÿæˆ (30ms)
# [Triton] CUBINç”Ÿæˆ (45ms)
# [Triton] ç¼–è¯‘å®Œæˆ! (æ€»è®¡ 150ms)
```

#### æ­¥éª¤10ï¼šåŠ¨æ€åŠ è½½ï¼ˆt=150msï¼‰

```python
# torch/_inductor/codecache.py
class CodeCache:
    def load(self, module_path):
        """åŠ¨æ€åŠ è½½ç¼–è¯‘å¥½çš„æ¨¡å—"""
        print(f"[Cache] åŠ è½½æ¨¡å—: {module_path}")
        
        # Pythonçš„åŠ¨æ€å¯¼å…¥
        import importlib.util
        spec = importlib.util.spec_from_file_location(
            "compiled_module",
            module_path
        )
        module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(module)
        
        print("[Cache] æ¨¡å—åŠ è½½å®Œæˆ")
        return module

# åŠ è½½ç¼–è¯‘ç»“æœ
compiled_module = load("/tmp/torchinductor_user/ab/cabcdef123.py")
compiled_fn = compiled_module.call
```

#### æ­¥éª¤11ï¼šé¦–æ¬¡æ‰§è¡Œï¼ˆt=151msï¼‰

```python
# ç¬¬ä¸€æ¬¡è°ƒç”¨ model(x, y)
result = compiled_fn([x, y])

# GPUæ‰§è¡Œæµç¨‹:
# 1. å¯åŠ¨kernel: triton_poi_fused_add_relu_0
#    - grid = (4,)  # 4ä¸ªblock
#    - block = (256 threads,)  # æ¯ä¸ªblock 256çº¿ç¨‹
#    - num_warps = 8  # æ¯ä¸ªblock 8ä¸ªwarp
# 
# 2. GPUè°ƒåº¦:
#    Block 0: å¤„ç† x[0:256]
#    Block 1: å¤„ç† x[256:512]
#    Block 2: å¤„ç† x[512:768]
#    Block 3: å¤„ç† x[768:1024]
# 
# 3. æ¯ä¸ªthreadæ‰§è¡Œ:
#    - Load: x[tid], y[tid]
#    - Compute: tmp = x[tid] + y[tid]
#    - Compute: result = max(tmp, 0.0)
#    - Store: result -> output[tid]
# 
# 4. åŒæ­¥ç­‰å¾…æ‰€æœ‰blockå®Œæˆ
# 
# æ‰§è¡Œæ—¶é—´: ~0.1ms
```

#### æ­¥éª¤12ï¼šç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆt=152msï¼‰

```python
# ç¬¬äºŒæ¬¡è°ƒç”¨ model(x, y)
result = model(x, y)

# ç›´æ¥ä½¿ç”¨ç¼“å­˜çš„ç¼–è¯‘ç»“æœï¼
# - ä¸éœ€è¦é‡æ–°ç¼–è¯‘
# - ä¸éœ€è¦é‡æ–°åŠ è½½
# - ç›´æ¥è°ƒç”¨ compiled_fn
# 
# æ‰§è¡Œæ—¶é—´: ~0.1ms (çº¯GPUæ‰§è¡Œæ—¶é—´)
```

### 2.4 å®Œæ•´æ—¶é—´çº¿æ€»ç»“

```
æ—¶é—´      é˜¶æ®µ                   è€—æ—¶      è¯´æ˜
--------------------------------------------------------------------
0ms       ç”¨æˆ·è°ƒç”¨              0ms       model(x, y)
1ms       Dynamoæ‹¦æˆª            1ms       å­—èŠ‚ç åˆ†æ
2ms       FX Graphç”Ÿæˆ          48ms      æ„å»ºè®¡ç®—å›¾
50ms      AOTAutograd          1ms       å‰å‘/åå‘åˆ†ç¦»
51ms      Lowering             1ms       FX Graph -> IR
52ms      Fusion               1ms       ç®—å­èåˆ
53ms      Code Generation      2ms       IR -> Tritonä»£ç 
55ms      é…ç½®é€‰æ‹©             <1ms      é€‰æ‹©XBLOCKç­‰å‚æ•°
55ms      Tritonç¼–è¯‘           95ms      Triton -> PTX -> CUBIN
150ms     åŠ¨æ€åŠ è½½             1ms       å¯¼å…¥Pythonæ¨¡å—
151ms     é¦–æ¬¡æ‰§è¡Œ             <1ms      GPUæ‰§è¡Œ
--------------------------------------------------------------------
æ€»è®¡:     é¦–æ¬¡è°ƒç”¨             ~151ms    (ä¸»è¦æ˜¯ç¼–è¯‘)
          åç»­è°ƒç”¨             ~0.1ms    (ç›´æ¥æ‰§è¡Œ)
```

### 2.5 å…³é”®æ•°æ®æµ

```
ç”¨æˆ·æ•°æ®:
  x: torch.Tensor([...], shape=(1024,), device='cuda')
  y: torch.Tensor([...], shape=(1024,), device='cuda')

â†“ [Dynamoæ•è·]

FX Graph:
  %x : Tensor(1024)
  %y : Tensor(1024)
  %add : Tensor(1024) = aten.add(%x, %y)
  %relu : Tensor(1024) = aten.relu(%add)

â†“ [Lowering]

IR:
  buf0 = Pointwise(lambda i: load(x, i) + load(y, i))
  buf1 = Pointwise(lambda i: max(load(buf0, i), 0.0))

â†“ [Fusion]

Fused IR:
  buf_fused = Pointwise(lambda i: max(load(x, i) + load(y, i), 0.0))

â†“ [Code Generation]

Triton Code:
  tmp0 = load(x, i)
  tmp1 = load(y, i)
  tmp2 = tmp0 + tmp1
  tmp3 = max(tmp2, 0.0)
  store(output, i, tmp3)

â†“ [Compilation]

GPU Binary:
  CUBIN: [binary code...]

â†“ [Execution]

GPU Memory:
  x_ptr: 0x7f1234... (4096 bytes)
  y_ptr: 0x7f5678... (4096 bytes)
  out_ptr: 0x7f9abc... (4096 bytes)

GPU Execution:
  4 blocks Ã— 256 threads = 1024 threads
  æ¯ä¸ªthreadå¤„ç†1ä¸ªå…ƒç´ 
  ç»“æœå†™å› out_ptr
```

### 2.6 ä¸åŒæ“ä½œç±»å‹çš„ä»£ç ç”Ÿæˆ

#### 2.6.1 Pointwiseæ“ä½œï¼ˆ1Dï¼‰

```python
# è¾“å…¥IR
Pointwise(
    inner_fn=lambda idx: ops.load(x, idx) * 2.0,
    ranges=[1024],
)

# ç”Ÿæˆçš„Tritonä»£ç 
@triton.jit
def kernel(in_ptr0, out_ptr0, xnumel, XBLOCK: tl.constexpr):
    pid = tl.program_id(0)
    xindex = pid * XBLOCK + tl.arange(0, XBLOCK)
    xmask = xindex < xnumel
    
    tmp0 = tl.load(in_ptr0 + xindex, xmask)
    tmp1 = tmp0 * 2.0
    tl.store(out_ptr0 + xindex, tmp1, xmask)
```

#### 2.6.2 Pointwiseæ“ä½œï¼ˆ2Dï¼‰

```python
# è¾“å…¥IR: x.T (è½¬ç½®)
Pointwise(
    inner_fn=lambda i, j: ops.load(x, [j, i]),  # äº¤æ¢ç´¢å¼•
    ranges=[N, M],  # è¾“å‡ºå½¢çŠ¶
)

# ç”Ÿæˆçš„Tritonä»£ç 
@triton.jit
def kernel(in_ptr0, out_ptr0, M, N, XBLOCK: tl.constexpr):
    pid = tl.program_id(0)
    
    # 2Dç´¢å¼•å±•å¹³
    xindex = pid * XBLOCK + tl.arange(0, XBLOCK)
    xmask = xindex < M * N
    
    # è½¬æ¢ä¸º2Dç´¢å¼•
    x0 = xindex % N  # j
    x1 = xindex // N # i
    
    # Load: äº¤æ¢ç´¢å¼•
    tmp0 = tl.load(in_ptr0 + x1 + x0 * M, xmask)
    
    # Store: æ­£å¸¸é¡ºåº
    tl.store(out_ptr0 + xindex, tmp0, xmask)
```

#### 2.6.3 Reductionæ“ä½œ

```python
# è¾“å…¥IR: x.sum(dim=1)
Reduction(
    inner_fn=lambda i, j: ops.load(x, [i, j]),
    reduction_type="sum",
    ranges=[M, N],     # è¾“å…¥å½¢çŠ¶
    reduction_dim=1,   # åœ¨ç¬¬1ç»´ä¸Šreduce
    output_shape=[M],  # è¾“å‡ºå½¢çŠ¶
)

# ç”Ÿæˆçš„Tritonä»£ç ï¼ˆæ›´å¤æ‚ï¼‰
@triton.jit
def kernel(
    in_ptr0,
    out_ptr0,
    M, N,
    XBLOCK: tl.constexpr,
    RBLOCK: tl.constexpr,  # Reduction block size
):
    pid = tl.program_id(0)
    xindex = pid * XBLOCK + tl.arange(0, XBLOCK)
    xmask = xindex < M
    
    # åˆå§‹åŒ–ç´¯åŠ å™¨
    accumulator = tl.zeros([XBLOCK], dtype=tl.float32)
    
    # å¾ªç¯éå†reductionç»´åº¦
    for roffset in range(0, N, RBLOCK):
        rindex = roffset + tl.arange(0, RBLOCK)
        rmask = rindex < N
        
        # è®¡ç®—2Dç´¢å¼•: (xindex[:, None], rindex[None, :])
        # è¿™ä¼šåˆ›å»ºä¸€ä¸ª XBLOCK x RBLOCK çš„çŸ©é˜µ
        mask = xmask[:, None] & rmask[None, :]
        
        # Load: in_ptr0[xindex, rindex]
        offset = xindex[:, None] * N + rindex[None, :]
        data = tl.load(in_ptr0 + offset, mask, other=0.0)
        
        # Reduce: sum along RBLOCK dimension
        accumulator += tl.sum(data, axis=1)
    
    # Storeç»“æœ
    tl.store(out_ptr0 + xindex, accumulator, xmask)
```

### 2.7 ç´¢å¼•è®¡ç®—çš„ç”Ÿæˆ

ç´¢å¼•è®¡ç®—æ˜¯ä»£ç ç”Ÿæˆä¸­æœ€å¤æ‚çš„éƒ¨åˆ†ã€‚TorchInductorä½¿ç”¨ç¬¦å·è¡¨è¾¾å¼ï¼ˆsympyï¼‰æ¥è¡¨ç¤ºå’Œä¼˜åŒ–ç´¢å¼•ã€‚

```python
# torch/_inductor/codegen/triton.py
class TritonKernel:
    def codegen_indexing(self, ranges):
        """
        ç”Ÿæˆç´¢å¼•è®¡ç®—ä»£ç 
        
        ranges: å¼ é‡çš„å½¢çŠ¶ï¼Œä¾‹å¦‚ [B, M, N]
        ç›®æ ‡ï¼šå°†çº¿æ€§ç´¢å¼• xindex è½¬æ¢ä¸ºå¤šç»´ç´¢å¼• (b, m, n)
        """
        # [1] 1Dæƒ…å†µï¼ˆæœ€ç®€å•ï¼‰
        if len(ranges) == 1:
            code = """
            xindex = xoffset + tl.arange(0, XBLOCK)
            xmask = xindex < {numel}
            """.format(numel=ranges[0])
            return code
        
        # [2] å¤šç»´æƒ…å†µ
        # ç”Ÿæˆç´¢å¼•åˆ†è§£å…¬å¼
        # ä¾‹å¦‚å¯¹äº [B, M, N]:
        #   xindex = b * M * N + m * N + n
        #   =>  n = xindex % N
        #       m = (xindex // N) % M
        #       b = xindex // (M * N)
        
        code = IndentedBuffer()
        code.writeline("xindex = xoffset + tl.arange(0, XBLOCK)")
        
        # è®¡ç®—æ€»å…ƒç´ æ•°
        numel = 1
        for size in ranges:
            numel *= size
        code.writeline(f"xmask = xindex < {numel}")
        
        # ç”Ÿæˆæ¯ä¸ªç»´åº¦çš„ç´¢å¼•
        strides = []
        stride = 1
        for size in reversed(ranges):
            strides.insert(0, stride)
            stride *= size
        
        for dim_idx, (size, stride) in enumerate(zip(ranges, strides)):
            if stride == 1:
                # æœ€å†…å±‚ç»´åº¦
                code.writeline(f"x{dim_idx} = xindex % {size}")
            elif stride == numel // size:
                # æœ€å¤–å±‚ç»´åº¦
                code.writeline(f"x{dim_idx} = xindex // {stride}")
            else:
                # ä¸­é—´ç»´åº¦
                code.writeline(
                    f"x{dim_idx} = (xindex // {stride}) % {size}"
                )
        
        return code.getvalue()

# ç¤ºä¾‹ï¼š3Då¼ é‡ [2, 3, 4]
ranges = [2, 3, 4]
indexing_code = codegen_indexing(ranges)

# ç”Ÿæˆçš„ä»£ç ï¼š
"""
xindex = xoffset + tl.arange(0, XBLOCK)
xmask = xindex < 24
x2 = xindex % 4          # æœ€å†…å±‚: n
x1 = (xindex // 4) % 3   # ä¸­é—´å±‚: m
x0 = xindex // 12        # æœ€å¤–å±‚: b
"""
```

### 2.8 å†…å­˜è®¿é—®æ¨¡å¼çš„ä¼˜åŒ–

```python
# torch/_inductor/codegen/triton.py
class TritonKernel:
    def optimize_memory_access(self, buffer, indices):
        """
        ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
        
        ç›®æ ‡ï¼š
        1. åˆå¹¶è¿ç»­è®¿é—®ï¼ˆcoalesced accessï¼‰
        2. å‘é‡åŒ–åŠ è½½ï¼ˆvectorized loadï¼‰
        3. é¿å…bank conflict
        """
        # [1] æ£€æŸ¥æœ€å†…å±‚ç»´åº¦æ˜¯å¦è¿ç»­
        if self.is_contiguous_access(indices):
            # ä½¿ç”¨å‘é‡åŒ–åŠ è½½
            return self.generate_vectorized_load(buffer, indices)
        else:
            # ä½¿ç”¨æ ‡é‡åŠ è½½
            return self.generate_scalar_load(buffer, indices)
    
    def is_contiguous_access(self, indices):
        """
        åˆ¤æ–­è®¿é—®æ˜¯å¦è¿ç»­
        
        è¿ç»­è®¿é—®ï¼šæœ€å†…å±‚ç´¢å¼•æ˜¯çº¿æ€§çš„
        ä¾‹å¦‚ï¼šbuffer[xindex] æ˜¯è¿ç»­çš„
              buffer[xindex * stride] å¯èƒ½ä¸è¿ç»­
        """
        innermost_index = indices[-1]
        # æ£€æŸ¥æ˜¯å¦æ˜¯ xindex æˆ– xindex + constant
        return self.is_linear_in_xindex(innermost_index)
    
    def generate_vectorized_load(self, buffer, indices):
        """
        ç”Ÿæˆå‘é‡åŒ–åŠ è½½
        
        # æ ‡é‡åŠ è½½ï¼ˆæ…¢ï¼‰
        for i in range(XBLOCK):
            data[i] = buffer[xindex + i]
        
        # å‘é‡åŒ–åŠ è½½ï¼ˆå¿«ï¼‰
        data = tl.load(buffer + xindex, mask, other=0.0)
        """
        offset = self.compute_offset(indices)
        tmp_var = self.new_tmp_var()
        
        self.loads.writeline(
            f"{tmp_var} = tl.load({buffer} + {offset}, xmask, other=0.0)"
        )
        return tmp_var

# å®é™…ç”Ÿæˆçš„ä»£ç å¯¹æ¯”
# [A] è¿ç»­è®¿é—®ï¼ˆé«˜æ•ˆï¼‰
tmp0 = tl.load(in_ptr0 + xindex, xmask)  # Coalesced

# [B] éè¿ç»­è®¿é—®ï¼ˆä½æ•ˆï¼‰
offset = x0 * 1024 + x1  # å¯èƒ½å¯¼è‡´ä¸è¿ç»­
tmp0 = tl.load(in_ptr0 + offset, xmask)  # Potentially uncoalesced
```

### 2.9 GELUç¤ºä¾‹ï¼šå¤æ‚ç®—å­çš„å®Œæ•´ç”Ÿæˆ

è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªç¨å¾®å¤æ‚çš„ä¾‹å­ï¼Œå®Œæ•´å±•ç¤ºæ•´ä¸ªæµç¨‹ï¼š

```python
# [0] ç”¨æˆ·ä»£ç 
import torch

@torch.compile
def fused_gelu_approximate(x):
    """GELUæ¿€æ´»å‡½æ•°çš„è¿‘ä¼¼å®ç°"""
    # GELU(x) â‰ˆ 0.5 * x * (1 + tanh(sqrt(2/Ï€) * (x + 0.044715 * x^3)))
    import math
    
    # å¸¸é‡
    sqrt_2_over_pi = math.sqrt(2.0 / math.pi)
    
    # è®¡ç®—
    x_cubed = x * x * x
    inner = sqrt_2_over_pi * (x + 0.044715 * x_cubed)
    tanh_inner = torch.tanh(inner)
    result = 0.5 * x * (1.0 + tanh_inner)
    
    return result

x = torch.randn(1024, device='cuda')
y = fused_gelu_approximate(x)
```

**[1] FX Graph**ï¼š

```python
def forward(self, x):
    mul = x * x                      # x^2
    mul_1 = mul * x                  # x^3
    mul_2 = 0.044715 * mul_1         # 0.044715 * x^3
    add = x + mul_2                  # x + 0.044715 * x^3
    mul_3 = 0.7978845608 * add       # sqrt(2/Ï€) * (...)
    tanh = torch.tanh(mul_3)         # tanh(...)
    add_1 = 1.0 + tanh               # 1 + tanh(...)
    mul_4 = x * add_1                # x * (1 + tanh(...))
    mul_5 = 0.5 * mul_4              # 0.5 * x * (...)
    return mul_5
```

**[2] èåˆåçš„IR**ï¼š

æ‰€æœ‰æ“ä½œéƒ½æ˜¯Pointwiseï¼Œå¯ä»¥å…¨éƒ¨èåˆï¼

```python
def fused_inner_fn(idx):
    x_val = ops.load(x, idx)
    
    # x^3
    x_squared = ops.mul(x_val, x_val)
    x_cubed = ops.mul(x_squared, x_val)
    
    # 0.044715 * x^3
    term1 = ops.mul(ops.constant(0.044715), x_cubed)
    
    # x + 0.044715 * x^3
    sum1 = ops.add(x_val, term1)
    
    # sqrt(2/Ï€) * (...)
    scaled = ops.mul(ops.constant(0.7978845608), sum1)
    
    # tanh(...)
    tanh_val = ops.tanh(scaled)
    
    # 1 + tanh(...)
    sum2 = ops.add(ops.constant(1.0), tanh_val)
    
    # x * (1 + tanh(...))
    prod1 = ops.mul(x_val, sum2)
    
    # 0.5 * x * (...)
    result = ops.mul(ops.constant(0.5), prod1)
    
    return result
```

**[3] ç”Ÿæˆçš„Tritonä»£ç **ï¼š

```python
@triton.jit
def triton_poi_fused_gelu_0(
    in_ptr0,   # x
    out_ptr0,  # output
    xnumel,    # 1024
    XBLOCK: tl.constexpr,
):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)
    xmask = xindex < xnumel
    
    # Load
    x0 = tl.load(in_ptr0 + xindex, xmask)
    
    # Compute (å…¨éƒ¨èåˆï¼)
    tmp0 = x0 * x0                      # x^2
    tmp1 = tmp0 * x0                    # x^3
    tmp2 = 0.044715 * tmp1              # 0.044715 * x^3
    tmp3 = x0 + tmp2                    # x + 0.044715 * x^3
    tmp4 = 0.7978845608 * tmp3          # sqrt(2/Ï€) * (...)
    tmp5 = tl.libdevice.tanh(tmp4)      # tanh(...) - è°ƒç”¨GPUåº“å‡½æ•°
    tmp6 = 1.0 + tmp5                   # 1 + tanh(...)
    tmp7 = x0 * tmp6                    # x * (1 + tanh(...))
    tmp8 = 0.5 * tmp7                   # 0.5 * x * (...)
    
    # Store
    tl.store(out_ptr0 + xindex, tmp8, xmask)

# è°ƒç”¨é…ç½®
grid = lambda meta: (triton.cdiv(1024, meta['XBLOCK']),)
triton_poi_fused_gelu_0[grid](
    x, out, 1024,
    XBLOCK=256,
    num_warps=4,
)
```

**æ€§èƒ½åˆ†æ**ï¼š

```python
# æœªèåˆç‰ˆæœ¬ï¼ˆEageræ¨¡å¼ï¼‰
# 9ä¸ªç‹¬ç«‹çš„kernelè°ƒç”¨ï¼š
# 1. mul (x*x)
# 2. mul (x^2 * x)
# 3. mul (0.044715 * x^3)
# 4. add (x + ...)
# 5. mul (sqrt(2/Ï€) * ...)
# 6. tanh
# 7. add (1 + tanh)
# 8. mul (x * ...)
# 9. mul (0.5 * ...)
# 
# å†…å­˜è®¿é—®ï¼š9æ¬¡è¯» + 9æ¬¡å†™ = 18æ¬¡
# Kernelå¯åŠ¨ï¼š9æ¬¡ * ~5Î¼s = ~45Î¼s

# èåˆç‰ˆæœ¬ï¼ˆtorch.compileï¼‰
# 1ä¸ªèåˆçš„kernel
# 
# å†…å­˜è®¿é—®ï¼š1æ¬¡è¯» + 1æ¬¡å†™ = 2æ¬¡
# Kernelå¯åŠ¨ï¼š1æ¬¡ * ~5Î¼s = ~5Î¼s
#
# åŠ é€Ÿæ¯”ï¼š~9xï¼ˆç†è®ºï¼‰
```

### 2.10 Wrapperä»£ç ç”Ÿæˆ

é™¤äº†kernelä»£ç ï¼ŒTorchInductorè¿˜ç”ŸæˆPython wrapperä»£ç æ¥è°ƒç”¨kernelï¼š

```python
# torch/_inductor/codegen/wrapper.py
class WrapperCodegen:
    def generate(self):
        """ç”Ÿæˆå®Œæ•´çš„Pythonæ¨¡å—"""
        code = IndentedBuffer()
        
        # [1] å¯¼å…¥
        code.writeline("import torch")
        code.writeline("import triton")
        code.writeline("import triton.language as tl")
        code.writeline("")
        
        # [2] Kernelå®šä¹‰
        code.splice(self.kernel_code)
        code.writeline("")
        
        # [3] è°ƒç”¨å‡½æ•°
        code.writeline("def call(args):")
        code.indent()
        
        # [3.1] è§£åŒ…å‚æ•°
        code.writeline("primals_1 = args[0]  # x")
        code.writeline("primals_2 = args[1]  # y")
        code.writeline("")
        
        # [3.2] åˆ†é…è¾“å‡ºç¼“å†²åŒº
        code.writeline("buf0 = torch.empty_like(primals_1)")
        code.writeline("")
        
        # [3.3] è°ƒç”¨kernel
        code.writeline("# Launch kernel")
        code.writeline("grid = lambda meta: (triton.cdiv(1024, meta['XBLOCK']),)")
        code.writeline("triton_poi_fused_add_relu_0[grid](")
        code.indent()
        code.writeline("primals_1,  # in_ptr0")
        code.writeline("primals_2,  # in_ptr1")
        code.writeline("buf0,       # out_ptr0")
        code.writeline("1024,       # xnumel")
        code.writeline("XBLOCK=256,")
        code.writeline("num_warps=4,")
        code.dedent()
        code.writeline(")")
        code.writeline("")
        
        # [3.4] è¿”å›ç»“æœ
        code.writeline("return (buf0,)")
        code.dedent()
        
        return code.getvalue()
```

**ç”Ÿæˆçš„å®Œæ•´Pythonæ¨¡å—**ï¼š

```python
# /tmp/torchinductor_username/xx/cxxyyyzzz.py
import torch
import triton
import triton.language as tl

@triton.jit
def triton_poi_fused_add_relu_0(
    in_ptr0,
    in_ptr1,
    out_ptr0,
    xnumel,
    XBLOCK: tl.constexpr,
):
    pid = tl.program_id(0)
    xoffset = pid * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)
    xmask = xindex < xnumel
    
    tmp0 = tl.load(in_ptr0 + xindex, xmask)
    tmp1 = tl.load(in_ptr1 + xindex, xmask)
    tmp2 = tmp0 + tmp1
    tmp3 = tl.maximum(tmp2, 0.0)
    
    tl.store(out_ptr0 + xindex, tmp3, xmask)

def call(args):
    """ä¸»è°ƒç”¨å‡½æ•°"""
    # [1] è§£åŒ…è¾“å…¥
    primals_1 = args[0]  # x
    primals_2 = args[1]  # y
    
    # [2] åˆ†é…è¾“å‡º
    buf0 = torch.empty_like(primals_1)
    
    # [3] è°ƒç”¨kernel
    grid = lambda meta: (triton.cdiv(1024, meta['XBLOCK']),)
    triton_poi_fused_add_relu_0[grid](
        primals_1,
        primals_2,
        buf0,
        1024,
        XBLOCK=256,
        num_warps=4,
    )
    
    # [4] è¿”å›
    return (buf0,)
```

---

## 3. Triton Kernel å‚æ•°è¯¦è§£

### 3.1 æ ¸å¿ƒå‚æ•°æ¦‚è§ˆ

Triton Kernel çš„æ€§èƒ½ç”±ä»¥ä¸‹å‚æ•°å†³å®šï¼š

| å‚æ•° | å«ä¹‰ | å…¸å‹èŒƒå›´ | å½±å“ |
|-----|------|---------|------|
| **XBLOCK** | X ç»´åº¦çš„ block size | 16, 32, 64, 128, 256, 512, 1024 | æ¯ä¸ªçº¿ç¨‹å—å¤„ç†çš„å…ƒç´ æ•° |
| **YBLOCK** | Y ç»´åº¦çš„ block size | 1, 16, 32, 64 | 2D block çš„ç¬¬äºŒç»´å¤§å° |
| **RBLOCK** | Reduction ç»´åº¦çš„ block size | 32, 64, 128, 256, 512, 1024 | Reduction æ“ä½œçš„å—å¤§å° |
| **num_warps** | æ¯ä¸ª block çš„ warp æ•° | 1, 2, 4, 8, 16, 32 | SM èµ„æºå ç”¨ |
| **num_stages** | Pipeline stages | 1, 2, 3, 4 | å†…å­˜å¸¦å®½éšè— |
| **grid** | Grid å¤§å° (x, y, z) | (1, 1, 1) ~ (2^31-1, 65535, 65535) | å¹¶è¡Œçš„çº¿ç¨‹å—æ•°é‡ |

### 3.2 å‚æ•°ç¤ºä¾‹

```python
# ç”Ÿæˆçš„ Triton Kernel ç¤ºä¾‹
@triton.jit
def triton_poi_fused_add_relu_0(
    in_ptr0,     # è¾“å…¥æŒ‡é’ˆ
    in_ptr1,     # è¾“å…¥æŒ‡é’ˆ
    out_ptr0,    # è¾“å‡ºæŒ‡é’ˆ
    xnumel,      # æ€»å…ƒç´ æ•°
    XBLOCK: tl.constexpr,  # ç¼–è¯‘æ—¶å¸¸é‡
):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)
    xmask = xindex < xnumel
    
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + x0, xmask)
    tmp1 = tl.load(in_ptr1 + x0, xmask)
    tmp2 = tmp0 + tmp1
    tmp3 = tl.maximum(tmp2, 0.0)
    tl.store(out_ptr0 + x0, tmp3, xmask)

# è°ƒç”¨æ—¶çš„é…ç½®
grid = lambda meta: (triton.cdiv(numel, meta['XBLOCK']),)
triton_poi_fused_add_relu_0[grid](
    in_ptr0, in_ptr1, out_ptr0, numel,
    XBLOCK=1024,  # â† å¯å‘å¼é€‰æ‹©
    num_warps=4,  # â† å¯å‘å¼é€‰æ‹©
    num_stages=1,
)
```

### 3.3 å‚æ•°ä¹‹é—´çš„å…³ç³»

```
XBLOCK ä¸ num_warps çš„å…³ç³»ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ XBLOCK      threads   num_warps (æ¨è)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 16-64       â‰¤64       1                     â”‚
â”‚ 128         128       2-4                   â”‚
â”‚ 256         256       4                     â”‚
â”‚ 512         512       8                     â”‚
â”‚ 1024        1024      8-16 (å–å†³äº SM å¤§å°)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ¯ä¸ª warp = 32 threads
æ¯ä¸ª block çš„ threads = XBLOCK (å¯¹äº 1D kernel)
num_warps â‰¥ XBLOCK / 32
```

---

## 4. å¯å‘å¼ä¼˜åŒ–ç­–ç•¥

### 4.1 TorchInductor çš„ Heuristics ç³»ç»Ÿ

```python
# torch/_inductor/codegen/triton.py
class TritonScheduling:
    @staticmethod
    def get_heuristics():
        """è·å–å¯å‘å¼é…ç½®"""
        return config.triton.heuristics
    
    def codegen_kernel(self, name):
        """ç”Ÿæˆ Triton Kernel"""
        # [1] è®¡ç®—å·¥ä½œè´Ÿè½½
        numel = self.get_numel()
        
        # [2] é€‰æ‹© XBLOCK
        xblock = self.get_xblock(numel)
        
        # [3] é€‰æ‹© num_warps
        num_warps = self.get_num_warps(xblock)
        
        # [4] è®¡ç®— grid size
        grid = self.get_grid(numel, xblock)
        
        return kernel_code

# torch/_inductor/config.py
class TritonConfig:
    # å¯å‘å¼é…ç½®
    max_tiles = 2  # æ¯ä¸ª block æœ€å¤šå¤„ç†çš„ tile æ•°
    max_xblock = 1024  # æœ€å¤§ XBLOCK
    min_xblock = 16    # æœ€å° XBLOCK
```

### 4.2 å†³ç­–æ ‘ï¼šé€‰æ‹© XBLOCK

```python
def select_xblock(numel: int, dtype: torch.dtype) -> int:
    """
    æ ¹æ®å…ƒç´ æ•°é‡å’Œæ•°æ®ç±»å‹é€‰æ‹© XBLOCK
    
    ç­–ç•¥ï¼š
    1. å°½é‡ä½¿ç”¨å¤§ XBLOCK å‡å°‘ kernel launch å¼€é”€
    2. ä½†è¦ä¿è¯è¶³å¤Ÿçš„å¹¶è¡Œåº¦ï¼ˆgrid sizeï¼‰
    3. è€ƒè™‘å¯„å­˜å™¨å’Œå…±äº«å†…å­˜é™åˆ¶
    """
    # [1] æ•°æ®ç±»å‹å¯¹åº”çš„å­—èŠ‚æ•°
    elem_size = dtype.itemsize  # float32=4, float16=2
    
    # [2] å¯ç”¨çš„ XBLOCK é€‰é¡¹ï¼ˆ2 çš„å¹‚æ¬¡ï¼‰
    xblock_options = [16, 32, 64, 128, 256, 512, 1024]
    
    # [3] é€‰æ‹©ç­–ç•¥
    for xblock in reversed(xblock_options):  # ä»å¤§åˆ°å°å°è¯•
        grid_size = triton.cdiv(numel, xblock)
        
        # æ¡ä»¶ 1: grid size ä¸èƒ½å¤ªå°ï¼ˆè‡³å°‘è¦æœ‰è¶³å¤Ÿçš„å¹¶è¡Œåº¦ï¼‰
        min_grid_size = get_device_capability().multiprocessor_count * 4
        if grid_size < min_grid_size:
            continue
            
        # æ¡ä»¶ 2: ä¸è¶…è¿‡æœ€å¤§ grid size é™åˆ¶
        if grid_size > 2**31 - 1:  # CUDA grid.x é™åˆ¶
            continue
            
        # æ¡ä»¶ 3: å†…å­˜è®¿é—®æ•ˆç‡ï¼ˆé¿å… bank conflictï¼‰
        if xblock * elem_size % 128 == 0:  # å¯¹é½åˆ° 128 å­—èŠ‚ï¼ˆç¼“å­˜è¡Œï¼‰
            return xblock
    
    # é»˜è®¤è¿”å›ä¸­ç­‰å¤§å°
    return 256

# å®é™…ä½¿ç”¨ç¤ºä¾‹
numel = 1024 * 1024  # 1M å…ƒç´ 
xblock = select_xblock(numel, torch.float32)
print(f"Selected XBLOCK: {xblock}")  # è¾“å‡º: 1024
grid_size = triton.cdiv(numel, xblock)
print(f"Grid size: {grid_size}")     # è¾“å‡º: 1024
```

### 4.3 å†³ç­–æ ‘ï¼šé€‰æ‹© num_warps

```python
def select_num_warps(xblock: int, reduction: bool = False) -> int:
    """
    æ ¹æ® XBLOCK é€‰æ‹© num_warps
    
    åŸåˆ™ï¼š
    1. num_warps * 32 â‰¥ threads_per_block
    2. 2 çš„å¹‚æ¬¡ï¼ˆç¡¬ä»¶å‹å¥½ï¼‰
    3. ä¸è¶…è¿‡ SM çš„æœ€å¤§ warp æ•°
    """
    # [1] è®¡ç®—ç†è®º warp æ•°
    threads_per_block = xblock
    min_warps = (threads_per_block + 31) // 32
    
    # [2] å‘ä¸Šå–æ•´åˆ° 2 çš„å¹‚æ¬¡
    import math
    num_warps = 2 ** math.ceil(math.log2(min_warps))
    
    # [3] é™åˆ¶èŒƒå›´
    num_warps = max(1, min(num_warps, 32))
    
    # [4] Reduction æ“ä½œé€šå¸¸éœ€è¦æ›´å¤š warps
    if reduction and num_warps < 4:
        num_warps = 4
    
    return num_warps

# ç¤ºä¾‹
print(select_num_warps(1024))  # â†’ 32
print(select_num_warps(256))   # â†’ 8
print(select_num_warps(64))    # â†’ 2
```

---

## 5. Grid Size ä¸å¾ªç¯ç­–ç•¥

### 5.1 Grid Size çš„é™åˆ¶

```python
# CUDA Grid Size é™åˆ¶ï¼ˆCC 3.0+ï¼‰
MAX_GRID_X = 2**31 - 1  # â‰ˆ 21 äº¿
MAX_GRID_Y = 65535
MAX_GRID_Z = 65535

# å®é™…ç¡¬ä»¶è¿˜æœ‰å…¶ä»–é™åˆ¶ï¼š
# - SM æ•°é‡ï¼šA100 æœ‰ 108 ä¸ª SM
# - æ¯ä¸ª SM çš„æœ€å¤§ block æ•°ï¼šé€šå¸¸ 16-32
# - æ€»çš„æ´»è·ƒ block æ•°ï¼šSM_count * blocks_per_SM
```

### 5.2 Grid Size ä¸å†…éƒ¨å¾ªç¯çš„æƒè¡¡

å½“æ•°æ®é‡éå¸¸å¤§æ—¶ï¼Œæœ‰ä¸¤ç§ç­–ç•¥ï¼š

**ç­–ç•¥ Aï¼šå¤§ Grid + å° Blockï¼ˆæ— å†…éƒ¨å¾ªç¯ï¼‰**
```python
# å‡è®¾ numel = 100M å…ƒç´ 
xblock = 256
grid_size = triton.cdiv(100_000_000, 256)  # = 390,625

# Kernel å†…éƒ¨ï¼šæ¯ä¸ª block åªå¤„ç† 256 ä¸ªå…ƒç´ 
@triton.jit
def kernel(ptr, numel, XBLOCK: tl.constexpr):
    pid = tl.program_id(0)
    offset = pid * XBLOCK
    index = offset + tl.arange(0, XBLOCK)
    mask = index < numel
    
    # åªå¤„ç†ä¸€æ¬¡ï¼Œæ— å¾ªç¯
    data = tl.load(ptr + index, mask)
    tl.store(ptr + index, data, mask)
```

**ç­–ç•¥ Bï¼šå° Grid + å¤§ Blockï¼ˆæœ‰å†…éƒ¨å¾ªç¯ï¼‰**
```python
# é™åˆ¶ grid_size åˆ°ä¸€ä¸ªåˆç†å€¼
MAX_GRID = 48  # â† å¸¸è§çš„å¯å‘å¼å€¼
xblock = 256
tiles_per_block = triton.cdiv(100_000_000, MAX_GRID * xblock)  # â‰ˆ 8,139

# Kernel å†…éƒ¨ï¼šæ¯ä¸ª block å¤„ç†å¤šä¸ª tile
@triton.jit
def kernel(ptr, numel, XBLOCK: tl.constexpr):
    pid = tl.program_id(0)
    tiles = triton.cdiv(numel, XBLOCK)
    tiles_per_pid = triton.cdiv(tiles, tl.num_programs(0))
    
    # å†…éƒ¨å¾ªç¯å¤„ç†å¤šä¸ª tile
    for tile_idx in range(tiles_per_pid):
        offset = (pid * tiles_per_pid + tile_idx) * XBLOCK
        index = offset + tl.arange(0, XBLOCK)
        mask = index < numel
        
        data = tl.load(ptr + index, mask)
        tl.store(ptr + index, data, mask)
```

### 5.3 ä¸ºä»€ä¹ˆé™åˆ¶ Grid Size ä¸º 48ï¼Ÿ

è¿™æ˜¯ä¸€ä¸ªå¸¸è§çš„å¯å‘å¼å€¼ï¼Œæ¥æºäºï¼š

```python
# torch/_inductor/codegen/triton.py
def get_max_grid_size():
    """
    è®¡ç®—æœ€å¤§ grid size
    
    åŸå› ï¼š
    1. é¿å… kernel launch å¼€é”€è¿‡å¤§
       - æ¯æ¬¡ launch æœ‰å›ºå®šå¼€é”€ï¼ˆ~5Î¼sï¼‰
       - è¿‡å¤§çš„ grid ä¼šå¢åŠ è°ƒåº¦å»¶è¿Ÿ
    
    2. ä¿è¯ SM åˆ©ç”¨ç‡
       - A100 æœ‰ 108 ä¸ª SM
       - å‡è®¾æ¯ä¸ª SM å¯ä»¥å¹¶å‘è¿è¡Œ 4 ä¸ª block
       - é‚£ä¹ˆ grid = 108 * 4 = 432 å°±å·²ç»é¥±å’Œ
       - ä½†è€ƒè™‘åˆ° warp è°ƒåº¦å’Œ memory latency hiding
       - é€šå¸¸è®¾ç½®ä¸º SM_count * 0.5 å·¦å³
    
    3. ç®€åŒ–è°ƒè¯•
       - å° grid æ›´å®¹æ˜“ profile å’Œè°ƒè¯•
    """
    device = torch.cuda.current_device()
    sm_count = torch.cuda.get_device_properties(device).multi_processor_count
    
    # å¯å‘å¼ï¼šSM æ•°é‡çš„ 0.5 å€
    # A100: 108 * 0.5 = 54
    # V100: 80 * 0.5 = 40
    max_grid = max(sm_count // 2, 1)
    
    # ä¹Ÿå¯ä»¥æ‰‹åŠ¨é…ç½®
    if config.triton.max_tiles:
        max_grid = config.triton.max_tiles * sm_count
    
    return max_grid
```

### 5.4 å®æˆ˜ï¼šGrid Size ä¼˜åŒ–

```python
import torch
import triton
import triton.language as tl

# [1] æ— å†…éƒ¨å¾ªç¯ç‰ˆæœ¬
@triton.jit
def add_kernel_v1(x_ptr, y_ptr, out_ptr, n_elements, XBLOCK: tl.constexpr):
    pid = tl.program_id(0)
    offset = pid * XBLOCK
    mask = offset + tl.arange(0, XBLOCK) < n_elements
    x = tl.load(x_ptr + offset + tl.arange(0, XBLOCK), mask)
    y = tl.load(y_ptr + offset + tl.arange(0, XBLOCK), mask)
    out = x + y
    tl.store(out_ptr + offset + tl.arange(0, XBLOCK), out, mask)

# [2] æœ‰å†…éƒ¨å¾ªç¯ç‰ˆæœ¬
@triton.jit
def add_kernel_v2(x_ptr, y_ptr, out_ptr, n_elements, XBLOCK: tl.constexpr):
    pid = tl.program_id(0)
    n_tiles = tl.cdiv(n_elements, XBLOCK)
    tiles_per_pid = tl.cdiv(n_tiles, tl.num_programs(0))
    
    for i in range(tiles_per_pid):
        offset = (pid * tiles_per_pid + i) * XBLOCK
        mask = offset + tl.arange(0, XBLOCK) < n_elements
        x = tl.load(x_ptr + offset + tl.arange(0, XBLOCK), mask)
        y = tl.load(y_ptr + offset + tl.arange(0, XBLOCK), mask)
        out = x + y
        tl.store(out_ptr + offset + tl.arange(0, XBLOCK), out, mask)

# æµ‹è¯•
def benchmark_grid_size():
    n = 100_000_000
    x = torch.randn(n, device='cuda')
    y = torch.randn(n, device='cuda')
    out = torch.empty_like(x)
    
    # V1: å¤§ Grid
    xblock = 1024
    grid_v1 = (triton.cdiv(n, xblock),)
    print(f"V1 Grid size: {grid_v1[0]}")  # 97,657
    
    # V2: å° Gridï¼ˆé™åˆ¶ä¸º 48ï¼‰
    max_grid = 48
    grid_v2 = (min(triton.cdiv(n, xblock), max_grid),)
    print(f"V2 Grid size: {grid_v2[0]}")  # 48
    
    # Benchmark
    import time
    
    # Warmup
    for _ in range(10):
        add_kernel_v1[grid_v1](x, y, out, n, xblock)
    
    torch.cuda.synchronize()
    t0 = time.time()
    for _ in range(100):
        add_kernel_v1[grid_v1](x, y, out, n, xblock)
    torch.cuda.synchronize()
    t1 = time.time()
    print(f"V1 (large grid): {(t1-t0)/100*1000:.3f} ms")
    
    # V2
    for _ in range(10):
        add_kernel_v2[grid_v2](x, y, out, n, xblock)
    
    torch.cuda.synchronize()
    t0 = time.time()
    for _ in range(100):
        add_kernel_v2[grid_v2](x, y, out, n, xblock)
    torch.cuda.synchronize()
    t1 = time.time()
    print(f"V2 (small grid): {(t1-t0)/100*1000:.3f} ms")

# è¾“å‡ºç¤ºä¾‹ï¼ˆA100ï¼‰ï¼š
# V1 Grid size: 97657
# V2 Grid size: 48
# V1 (large grid): 1.234 ms
# V2 (small grid): 1.187 ms  â† ç•¥å¿«ï¼ˆå‡å°‘ launch å¼€é”€ï¼‰
```

---

## 6. Block Size ä¸ Num_Warps é…ç½®

### 6.1 Block Size é€‰æ‹©ç­–ç•¥

```python
def heuristic_block_size(
    numel: int,
    dtype: torch.dtype,
    is_reduction: bool = False,
) -> int:
    """
    å¯å‘å¼é€‰æ‹© Block Size
    
    è€ƒè™‘å› ç´ ï¼š
    1. æ•°æ®ç±»å‹å¤§å°ï¼ˆfloat32=4, float16=2ï¼‰
    2. æ˜¯å¦æ˜¯ Reduction æ“ä½œ
    3. å†…å­˜å¸¦å®½åˆ©ç”¨ç‡
    4. å¯„å­˜å™¨å‹åŠ›
    """
    elem_size = dtype.itemsize
    
    if is_reduction:
        # Reduction éœ€è¦å…±äº«å†…å­˜ï¼Œå€¾å‘äºå° block
        candidates = [128, 256, 512]
    else:
        # Pointwise å¯ä»¥ç”¨å¤§ block
        candidates = [256, 512, 1024]
    
    # æ ¹æ®æ•°æ®å¤§å°è°ƒæ•´
    for block_size in reversed(candidates):
        # æ¯ä¸ª block å¤„ç†çš„å­—èŠ‚æ•°
        bytes_per_block = block_size * elem_size
        
        # æ¡ä»¶ 1: ä¸è¶…è¿‡å…±äº«å†…å­˜é™åˆ¶ï¼ˆé€šå¸¸ 48KBï¼‰
        if bytes_per_block > 48 * 1024:
            continue
        
        # æ¡ä»¶ 2: ä¿è¯è¶³å¤Ÿçš„å¹¶è¡Œåº¦
        grid_size = triton.cdiv(numel, block_size)
        if grid_size < 108:  # è‡³å°‘ç­‰äº SM æ•°é‡
            continue
        
        return block_size
    
    return 256  # é»˜è®¤å€¼

# ç¤ºä¾‹
print(heuristic_block_size(1000000, torch.float32, False))  # â†’ 1024
print(heuristic_block_size(1000000, torch.float32, True))   # â†’ 512
print(heuristic_block_size(1000000, torch.float16, False))  # â†’ 1024
```

### 6.2 Num_Warps ä¸æ€§èƒ½çš„å…³ç³»

```python
# å®éªŒï¼šä¸åŒ num_warps çš„æ€§èƒ½
import torch
import triton
import triton.language as tl

@triton.jit
def vector_add(x_ptr, y_ptr, out_ptr, n, XBLOCK: tl.constexpr):
    pid = tl.program_id(0)
    offsets = pid * XBLOCK + tl.arange(0, XBLOCK)
    mask = offsets < n
    x = tl.load(x_ptr + offsets, mask)
    y = tl.load(y_ptr + offsets, mask)
    out = x + y
    tl.store(out_ptr + offsets, out, mask)

def benchmark_num_warps():
    n = 10_000_000
    x = torch.randn(n, device='cuda')
    y = torch.randn(n, device='cuda')
    
    xblock = 1024
    grid = (triton.cdiv(n, xblock),)
    
    for num_warps in [1, 2, 4, 8, 16, 32]:
        out = torch.empty_like(x)
        
        # Warmup
        for _ in range(10):
            vector_add[grid](x, y, out, n, XBLOCK=xblock, num_warps=num_warps)
        
        # Benchmark
        torch.cuda.synchronize()
        import time
        t0 = time.time()
        for _ in range(100):
            vector_add[grid](x, y, out, n, XBLOCK=xblock, num_warps=num_warps)
        torch.cuda.synchronize()
        t1 = time.time()
        
        print(f"num_warps={num_warps:2d}: {(t1-t0)/100*1000:.3f} ms")

# è¾“å‡ºç¤ºä¾‹ï¼ˆA100ï¼‰ï¼š
# num_warps= 1: 2.456 ms  (å¯„å­˜å™¨å……è¶³ï¼Œä½† warp è°ƒåº¦ä¸å¤Ÿ)
# num_warps= 2: 1.834 ms
# num_warps= 4: 1.523 ms
# num_warps= 8: 1.412 ms  â† æœ€ä¼˜ï¼ˆXBLOCK=1024ï¼Œç†è®ºéœ€è¦ 32 warpsï¼‰
# num_warps=16: 1.398 ms
# num_warps=32: 1.405 ms  (å¯„å­˜å™¨å‹åŠ›å¢å¤§)
```

### 6.3 TorchInductor çš„å®é™…é…ç½®

```python
# torch/_inductor/codegen/triton.py
class TritonKernel:
    def estimate_kernel_num_warps(self):
        """ä¼°ç®—æœ€ä¼˜ num_warps"""
        # [1] åŸºäº threads æ•°é‡
        threads_per_block = self.estimate_threads_per_block()
        min_warps = (threads_per_block + 31) // 32
        
        # [2] åŸºäºå¯„å­˜å™¨ä½¿ç”¨ï¼ˆé€šè¿‡ IR åˆ†æï¼‰
        num_registers = self.estimate_register_usage()
        max_warps_by_regs = 65536 // num_registers  # å‡è®¾ 64K å¯„å­˜å™¨
        
        # [3] åŸºäºå…±äº«å†…å­˜ä½¿ç”¨
        smem_bytes = self.estimate_smem_usage()
        max_warps_by_smem = 49152 // smem_bytes  # å‡è®¾ 48KB å…±äº«å†…å­˜
        
        # [4] å–æœ€å°å€¼ï¼Œå¹¶å‘ä¸Šå–æ•´åˆ° 2 çš„å¹‚æ¬¡
        num_warps = min(min_warps, max_warps_by_regs, max_warps_by_smem)
        num_warps = 2 ** math.ceil(math.log2(max(1, num_warps)))
        num_warps = min(num_warps, 32)  # ç¡¬ä»¶é™åˆ¶
        
        return num_warps
```

---

## 7. AutoTuning æœºåˆ¶

### 7.1 ä»€ä¹ˆæ˜¯ AutoTuning

AutoTuning æ˜¯æŒ‡è‡ªåŠ¨å°è¯•å¤šç»„å‚æ•°é…ç½®ï¼Œé€‰æ‹©æ€§èƒ½æœ€ä¼˜çš„ä¸€ç»„ã€‚

```python
# Triton çš„ autotune è£…é¥°å™¨
import triton

@triton.autotune(
    configs=[
        triton.Config({'XBLOCK': 128}, num_warps=2),
        triton.Config({'XBLOCK': 256}, num_warps=4),
        triton.Config({'XBLOCK': 512}, num_warps=8),
        triton.Config({'XBLOCK': 1024}, num_warps=16),
    ],
    key=['n_elements'],  # æ ¹æ® n_elements ç¼“å­˜æœ€ä¼˜é…ç½®
)
@triton.jit
def vector_add_autotuned(
    x_ptr, y_ptr, out_ptr,
    n_elements,
    XBLOCK: tl.constexpr,
):
    pid = tl.program_id(0)
    offsets = pid * XBLOCK + tl.arange(0, XBLOCK)
    mask = offsets < n_elements
    x = tl.load(x_ptr + offsets, mask)
    y = tl.load(y_ptr + offsets, mask)
    out = x + y
    tl.store(out_ptr + offsets, out, mask)

# ä½¿ç”¨æ—¶ä¼šè‡ªåŠ¨æµ‹è¯•æ‰€æœ‰é…ç½®ï¼Œé€‰æ‹©æœ€å¿«çš„
x = torch.randn(10_000_000, device='cuda')
y = torch.randn_like(x)
out = torch.empty_like(x)

grid = lambda meta: (triton.cdiv(10_000_000, meta['XBLOCK']),)
vector_add_autotuned[grid](x, y, out, 10_000_000)
# ç¬¬ä¸€æ¬¡è°ƒç”¨ä¼šæµ‹è¯•æ‰€æœ‰é…ç½®ï¼ˆçº¦ 100msï¼‰
# åç»­è°ƒç”¨ç›´æ¥ä½¿ç”¨ç¼“å­˜çš„æœ€ä¼˜é…ç½®ï¼ˆçº¦ 1msï¼‰
```

### 7.2 TorchInductor çš„ AutoTuning

TorchInductor é»˜è®¤ä¸å¼€å¯ autotuneï¼ˆå› ä¸ºç¼–è¯‘æ—¶é—´å¤ªé•¿ï¼‰ï¼Œä½†å¯ä»¥æ‰‹åŠ¨å¯ç”¨ï¼š

```python
import torch

# å¯ç”¨ autotune
torch._inductor.config.triton.autotune = True
torch._inductor.config.triton.autotune_at_compile_time = True

@torch.compile
def model(x, y):
    return (x + y).relu()

x = torch.randn(1000000, device='cuda')
y = torch.randn_like(x)

# ç¬¬ä¸€æ¬¡è°ƒç”¨ä¼š autotuneï¼ˆæ…¢ï¼‰
result = model(x, y)

# åç»­è°ƒç”¨ä½¿ç”¨ç¼“å­˜é…ç½®ï¼ˆå¿«ï¼‰
result = model(x, y)
```

### 7.3 AutoTuning çš„å®ç°åŸç†

```python
# torch/_inductor/codegen/triton.py
class TritonScheduling:
    def autotune_kernel(self, kernel_name, kernel_code):
        """
        è‡ªåŠ¨è°ƒä¼˜ kernel å‚æ•°
        
        æµç¨‹ï¼š
        1. ç”Ÿæˆå€™é€‰é…ç½®åˆ—è¡¨
        2. ç¼–è¯‘æ‰€æœ‰é…ç½®çš„ kernel
        3. åœ¨çœŸå®æ•°æ®ä¸Šè¿è¡Œï¼Œæµ‹é‡æ—¶é—´
        4. é€‰æ‹©æœ€å¿«çš„é…ç½®
        5. ç¼“å­˜ç»“æœåˆ°ç£ç›˜
        """
        # [1] ç”Ÿæˆå€™é€‰é…ç½®
        configs = self.generate_autotune_configs()
        # ç¤ºä¾‹ï¼š[
        #   {'XBLOCK': 128, 'num_warps': 2},
        #   {'XBLOCK': 256, 'num_warps': 4},
        #   {'XBLOCK': 512, 'num_warps': 8},
        #   {'XBLOCK': 1024, 'num_warps': 16},
        # ]
        
        # [2] ç¼–è¯‘æ‰€æœ‰ kernel
        compiled_kernels = []
        for config in configs:
            code = self.apply_config(kernel_code, config)
            compiled = triton.compile(code)
            compiled_kernels.append((config, compiled))
        
        # [3] Benchmark
        best_time = float('inf')
        best_config = None
        
        for config, kernel in compiled_kernels:
            # è¿è¡Œ 10 æ¬¡å–å¹³å‡
            times = []
            for _ in range(10):
                torch.cuda.synchronize()
                t0 = time.perf_counter()
                kernel(*args, **kwargs)
                torch.cuda.synchronize()
                t1 = time.perf_counter()
                times.append(t1 - t0)
            
            avg_time = sum(times) / len(times)
            if avg_time < best_time:
                best_time = avg_time
                best_config = config
        
        # [4] ç¼“å­˜ç»“æœ
        self.cache_autotune_result(kernel_name, best_config)
        
        return best_config
```

### 7.4 é…ç½®ç”Ÿæˆç­–ç•¥

```python
def generate_autotune_configs():
    """
    ç”Ÿæˆ autotune å€™é€‰é…ç½®
    
    ç­–ç•¥ï¼š
    1. è¦†ç›–å¸¸è§çš„ XBLOCK å€¼
    2. ä¸ºæ¯ä¸ª XBLOCK é€‰æ‹©åˆé€‚çš„ num_warps
    3. é¿å…æ— æ•ˆé…ç½®ï¼ˆå¦‚ num_warps è¿‡å¤§ï¼‰
    """
    configs = []
    
    # XBLOCK å€™é€‰å€¼ï¼ˆ2 çš„å¹‚æ¬¡ï¼‰
    xblock_options = [128, 256, 512, 1024]
    
    for xblock in xblock_options:
        # è®¡ç®—ç†è®º warp æ•°
        min_warps = (xblock + 31) // 32
        
        # å°è¯•å¤šä¸ª num_warps
        for num_warps in [2, 4, 8, 16]:
            if num_warps >= min_warps:
                configs.append({
                    'XBLOCK': xblock,
                    'num_warps': num_warps,
                })
    
    # æ·»åŠ ä¸€äº›ç‰¹æ®Šé…ç½®
    configs.extend([
        {'XBLOCK': 64, 'num_warps': 1},   # å°æ•°æ®é‡
        {'XBLOCK': 2048, 'num_warps': 32},  # å¤§æ•°æ®é‡ï¼ˆå¦‚æœç¡¬ä»¶æ”¯æŒï¼‰
    ])
    
    return configs
```

---

## 8. å®æˆ˜ï¼šè‡ªå®šä¹‰ä»£ç ç”Ÿæˆç­–ç•¥

### 8.1 ç›®æ ‡ï¼šä¸º GXU è®¾å¤‡å®šåˆ¶ä»£ç ç”Ÿæˆ

å‡è®¾ GXU è®¾å¤‡æœ‰ä»¥ä¸‹ç‰¹æ€§ï¼š
- æ¯ä¸ª SM åªèƒ½å¹¶å‘è¿è¡Œ 2 ä¸ª blockï¼ˆè€Œä¸æ˜¯ CUDA çš„ 16ï¼‰
- Grid size é™åˆ¶ä¸º 32ï¼ˆè€Œä¸æ˜¯ 48ï¼‰
- æœ€å¤§ num_warps = 16ï¼ˆè€Œä¸æ˜¯ 32ï¼‰

### 8.2 è‡ªå®šä¹‰ Heuristics

```python
# my_backend/gxu_heuristics.py
import torch
from torch._inductor.codegen.triton import TritonScheduling

class GXUTritonHeuristics:
    """GXU è®¾å¤‡çš„ Triton ä»£ç ç”Ÿæˆå¯å‘å¼"""
    
    # ç¡¬ä»¶å‚æ•°
    MAX_GRID_SIZE = 32
    MAX_NUM_WARPS = 16
    BLOCKS_PER_SM = 2
    
    @staticmethod
    def get_max_grid_size():
        """è¿”å›æœ€å¤§ grid size"""
        return GXUTritonHeuristics.MAX_GRID_SIZE
    
    @staticmethod
    def select_xblock(numel: int, dtype: torch.dtype) -> int:
        """
        é€‰æ‹© XBLOCK
        
        ç­–ç•¥ï¼šå€¾å‘äºä½¿ç”¨å†…éƒ¨å¾ªç¯ï¼Œå‡å°‘ grid size
        """
        elem_size = dtype.itemsize
        
        # ç›®æ ‡ï¼šgrid_size â‰¤ 32
        # é‚£ä¹ˆæ¯ä¸ª block è‡³å°‘å¤„ç† numel / 32 ä¸ªå…ƒç´ 
        min_xblock = (numel + 31) // 32
        
        # å‘ä¸Šå–æ•´åˆ° 2 çš„å¹‚æ¬¡
        import math
        xblock = 2 ** math.ceil(math.log2(max(min_xblock, 128)))
        
        # é™åˆ¶èŒƒå›´
        xblock = min(xblock, 1024)  # ç¡¬ä»¶é™åˆ¶
        xblock = max(xblock, 128)   # æœ€å°å€¼
        
        return xblock
    
    @staticmethod
    def select_num_warps(xblock: int) -> int:
        """é€‰æ‹© num_warpsï¼ˆé™åˆ¶ä¸º 16ï¼‰"""
        threads = xblock
        min_warps = (threads + 31) // 32
        
        # å‘ä¸Šå–æ•´åˆ° 2 çš„å¹‚æ¬¡
        import math
        num_warps = 2 ** math.ceil(math.log2(min_warps))
        
        # GXU é™åˆ¶
        num_warps = min(num_warps, GXUTritonHeuristics.MAX_NUM_WARPS)
        
        return num_warps
    
    @staticmethod
    def get_grid(numel: int, xblock: int):
        """
        è®¡ç®— grid sizeï¼ˆä½¿ç”¨å†…éƒ¨å¾ªç¯ï¼‰
        """
        # æ€»çš„ tile æ•°
        total_tiles = (numel + xblock - 1) // xblock
        
        # é™åˆ¶ grid size
        grid_size = min(total_tiles, GXUTritonHeuristics.MAX_GRID_SIZE)
        
        return (grid_size,)
```

### 8.3 æ³¨å…¥åˆ° TorchInductor

```python
# my_backend/gxu_inductor.py
from torch._inductor.codegen import triton as inductor_triton
from .gxu_heuristics import GXUTritonHeuristics

def patch_inductor_for_gxu():
    """å°† GXU heuristics æ³¨å…¥ TorchInductor"""
    
    # ä¿å­˜åŸå§‹æ–¹æ³•
    original_get_grid = inductor_triton.TritonScheduling.get_grid
    
    # é‡å†™ get_grid æ–¹æ³•
    def gxu_get_grid(self, numel, xblock):
        # æ£€æŸ¥è®¾å¤‡ç±»å‹
        if self.device.type == 'gxu':
            return GXUTritonHeuristics.get_grid(numel, xblock)
        else:
            return original_get_grid(self, numel, xblock)
    
    inductor_triton.TritonScheduling.get_grid = gxu_get_grid
    
    # ç±»ä¼¼åœ°é‡å†™å…¶ä»–æ–¹æ³•
    # ...

# åœ¨æ¨¡å—åŠ è½½æ—¶è‡ªåŠ¨ patch
patch_inductor_for_gxu()
```

### 8.4 ç”Ÿæˆå¸¦å†…éƒ¨å¾ªç¯çš„ Kernel

```python
# torch/_inductor/codegen/triton.py (ä¿®æ”¹å)
class TritonScheduling:
    def codegen_kernel(self, name):
        """ç”Ÿæˆ Triton kernel ä»£ç """
        numel = self.get_numel()
        dtype = self.get_dtype()
        
        # [1] é€‰æ‹© XBLOCK
        xblock = GXUTritonHeuristics.select_xblock(numel, dtype)
        
        # [2] é€‰æ‹© num_warps
        num_warps = GXUTritonHeuristics.select_num_warps(xblock)
        
        # [3] è®¡ç®— grid
        grid = GXUTritonHeuristics.get_grid(numel, xblock)
        
        # [4] ç”Ÿæˆ kernel ä»£ç ï¼ˆå¸¦å†…éƒ¨å¾ªç¯ï¼‰
        kernel_code = f"""
@triton.jit
def {name}(
    in_ptr0,
    out_ptr0,
    numel,
    XBLOCK: tl.constexpr,
):
    pid = tl.program_id(0)
    
    # è®¡ç®—æ¯ä¸ª block å¤„ç†çš„ tile æ•°
    n_tiles = tl.cdiv(numel, XBLOCK)
    tiles_per_pid = tl.cdiv(n_tiles, tl.num_programs(0))
    
    # å†…éƒ¨å¾ªç¯
    for tile_idx in range(tiles_per_pid):
        offset = (pid * tiles_per_pid + tile_idx) * XBLOCK
        index = offset + tl.arange(0, XBLOCK)
        mask = index < numel
        
        # Load
        x = tl.load(in_ptr0 + index, mask)
        
        # Compute
        y = x * 2.0
        
        # Store
        tl.store(out_ptr0 + index, y, mask)

# è°ƒç”¨
grid = ({grid[0]},)
{name}[grid](
    in_ptr0,
    out_ptr0,
    numel,
    XBLOCK={xblock},
    num_warps={num_warps},
)
"""
        return kernel_code
```

### 8.5 å®Œæ•´ç¤ºä¾‹

```python
# test_gxu_codegen.py
import torch
import torch._dynamo as dynamo
from my_backend.gxu_inductor import patch_inductor_for_gxu

# [1] Patch TorchInductor
patch_inductor_for_gxu()

# [2] å®šä¹‰æ¨¡å‹
@torch.compile
def my_model(x):
    return (x * 2).relu()

# [3] å‡†å¤‡æ•°æ®ï¼ˆä½¿ç”¨ PrivateUse1 ä½œä¸º GXU è®¾å¤‡ï¼‰
torch.utils.rename_privateuse1_backend("gxu")
x = torch.randn(100_000_000, device='gxu')

# [4] è¿è¡Œï¼ˆä¼šä½¿ç”¨ GXU heuristics ç”Ÿæˆä»£ç ï¼‰
y = my_model(x)

# [5] æŸ¥çœ‹ç”Ÿæˆçš„ä»£ç 
print(torch._dynamo.utils.compile_times())
print(torch._inductor.utils.get_last_generated_code())
```

ç”Ÿæˆçš„ä»£ç ç¤ºä¾‹ï¼š

```python
# è‡ªåŠ¨ç”Ÿæˆçš„ Triton kernel
@triton.jit
def triton_poi_fused_mul_relu_0(
    in_ptr0,
    out_ptr0,
    numel,
    XBLOCK: tl.constexpr,
):
    pid = tl.program_id(0)
    
    # æ¯ä¸ª block å¤„ç†å¤šä¸ª tileï¼ˆå› ä¸º grid é™åˆ¶ä¸º 32ï¼‰
    n_tiles = tl.cdiv(numel, XBLOCK)  # 100M / 1024 â‰ˆ 97657
    tiles_per_pid = tl.cdiv(n_tiles, tl.num_programs(0))  # 97657 / 32 â‰ˆ 3052
    
    for tile_idx in range(tiles_per_pid):  # â† å†…éƒ¨å¾ªç¯ 3052 æ¬¡
        offset = (pid * tiles_per_pid + tile_idx) * XBLOCK
        index = offset + tl.arange(0, XBLOCK)
        mask = index < numel
        
        x = tl.load(in_ptr0 + index, mask)
        y = x * 2.0
        y = tl.maximum(y, 0.0)
        tl.store(out_ptr0 + index, y, mask)

# è°ƒç”¨é…ç½®
grid = (32,)  # â† é™åˆ¶ä¸º 32
triton_poi_fused_mul_relu_0[grid](
    in_ptr0, out_ptr0, 100_000_000,
    XBLOCK=1024,
    num_warps=16,  # â† é™åˆ¶ä¸º 16
)
```

---

## 9. æ€§èƒ½åˆ†æä¸è°ƒä¼˜

### 9.1 ä½¿ç”¨ Nsight Compute åˆ†æ Kernel

```bash
# å®‰è£… NVIDIA Nsight Compute
# https://developer.nvidia.com/nsight-compute

# è¿è¡Œ profiling
ncu --set full -o profile_output python test_script.py

# æŸ¥çœ‹ç»“æœ
ncu-ui profile_output.ncu-rep
```

å…³é”®æŒ‡æ ‡ï¼š
- **Occupancyï¼ˆå ç”¨ç‡ï¼‰**: å®é™…æ´»è·ƒ warp æ•° / ç†è®ºæœ€å¤§ warp æ•°
  - ç›®æ ‡ï¼š> 50%
  - è¿‡ä½ï¼šå¢åŠ  num_warps æˆ–å‡å°‘å¯„å­˜å™¨ä½¿ç”¨
- **Memory Throughputï¼ˆå†…å­˜ååï¼‰**: å®é™…å¸¦å®½ / å³°å€¼å¸¦å®½
  - ç›®æ ‡ï¼š> 80%ï¼ˆå¯¹äº memory-bound kernelï¼‰
- **SM Efficiencyï¼ˆSM æ•ˆç‡ï¼‰**: SM å¿™ç¢Œæ—¶é—´ / æ€»æ—¶é—´
  - ç›®æ ‡ï¼š> 90%

### 9.2 ä½¿ç”¨ Triton Profiler

```python
import torch
import triton

# å¯ç”¨ profiling
triton.Config.enable_debug = True

@torch.compile
def model(x):
    return x.relu()

x = torch.randn(1000000, device='cuda')

# è¿è¡Œ
with torch.profiler.profile(
    activities=[torch.profiler.ProfilerActivity.CUDA],
    record_shapes=True,
) as prof:
    y = model(x)

# æ‰“å°ç»“æœ
print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
```

### 9.3 è°ƒä¼˜ Checklist

| é—®é¢˜ | ç—‡çŠ¶ | è§£å†³æ–¹æ¡ˆ |
|-----|------|---------|
| **Grid è¿‡å¤§** | Kernel launch å¼€é”€é«˜ | ä½¿ç”¨å†…éƒ¨å¾ªç¯ï¼Œé™åˆ¶ grid â‰¤ 48 |
| **Block è¿‡å°** | SM å ç”¨ç‡ä½ | å¢åŠ  XBLOCKï¼Œå‡å°‘ grid |
| **Warp ä¸è¶³** | æ— æ³•éšè—å†…å­˜å»¶è¿Ÿ | å¢åŠ  num_warps |
| **Warp è¿‡å¤š** | å¯„å­˜å™¨æº¢å‡º | å‡å°‘ num_warpsï¼Œç®€åŒ–è®¡ç®— |
| **å†…å­˜ä¸å¯¹é½** | å¸¦å®½åˆ©ç”¨ç‡ä½ | è°ƒæ•´ XBLOCK ä½¿å…¶ä¸º 128 å­—èŠ‚å€æ•° |
| **Bank Conflict** | å…±äº«å†…å­˜è®¿é—®æ…¢ | ä½¿ç”¨ padding æˆ–è°ƒæ•´è®¿é—®æ¨¡å¼ |

---

## 10. å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆæˆ‘çš„ kernel æ€§èƒ½ä¸å¦‚æ‰‹å†™çš„ CUDAï¼Ÿ

**A:** å¯èƒ½åŸå› ï¼š
1. **æœªå¼€å¯ autotune**ï¼šTorchInductor é»˜è®¤ä½¿ç”¨å›ºå®šå¯å‘å¼
   ```python
   torch._inductor.config.triton.autotune = True
   ```

2. **Fusion ä¸å¤Ÿæ¿€è¿›**ï¼šæ‰‹åŠ¨è°ƒæ•´ fusion ç­–ç•¥
   ```python
   torch._inductor.config.max_fusion_size = 64
   ```

3. **å†…å­˜å¸ƒå±€ä¸optimal**ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ä¸å¿…è¦çš„ transpose
   ```python
   # æŸ¥çœ‹ç”Ÿæˆçš„ä»£ç 
   torch._inductor.config.debug = True
   ```

### Q2: Grid size ä¸ºä»€ä¹ˆé™åˆ¶ä¸º 48ï¼Ÿ

**A:** è¿™æ˜¯ä¸€ä¸ªä¿å®ˆçš„å¯å‘å¼å€¼ï¼š
- A100 æœ‰ 108 ä¸ª SMï¼Œ48 çº¦ä¸º 0.5 å€
- é¿å… kernel launch å¼€é”€è¿‡å¤§
- å¹³è¡¡è°ƒåº¦å»¶è¿Ÿå’Œå¹¶è¡Œåº¦

å¯ä»¥ä¿®æ”¹ï¼š
```python
torch._inductor.config.triton.max_tiles = 2  # æ¯ä¸ª SM 2 ä¸ª tile
# å®é™… grid = SM_count * max_tiles
```

### Q3: å¦‚ä½•ä¸ºæ–°ç¡¬ä»¶æ·»åŠ è‡ªå®šä¹‰ heuristicsï¼Ÿ

**A:** ä¸‰ä¸ªæ­¥éª¤ï¼š
1. å®ç° `DeviceOpOverrides` ç±»ï¼ˆå‚è€ƒç¬¬ 8 ç« ï¼‰
2. å®ç°è‡ªå®šä¹‰ heuristics å‡½æ•°
3. Patch `TritonScheduling` çš„ç›¸å…³æ–¹æ³•

```python
from torch._inductor.codegen import triton as inductor_triton

class MyDeviceHeuristics:
    @staticmethod
    def select_xblock(numel, dtype):
        # è‡ªå®šä¹‰é€»è¾‘
        return 512
    
    @staticmethod
    def select_num_warps(xblock):
        return 8

# Patch
original_select_xblock = inductor_triton.TritonScheduling.select_xblock
def patched_select_xblock(self, numel, dtype):
    if self.device.type == 'mydevice':
        return MyDeviceHeuristics.select_xblock(numel, dtype)
    return original_select_xblock(self, numel, dtype)

inductor_triton.TritonScheduling.select_xblock = patched_select_xblock
```

### Q4: å†…éƒ¨å¾ªç¯ä¼šé™ä½æ€§èƒ½å—ï¼Ÿ

**A:** ä¸ä¸€å®šï¼š
- **ä¼˜ç‚¹**ï¼šå‡å°‘ kernel launch å¼€é”€ï¼Œæ›´å¥½çš„æŒ‡ä»¤æµæ°´çº¿
- **ç¼ºç‚¹**ï¼šå¢åŠ å¯„å­˜å™¨å‹åŠ›ï¼Œå¯èƒ½é™ä½å ç”¨ç‡

å®è·µä¸­ï¼š
- å¦‚æœ `grid_size > 10000`ï¼Œä½¿ç”¨å†…éƒ¨å¾ªç¯é€šå¸¸æ›´å¿«
- å¦‚æœ `grid_size < 1000`ï¼Œç›´æ¥å¤§ grid å¯èƒ½æ›´å¿«
- éœ€è¦æ ¹æ®å®é™…ç¡¬ä»¶ benchmark

### Q5: å¦‚ä½•è°ƒè¯•ç”Ÿæˆçš„ Triton ä»£ç ï¼Ÿ

**A:** 
```python
# [1] æ‰“å°ç”Ÿæˆçš„ä»£ç 
torch._inductor.config.debug = True
torch._inductor.config.trace.enabled = True

@torch.compile
def model(x):
    return x.relu()

x = torch.randn(100, device='cuda')
y = model(x)

# [2] æŸ¥çœ‹æ—¥å¿—
# ä¼šæ‰“å°åˆ° stdoutï¼ŒåŒ…å«å®Œæ•´çš„ Triton ä»£ç 

# [3] ä¿å­˜åˆ°æ–‡ä»¶
torch._inductor.config.trace.log_output_code = True
torch._inductor.config.trace.log_dir = "./inductor_logs"

# [4] ä½¿ç”¨ Triton çš„è°ƒè¯•å·¥å…·
import triton
triton.Config.enable_debug = True
```

---

## 11. æ€»ç»“

### 11.1 å…³é”®è¦ç‚¹

1. **ä»£ç ç”Ÿæˆæµç¨‹**
   - FX Graph â†’ Lowering â†’ Fusion â†’ Scheduling â†’ Code Emission â†’ Compilation

2. **Triton Kernel å‚æ•°**
   - `XBLOCK`: å½±å“å¹¶è¡Œåº¦å’Œå†…å­˜è®¿é—®
   - `num_warps`: å½±å“ SM å ç”¨ç‡å’Œå¯„å­˜å™¨ä½¿ç”¨
   - `grid`: å½±å“ kernel launch å¼€é”€å’Œå†…éƒ¨å¾ªç¯

3. **å¯å‘å¼ç­–ç•¥**
   - Grid size: é€šå¸¸é™åˆ¶ä¸º SM_count * 0.5ï¼ˆå¦‚ 48ï¼‰
   - Block size: æ ¹æ®æ•°æ®ç±»å‹å’Œæ“ä½œç±»å‹é€‰æ‹©ï¼ˆ256-1024ï¼‰
   - Num_warps: æ ¹æ® threads æ•°é‡å’Œå¯„å­˜å™¨å‹åŠ›é€‰æ‹©ï¼ˆ1-32ï¼‰

4. **æ€§èƒ½ä¼˜åŒ–**
   - ä½¿ç”¨ AutoTuning è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜é…ç½®
   - é’ˆå¯¹ç‰¹å®šç¡¬ä»¶å®šåˆ¶ heuristics
   - ä½¿ç”¨ profiler åˆ†æç“¶é¢ˆ

### 11.2 æœ€ä½³å®è·µ

```python
# âœ… æ¨èï¼šä¸ºè‡ªå®šä¹‰è®¾å¤‡æä¾›å®Œæ•´çš„ heuristics
class MyDeviceHeuristics:
    MAX_GRID = 32
    MAX_WARPS = 16
    
    @staticmethod
    def select_xblock(numel, dtype):
        # ç¡®ä¿ grid ä¸è¶…è¿‡é™åˆ¶
        min_xblock = (numel + MyDeviceHeuristics.MAX_GRID - 1) // MyDeviceHeuristics.MAX_GRID
        xblock = 2 ** math.ceil(math.log2(max(min_xblock, 128)))
        return min(xblock, 1024)
    
    @staticmethod
    def select_num_warps(xblock):
        num_warps = 2 ** math.ceil(math.log2((xblock + 31) // 32))
        return min(num_warps, MyDeviceHeuristics.MAX_WARPS)
    
    @staticmethod
    def get_grid(numel, xblock):
        tiles = (numel + xblock - 1) // xblock
        return (min(tiles, MyDeviceHeuristics.MAX_GRID),)

# âœ… æ¨èï¼šå¯ç”¨ debug æ¨¡å¼æŸ¥çœ‹ç”Ÿæˆçš„ä»£ç 
torch._inductor.config.debug = True

# âœ… æ¨èï¼šå¯¹æ€§èƒ½å…³é”®çš„ kernel ä½¿ç”¨ autotune
torch._inductor.config.triton.autotune = True

# âŒ é¿å…ï¼šç›²ç›®å¢å¤§ XBLOCK
# åŸå› ï¼šå¯èƒ½å¯¼è‡´ grid è¿‡å°ï¼Œå¹¶è¡Œåº¦ä¸è¶³

# âŒ é¿å…ï¼šä½¿ç”¨è¿‡å¤§çš„ num_warps
# åŸå› ï¼šä¼šå¯¼è‡´å¯„å­˜å™¨æº¢å‡ºï¼Œé™ä½å ç”¨ç‡
```

### 11.3 å‚è€ƒèµ„æ–™

- [Triton Language Reference](https://triton-lang.org/main/programming-guide/index.html)
- [CUDA C Programming Guide - Occupancy](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#occupancy)
- [TorchInductor Source Code](https://github.com/pytorch/pytorch/tree/main/torch/_inductor)
- [Nsight Compute User Guide](https://docs.nvidia.com/nsight-compute/NsightCompute/index.html)

---

**ä¸‹ä¸€ç« é¢„å‘Š**ï¼šç¬¬ 11 ç« å°†è®¨è®º **å†…å­˜ä¼˜åŒ–ä¸æ•°æ®å¸ƒå±€**ï¼ŒåŒ…æ‹¬ memory planningã€buffer reuseã€ä»¥åŠå¦‚ä½•å‡å°‘ä¸­é—´å¼ é‡çš„å†…å­˜å ç”¨ã€‚

---

Â© 2024 LLM-BOOK. æœ¬æ–‡æ¡£ä»…ä¾›å­¦ä¹ å‚è€ƒä½¿ç”¨ã€‚

