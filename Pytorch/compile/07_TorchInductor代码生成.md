# ç¬¬ä¸ƒç« ï¼šTorchInductor ä»£ç ç”Ÿæˆ â€”â€” ä»è®¡ç®—å›¾åˆ°æœºå™¨ç 

## æœ¬ç« ç›®æ ‡

- **æ ¸å¿ƒåŸç†**ï¼šç†è§£ TorchInductor å¦‚ä½•å°† FX Graph ç¼–è¯‘ä¸ºé«˜æ•ˆçš„æœºå™¨ç ã€‚
- **ä»£ç ç”Ÿæˆ**ï¼šæŒæ¡ Triton è¯­è¨€å’Œ GPU Kernel ç”Ÿæˆæœºåˆ¶ã€‚
- **ä¼˜åŒ–æŠ€æœ¯**ï¼šå­¦ä¹ ç®—å­èåˆã€å†…å­˜è§„åˆ’ã€å¾ªç¯ä¼˜åŒ–ç­‰ç¼–è¯‘å™¨æŠ€æœ¯ã€‚
- **å®æˆ˜æ€§èƒ½**ï¼šäº†è§£å¦‚ä½•è°ƒä¼˜å’Œè°ƒè¯•ç”Ÿæˆçš„ä»£ç ã€‚

---

## 1. ä¸ºä»€ä¹ˆéœ€è¦ TorchInductorï¼Ÿ

### 1.1 ç¼–è¯‘æ ˆçš„æœ€åä¸€ç¯

åœ¨ `torch.compile` çš„ä¸‰å±‚æ¶æ„ä¸­ï¼š
1.  **Dynamo**ï¼šæ•è·å‰å‘å›¾ï¼ˆPython å­—èŠ‚ç  â†’ FX Graphï¼‰
2.  **AOTAutograd**ï¼šç”Ÿæˆåå‘å›¾å¹¶è”åˆä¼˜åŒ–
3.  **TorchInductor**ï¼š**å°† FX Graph ç¼–è¯‘ä¸º GPU/CPU çš„æœºå™¨ç **

**å½¢è±¡æ¯”å–»**ï¼š
*   **Dynamo** æ˜¯å»ºç­‘è®¾è®¡å¸ˆï¼Œç”»å‡ºæˆ¿å­çš„è“å›¾ï¼ˆè®¡ç®—å›¾ï¼‰ã€‚
*   **AOTAutograd** æ˜¯ç»“æ„å·¥ç¨‹å¸ˆï¼Œç¡®ä¿æˆ¿å­ç»“æ„åˆç†ï¼ˆå‰å‘+åå‘ä¼˜åŒ–ï¼‰ã€‚
*   **TorchInductor** æ˜¯æ–½å·¥é˜Ÿï¼ŒæŠŠè“å›¾å˜æˆçœŸæ­£çš„æˆ¿å­ï¼ˆé«˜æ•ˆæœºå™¨ç ï¼‰ã€‚

### 1.2 ä¼ ç»Ÿæ–¹å¼çš„å±€é™

**PyTorch Eager æ¨¡å¼**ï¼š
```python
# æ¯ä¸ªæ“ä½œéƒ½æ˜¯ç‹¬ç«‹çš„ Python è°ƒç”¨
y = x + 1      # è°ƒç”¨ aten::add
z = y * 2      # è°ƒç”¨ aten::mul
out = z.sin()  # è°ƒç”¨ aten::sin

# é—®é¢˜ï¼š
# 1. æ¯ä¸ªæ“ä½œå¯åŠ¨ä¸€ä¸ªç‹¬ç«‹çš„ GPU Kernelï¼ˆå¯åŠ¨å¼€é”€å¤§ï¼‰
# 2. ä¸­é—´ç»“æœï¼ˆy, zï¼‰éƒ½éœ€è¦å†™å›æ˜¾å­˜ï¼ˆå†…å­˜å¸¦å®½æµªè´¹ï¼‰
# 3. æ— æ³•è¿›è¡Œè·¨æ“ä½œçš„ä¼˜åŒ–
```

**TorchInductor çš„ä¼˜åŒ–**ï¼š
```python
compiled_fn = torch.compile(lambda x: ((x + 1) * 2).sin())

# TorchInductor ç”Ÿæˆèåˆåçš„ Kernelï¼š
# out = sin((x + 1) * 2)  # å•ä¸ª Kernelï¼Œä¸€æ¬¡å†…å­˜è¯»å†™ï¼
```

**æ€§èƒ½æå‡**ï¼š
*   **å‡å°‘ Kernel å¯åŠ¨æ¬¡æ•°**ï¼š3 ä¸ª Kernel â†’ 1 ä¸ª
*   **å‡å°‘å†…å­˜è®¿é—®**ï¼š3 æ¬¡è¯»å†™ â†’ 1 æ¬¡è¯»å†™
*   **ç†è®ºåŠ é€Ÿæ¯”**ï¼šçº¦ **3-5x**ï¼ˆå®é™…å–å†³äºç®—å­ç±»å‹ï¼‰

---

## 2. æ ¸å¿ƒåŸç†ï¼šä»£ç ç”Ÿæˆæµç¨‹

### 2.1 æ•´ä½“æ¶æ„

```mermaid
graph TD
    A["FX Graph (æ¥è‡ª AOTAutograd)"] --> B["å›¾åˆ†æä¸ä¼˜åŒ–"]
    B --> C["ç®—å­èåˆ (Fusion)"]
    C --> D["å†…å­˜è§„åˆ’ (Memory Planning)"]
    D --> E{ç›®æ ‡è®¾å¤‡?}
    E -- GPU --> F["Triton ä»£ç ç”Ÿæˆ"]
    E -- CPU --> G["C++/OpenMP ä»£ç ç”Ÿæˆ"]
    F --> H["ç¼–è¯‘ä¸º CUDA Kernel"]
    G --> I["ç¼–è¯‘ä¸ºäºŒè¿›åˆ¶ä»£ç "]
    H --> J["ç¼“å­˜ç¼–è¯‘ç»“æœ"]
    I --> J
    J --> K["æ‰§è¡Œä¼˜åŒ–åçš„ä»£ç "]
```

### 2.2 å…³é”®æ­¥éª¤è¯¦è§£

#### æ­¥éª¤ 1ï¼šå›¾åˆ†æ

TorchInductor é¦–å…ˆåˆ†æ FX Graphï¼Œè¯†åˆ«ï¼š
*   **å“ªäº›æ“ä½œå¯ä»¥èåˆ**ï¼Ÿï¼ˆå¦‚é€å…ƒç´ æ“ä½œï¼‰
*   **æ•°æ®ä¾èµ–å…³ç³»**ï¼Ÿï¼ˆç¡®ä¿æ­£ç¡®æ€§ï¼‰
*   **å†…å­˜è®¿é—®æ¨¡å¼**ï¼Ÿï¼ˆä¼˜åŒ–å†…å­˜å¸ƒå±€ï¼‰

#### æ­¥éª¤ 2ï¼šç®—å­èåˆ

**æœªèåˆçš„è®¡ç®—**ï¼š
```python
# å‰å‘å›¾
x1 = x + 1       # Kernel 1: read x, write x1
x2 = x1 * 2      # Kernel 2: read x1, write x2
x3 = torch.sin(x2)  # Kernel 3: read x2, write x3
```

**èåˆåçš„è®¡ç®—**ï¼š
```python
# èåˆ Kernelï¼šread x, write x3
x3 = torch.sin((x + 1) * 2)
```

**å†…å­˜å¸¦å®½èŠ‚çœè®¡ç®—**ï¼š
```
æœªèåˆï¼š3 æ¬¡è¯» + 3 æ¬¡å†™ = 6 æ¬¡å†…å­˜è®¿é—®
èåˆåï¼š1 æ¬¡è¯» + 1 æ¬¡å†™ = 2 æ¬¡å†…å­˜è®¿é—®
èŠ‚çœï¼š(6 - 2) / 6 = 66.7% çš„å†…å­˜å¸¦å®½
```

#### æ­¥éª¤ 3ï¼šä»£ç ç”Ÿæˆ

TorchInductor ä½¿ç”¨ **Triton**ï¼ˆä¸€ç§ Python DSLï¼‰ç”Ÿæˆ GPU ä»£ç ã€‚

**Triton ä»£ç ç¤ºä¾‹**ï¼š
```python
import triton
import triton.language as tl

@triton.jit
def fused_kernel(x_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):
    # è·å–å½“å‰çº¿ç¨‹å—çš„èµ·å§‹ä½ç½®
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    
    # è¾¹ç•Œæ£€æŸ¥
    mask = offsets < n_elements
    
    # ä»å…¨å±€å†…å­˜åŠ è½½æ•°æ®
    x = tl.load(x_ptr + offsets, mask=mask)
    
    # èåˆè®¡ç®—ï¼š(x + 1) * 2 ç„¶å sin
    temp = (x + 1.0) * 2.0
    out = tl.sin(temp)
    
    # å†™å›å…¨å±€å†…å­˜
    tl.store(out_ptr + offsets, out, mask=mask)
```

**å…³é”®ç‰¹æ€§**ï¼š
*   **è‡ªåŠ¨å¹¶è¡Œ**ï¼šTriton ç¼–è¯‘å™¨è‡ªåŠ¨ç”Ÿæˆ CUDA çº¿ç¨‹ç½‘æ ¼ã€‚
*   **å†…å­˜åˆå¹¶**ï¼šä¼˜åŒ–å…¨å±€å†…å­˜è®¿é—®æ¨¡å¼ã€‚
*   **å¯„å­˜å™¨ä¼˜åŒ–**ï¼šä¸­é—´å˜é‡ï¼ˆ`temp`ï¼‰ä¿å­˜åœ¨å¯„å­˜å™¨ä¸­ï¼Œä¸å†™å›æ˜¾å­˜ã€‚

---

## 3. æ·±å…¥åº•å±‚ï¼šTriton vs æ‰‹å†™ CUDA

### 3.1 ä¸ºä»€ä¹ˆé€‰æ‹© Tritonï¼Ÿ

**æ‰‹å†™ CUDA çš„ç—›ç‚¹**ï¼š
1.  **å¤æ‚æ€§é«˜**ï¼šéœ€è¦ç®¡ç†çº¿ç¨‹å—ã€å…±äº«å†…å­˜ã€å¯„å­˜å™¨åˆ†é…ã€‚
2.  **å¯ç§»æ¤æ€§å·®**ï¼šä¸åŒ GPU æ¶æ„ï¼ˆå¦‚ A100 vs V100ï¼‰éœ€è¦æ‰‹åŠ¨è°ƒä¼˜ã€‚
3.  **å¼€å‘æ•ˆç‡ä½**ï¼šä¸€ä¸ªç®€å•çš„èåˆ Kernel å¯èƒ½éœ€è¦æ•°ç™¾è¡Œä»£ç ã€‚

**Triton çš„ä¼˜åŠ¿**ï¼š
1.  **Python è¯­æ³•**ï¼šæ˜“äºå­¦ä¹ å’Œç¼–å†™ã€‚
2.  **è‡ªåŠ¨ä¼˜åŒ–**ï¼šç¼–è¯‘å™¨è‡ªåŠ¨å¤„ç†å†…å­˜è®¿é—®ã€çº¿ç¨‹è°ƒåº¦ç­‰ã€‚
3.  **æ€§èƒ½æ¥è¿‘æ‰‹å†™ CUDA**ï¼šé€šå¸¸èƒ½è¾¾åˆ°æ‰‹å†™ CUDA çš„ 90-95% æ€§èƒ½ã€‚

### 3.2 Triton ç¼–è¯‘æµç¨‹

```mermaid
graph LR
    A["Triton Python ä»£ç "] --> B["Triton IR"]
    B --> C["LLVM IR"]
    C --> D["PTX (GPU æ±‡ç¼–)"]
    D --> E["CUDA Binary"]
    E --> F["GPU æ‰§è¡Œ"]
    
    style D fill:#f96
```

**ç¤ºä¾‹å¯¹æ¯”**ï¼š

<details>
<summary>æ‰‹å†™ CUDAï¼ˆçº¦ 50 è¡Œä»£ç ï¼‰</summary>

```cuda
__global__ void fused_kernel(float* x, float* out, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        float temp = (x[idx] + 1.0f) * 2.0f;
        out[idx] = sinf(temp);
    }
}

// å¯åŠ¨é…ç½®
int threads = 256;
int blocks = (n + threads - 1) / threads;
fused_kernel<<<blocks, threads>>>(x, out, n);
```
</details>

<details>
<summary>Triton ä»£ç ï¼ˆçº¦ 10 è¡Œæ ¸å¿ƒé€»è¾‘ï¼‰</summary>

```python
@triton.jit
def fused_kernel(x_ptr, out_ptr, n, BLOCK_SIZE: tl.constexpr):
    pid = tl.program_id(0)
    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n
    x = tl.load(x_ptr + offsets, mask=mask)
    out = tl.sin((x + 1.0) * 2.0)
    tl.store(out_ptr + offsets, out, mask=mask)
```
</details>

**ä»£ç é‡å¯¹æ¯”**ï¼šTriton å‡å°‘ **80% çš„ä»£ç **ï¼ŒåŒæ—¶æ€§èƒ½ç›¸å½“ï¼

---

## 4. å®æˆ˜ç¤ºä¾‹ï¼šæŸ¥çœ‹ç”Ÿæˆçš„ä»£ç 

### 4.1 åŸºæœ¬ç¤ºä¾‹

```python
import torch

def simple_fn(x):
    return ((x + 1) * 2).sin()

# ç¼–è¯‘å¹¶æŸ¥çœ‹ç”Ÿæˆçš„ä»£ç 
compiled_fn = torch.compile(simple_fn)

# å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œä¿å­˜ç”Ÿæˆçš„ä»£ç 
import torch._inductor.config as config
config.debug = True
config.trace.enabled = True

x = torch.randn(1024, device='cuda')
output = compiled_fn(x)

# ç”Ÿæˆçš„ä»£ç ä¼šä¿å­˜åœ¨ï¼š
# /tmp/torchinductor_<username>/
print("ç”Ÿæˆçš„ä»£ç è·¯å¾„:", config.trace.debug_dir)
```

### 4.2 æŸ¥çœ‹ Triton æºä»£ç 

**æ–¹æ³• 1ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡æ‰“å°åˆ°æ§åˆ¶å°ï¼ˆæ¨èï¼‰**

```python
import torch
import os

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œæ‰“å°ç”Ÿæˆçš„ä»£ç 
os.environ['TORCH_LOGS'] = '+output_code'

def simple_fn(x):
    return ((x + 1) * 2).sin()

compiled_fn = torch.compile(simple_fn)
x = torch.randn(1024, device='cuda')
output = compiled_fn(x)
```

**è¾“å‡ºï¼ˆä¼šç›´æ¥æ‰“å°åˆ°æ§åˆ¶å°ï¼‰ï¼š**

```python
# è‡ªåŠ¨ç”Ÿæˆçš„ Triton Kernel
@triton.jit
def triton_poi_fused_add_mul_sin_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = 1.0
    tmp2 = tmp0 + tmp1  # x + 1
    tmp3 = 2.0
    tmp4 = tmp2 * tmp3  # * 2
    tmp5 = tl.sin(tmp4)  # sin
    tl.store(out_ptr0 + (x0), tmp5, xmask)
```

**æ–¹æ³• 2ï¼šä¿å­˜åˆ°æ–‡ä»¶**

```python
import torch
import torch._inductor.config as config

# é…ç½®ä¿å­˜ç”Ÿæˆçš„ä»£ç 
config.debug = True
config.trace.enabled = True
config.trace.debug_dir = "/tmp/inductor_debug"  # è‡ªå®šä¹‰è·¯å¾„

def simple_fn(x):
    return ((x + 1) * 2).sin()

compiled_fn = torch.compile(simple_fn)
x = torch.randn(1024, device='cuda')
output = compiled_fn(x)

# æŸ¥çœ‹ç”Ÿæˆçš„ Python ä»£ç æ–‡ä»¶
# åœ¨ /tmp/inductor_debug/ ç›®å½•ä¸‹æ‰¾ output_code.py
print(f"ä»£ç ä¿å­˜åœ¨: {config.trace.debug_dir}")
```

**æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶ï¼š**

```bash
# åœ¨ç”Ÿæˆçš„ç›®å½•ä¸­æŸ¥æ‰¾ output_code.py
ls /tmp/inductor_debug/
cat /tmp/inductor_debug/*/output_code.py
```

**output code**

```python
# AOT ID: ['0_inference']
from ctypes import c_void_p, c_long, c_int
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from cmath import nanj
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align
from torch import device, empty_strided
from torch._inductor.async_compile import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
from torch._inductor.codegen.multi_kernel import MultiKernelCall
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
from torch._C import _cuda_getCurrentRawStream as get_raw_stream

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
_quantized = torch.ops._quantized
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
alloc_from_pool = torch.ops.inductor._alloc_from_pool
async_compile = AsyncCompile()
empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p


# kernel path: /tmp/torchinductor_root/22/c22sdgamlk64ygyog2jgyspawzthh5xjbre6u57jkwfjrbn2noae.py
# Topologically Sorted Source Nodes: [add, mul, sin], Original ATen: [aten.add, aten.mul, aten.sin]
# Source node to ATen node mapping:
#   add => add
#   mul => mul
#   sin => sin
# Graph fragment:
#   %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%arg0_1, 1), kwargs = {})
#   %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add, 2), kwargs = {})
#   %sin : [num_users=1] = call_function[target=torch.ops.aten.sin.default](args = (%mul,), kwargs = {})
triton_poi_fused_add_mul_sin_0 = async_compile.triton('triton_poi_fused_add_mul_sin_0', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints={'x': 1024}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=170, cc=120, major=12, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_mul_sin_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '55A6EF493402E0BEF4C44F13B3A4DFE62485551D5064B7A43C872C664AE8E85A', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_mul_sin_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1024
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = 1.0
    tmp2 = tmp0 + tmp1
    tmp3 = 2.0
    tmp4 = tmp2 * tmp3
    tmp5 = tl_math.sin(tmp4)
    tl.store(out_ptr0 + (x0), tmp5, xmask)
''', device_str='cuda')


async_compile.wait(globals())
del async_compile

def call(args):
    arg0_1, = args
    args.clear()
    assert_size_stride(arg0_1, (1024, ), (1, ))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((1024, ), (1, ), torch.float32)
        # Topologically Sorted Source Nodes: [add, mul, sin], Original ATen: [aten.add, aten.mul, aten.sin]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_mul_sin_0.run(arg0_1, buf0, 1024, stream=stream0)
        del arg0_1
    return (buf0, )


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    arg0_1 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
    fn = lambda: call([arg0_1])
    return print_performance(fn, times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('None', benchmark_compiled_module)

```

**ç¼–è¯‘äº§ç‰©**ï¼š
```bash
/tmp/torchinductor_root/triton/0/LBHSQGOAZD44.../
â”œâ”€â”€ triton_poi_fused_add_mul_sin_0.cubin   # CUDA äºŒè¿›åˆ¶ï¼ˆGPU ç›´æ¥æ‰§è¡Œçš„æœºå™¨ç ï¼‰
â”œâ”€â”€ triton_poi_fused_add_mul_sin_0.ptx     # PTX æ±‡ç¼–ä»£ç ï¼ˆGPU æ±‡ç¼–è¯­è¨€ï¼‰
â”œâ”€â”€ triton_poi_fused_add_mul_sin_0.ttir    # Triton IRï¼ˆTriton ä¸­é—´è¡¨ç¤ºï¼‰
â”œâ”€â”€ triton_poi_fused_add_mul_sin_0.ttgir   # Triton GPU IR
â”œâ”€â”€ triton_poi_fused_add_mul_sin_0.llir    # LLVM IR
â””â”€â”€ *.json                                 # å…ƒæ•°æ®
```

è¿™äº›éƒ½æ˜¯**ç¼–è¯‘äº§ç‰©**ï¼Œä¸æ˜¯æºä»£ç ã€‚è¦æŸ¥çœ‹ Triton Python æºä»£ç ï¼Œå¿…é¡»ä½¿ç”¨ä¸Šè¿°æ–¹æ³• 1 æˆ– 2ã€‚

**å…³é”®è§‚å¯Ÿï¼ˆä»æºä»£ç ä¸­ï¼‰**ï¼š
*   æ‰€æœ‰ä¸­é—´å€¼ï¼ˆ`tmp2`, `tmp4`ï¼‰éƒ½ä¿å­˜åœ¨å¯„å­˜å™¨ä¸­ã€‚
*   åªæœ‰ä¸€æ¬¡ `tl.load`ï¼ˆè¯»å– `x`ï¼‰å’Œä¸€æ¬¡ `tl.store`ï¼ˆå†™å…¥ç»“æœï¼‰ã€‚
*   å®Œç¾çš„ç®—å­èåˆï¼

### 4.3 æ€§èƒ½å¯¹æ¯”

```python
import torch
import time

x = torch.randn(10000000, device='cuda')

# Eager æ¨¡å¼
torch.cuda.synchronize()
start = time.time()
for _ in range(100):
    y = ((x + 1) * 2).sin()
torch.cuda.synchronize()
eager_time = time.time() - start

# Compiled æ¨¡å¼
compiled_fn = torch.compile(lambda x: ((x + 1) * 2).sin())
torch.cuda.synchronize()
start = time.time()
for _ in range(100):
    y = compiled_fn(x)
torch.cuda.synchronize()
compiled_time = time.time() - start

print(f"Eager æ¨¡å¼: {eager_time*1000:.2f} ms")
print(f"Compiled æ¨¡å¼: {compiled_time*1000:.2f} ms")
print(f"åŠ é€Ÿæ¯”: {eager_time/compiled_time:.2f}x")
```

**å…¸å‹ç»“æœ**ï¼š
```
Eager æ¨¡å¼: 45.23 ms
Compiled æ¨¡å¼: 15.67 ms
åŠ é€Ÿæ¯”: 2.89x
```

---

## 5. é«˜çº§ä¼˜åŒ–æŠ€æœ¯

### 5.1 å¾ªç¯ä¼˜åŒ–ï¼ˆLoop Tilingï¼‰

å¯¹äºå¤§å‹ Tensorï¼ŒTorchInductor ä¼šè‡ªåŠ¨è¿›è¡Œå¾ªç¯åˆ†å—ï¼ˆTilingï¼‰ï¼Œæé«˜ç¼“å­˜å‘½ä¸­ç‡ã€‚

**æœªä¼˜åŒ–çš„è®¿é—®**ï¼š
```python
# é€è¡Œå¤„ç†ï¼ˆç¼“å­˜ä¸å‹å¥½ï¼‰
for i in range(N):
    for j in range(M):
        out[i][j] = compute(A[i][j])
```

**ä¼˜åŒ–åçš„è®¿é—®**ï¼š
```python
# åˆ†å—å¤„ç†ï¼ˆç¼“å­˜å‹å¥½ï¼‰
for ii in range(0, N, TILE_SIZE):
    for jj in range(0, M, TILE_SIZE):
        for i in range(ii, min(ii+TILE_SIZE, N)):
            for j in range(jj, min(jj+TILE_SIZE, M)):
                out[i][j] = compute(A[i][j])
```

### 5.2 å†…å­˜å¸ƒå±€ä¼˜åŒ–

TorchInductor ä¼šåˆ†æå†…å­˜è®¿é—®æ¨¡å¼ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å¸ƒå±€ï¼š

```python
# åŸå§‹å¸ƒå±€ï¼šNCHW (Batch, Channel, Height, Width)
x = torch.randn(64, 3, 224, 224)

# å·ç§¯æ“ä½œæ›´é€‚åˆ NHWC å¸ƒå±€
# TorchInductor å¯èƒ½ä¼šè‡ªåŠ¨è½¬æ¢å¸ƒå±€ä»¥æå‡æ€§èƒ½
compiled_conv = torch.compile(conv_layer)
```

### 5.3 å¸¸é‡æŠ˜å ä¸ä¼ æ’­

```python
def compute(x):
    a = 2 * 3      # å¸¸é‡è¡¨è¾¾å¼
    b = x + a      # å¯ä»¥ä¼˜åŒ–ä¸º x + 6
    return b * a   # å¯ä»¥ä¼˜åŒ–ä¸º b * 6

# TorchInductor ä¼˜åŒ–åï¼š
def compute_optimized(x):
    return (x + 6) * 6  # å¸¸é‡åœ¨ç¼–è¯‘æ—¶å·²è®¡ç®—
```

---

## 6. TorchInductor çš„è¿è¡Œæœºåˆ¶

### 6.1 ç¼–è¯‘ç¼“å­˜

TorchInductor ä¼šç¼“å­˜ç¼–è¯‘ç»“æœï¼Œé¿å…é‡å¤ç¼–è¯‘ï¼š

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·ä»£ç 
    participant Cache as ç¼–è¯‘ç¼“å­˜
    participant Inductor as TorchInductor
    participant GPU as GPU
    
    User->>Cache: è°ƒç”¨ compiled_fn(x)
    Cache->>Cache: è®¡ç®— Cache Key (å›¾ + Shape)
    alt ç¼“å­˜å‘½ä¸­
        Cache->>GPU: ç›´æ¥æ‰§è¡Œç¼“å­˜çš„ Kernel
    else ç¼“å­˜æœªå‘½ä¸­
        Cache->>Inductor: è§¦å‘ç¼–è¯‘
        Inductor->>Inductor: ç”Ÿæˆ Triton ä»£ç 
        Inductor->>Inductor: ç¼–è¯‘ä¸º CUDA Binary
        Inductor->>Cache: ä¿å­˜åˆ°ç¼“å­˜
        Cache->>GPU: æ‰§è¡Œæ–°ç¼–è¯‘çš„ Kernel
    end
    GPU-->>User: è¿”å›ç»“æœ
```

**ç¼“å­˜ä½ç½®**ï¼š
```bash
~/.triton/cache/  # Triton ç¼–è¯‘ç¼“å­˜
/tmp/torchinductor_<user>/  # TorchInductor ä¸´æ—¶æ–‡ä»¶
```

### 6.2 åŠ¨æ€ Shape å¤„ç†

å¯¹äºåŠ¨æ€å½¢çŠ¶ï¼ŒTorchInductor ä½¿ç”¨ **ç¬¦å·åŒ– Shape** æŠ€æœ¯ï¼Œç”Ÿæˆå¯å¤ç”¨çš„é€šç”¨ä»£ç ã€‚

#### 6.2.1 ç¬¦å·åŒ–ç»´åº¦åŸç†

```python
def dynamic_fn(x):
    # x.shape å¯èƒ½æ˜¯ (10, 20) æˆ– (100, 200)
    return x.sum(dim=1)

compiled_fn = torch.compile(dynamic_fn)

# ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼šshape = (10, 20)
# TorchInductor ç”Ÿæˆæ”¯æŒ "ä»»æ„ M Ã— N" çš„ä»£ç 
x1 = torch.randn(10, 20)
compiled_fn(x1)  # ç¼–è¯‘

# ç¬¬äºŒæ¬¡è°ƒç”¨ï¼šshape = (100, 200)
x2 = torch.randn(100, 200)
compiled_fn(x2)  # å¤ç”¨ç¼–è¯‘ç»“æœï¼ˆæ— éœ€é‡æ–°ç¼–è¯‘ï¼‰
```

**ä¸ºä»€ä¹ˆæ— éœ€é‡æ–°ç¼–è¯‘ï¼Ÿ**

##### 1. ç¬¦å·åŒ–ç»´åº¦æ¨å¯¼

ç¬¬ä¸€æ¬¡ç¼–è¯‘æ—¶ï¼ŒTorchInductor ä¸ä¼šç¡¬ç¼–ç å…·ä½“æ•°å€¼ `(10, 20)`ï¼Œè€Œæ˜¯æ¨å¯¼å‡ºç¬¦å·åŒ–çš„ç»´åº¦ï¼š

```python
# ç¼–è¯‘æ—¶çš„ç¬¦å·æ¨å¯¼
x.shape = (s0, s1)  # s0 å’Œ s1 æ˜¯ç¬¦å·å˜é‡ï¼Œä¸æ˜¯å…·ä½“æ•°å€¼

# ç”Ÿæˆçš„ FX Graphï¼š
def forward(self, x):
    # è¾“å…¥ x: shape = (s0, s1)
    sum_1 = torch.ops.aten.sum.dim_IntList(x, [1], keepdim=False)
    # è¾“å‡º sum_1: shape = (s0,)
    return sum_1
```

##### 2. å‚æ•°åŒ–çš„ Kernel ä»£ç 

ç”Ÿæˆçš„ Triton Kernel å°† shape ä½œä¸ºè¿è¡Œæ—¶å‚æ•°ï¼š

```python
# TorchInductor ç”Ÿæˆçš„ Triton Kernelï¼ˆç®€åŒ–ç‰ˆï¼‰
@triton.jit
def sum_kernel(
    in_ptr,      # è¾“å…¥æŒ‡é’ˆ
    out_ptr,     # è¾“å‡ºæŒ‡é’ˆ
    M,           # ç¬¬ä¸€ä¸ªç»´åº¦ï¼ˆè¿è¡Œæ—¶å‚æ•°ï¼‰
    N,           # ç¬¬äºŒä¸ªç»´åº¦ï¼ˆè¿è¡Œæ—¶å‚æ•°ï¼‰
    BLOCK_SIZE: tl.constexpr
):
    # å…³é”®ï¼šM å’Œ N æ˜¯è¿è¡Œæ—¶ä¼ å…¥çš„å‚æ•°ï¼Œä¸æ˜¯ç¼–è¯‘æ—¶å¸¸é‡
    pid = tl.program_id(0)
    row_idx = pid
    
    if row_idx < M:  # M æ˜¯åŠ¨æ€çš„
        # è®¡ç®—è¯¥è¡Œçš„å’Œ
        sum_val = 0.0
        for col_idx in range(0, N, BLOCK_SIZE):  # N æ˜¯åŠ¨æ€çš„
            mask = col_idx + tl.arange(0, BLOCK_SIZE) < N
            data = tl.load(in_ptr + row_idx * N + col_idx + tl.arange(0, BLOCK_SIZE), mask=mask)
            sum_val += tl.sum(data)
        
        tl.store(out_ptr + row_idx, sum_val)

# ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼šä¼ å…¥ M=10, N=20
sum_kernel[(10,)](in_ptr, out_ptr, M=10, N=20, BLOCK_SIZE=32)

# ç¬¬äºŒæ¬¡è°ƒç”¨ï¼šä¼ å…¥ M=100, N=200ï¼ˆå¤ç”¨åŒä¸€ä¸ªç¼–è¯‘å¥½çš„ Kernelï¼‰
sum_kernel[(100,)](in_ptr, out_ptr, M=100, N=200, BLOCK_SIZE=32)
```

**å…³é”®ç‚¹**ï¼š
- Kernel ä»£ç åœ¨ç¼–è¯‘æ—¶å·²ç»ç”Ÿæˆä¸º CUDA äºŒè¿›åˆ¶æ–‡ä»¶
- è¿è¡Œæ—¶åªéœ€ä¼ å…¥ä¸åŒçš„ `M` å’Œ `N` å‚æ•°
- æ— éœ€é‡æ–°ç¼–è¯‘æ•´ä¸ª Kernel

##### 3. Guard æœºåˆ¶ä¿æŠ¤

è™½ç„¶å…·ä½“æ•°å€¼å¯ä»¥å˜åŒ–ï¼Œä½† TorchDynamo çš„ Guard ç¡®ä¿æŸäº›å±æ€§ä¸å˜ï¼š

```python
# ç¼–è¯‘æ—¶å»ºç«‹çš„ Guards
guards = [
    x.ndim == 2,           # ç»´åº¦æ•°é‡å¿…é¡»æ˜¯ 2
    x.dtype == torch.float32,  # æ•°æ®ç±»å‹å¿…é¡»æ˜¯ float32
    x.device.type == 'cuda',   # è®¾å¤‡ç±»å‹å¿…é¡»æ˜¯ CUDA
    x.requires_grad == False   # æ˜¯å¦éœ€è¦æ¢¯åº¦
]

# ä»¥ä¸‹æƒ…å†µä¼šè§¦å‘é‡æ–°ç¼–è¯‘ï¼š
x3 = torch.randn(10, 20, 30)  # âŒ ndim å˜ä¸º 3ï¼ŒGuard å¤±è´¥
x4 = torch.randn(10, 20, dtype=torch.float64)  # âŒ dtype å˜åŒ–
x5 = torch.randn(10, 20, device='cpu')  # âŒ device å˜åŒ–
```

#### 6.2.2 ç¼–è¯‘ç¼“å­˜ä¸å¤ç”¨ç­–ç•¥

```python
# ç¼“å­˜çš„ Key ç»„æˆ
cache_key = hash((
    fx_graph,           # è®¡ç®—å›¾ç»“æ„
    input_ndim,         # è¾“å…¥ç»´åº¦æ•°é‡ï¼ˆä¸æ˜¯å…·ä½“å¤§å°ï¼‰
    input_dtype,        # è¾“å…¥æ•°æ®ç±»å‹
    input_device,       # è¾“å…¥è®¾å¤‡
    # æ³¨æ„ï¼šä¸åŒ…æ‹¬å…·ä½“çš„ shape æ•°å€¼
))
```

**ç¤ºä¾‹å¯¹æ¯”**ï¼š

```python
import torch

def matmul_fn(a, b):
    return torch.matmul(a, b)

compiled_fn = torch.compile(matmul_fn)

# åœºæ™¯ 1ï¼šshape å˜åŒ–ä½†ç»´åº¦æ•°é‡ä¸å˜ï¼ˆå¯å¤ç”¨ï¼‰
a1 = torch.randn(10, 20)
b1 = torch.randn(20, 30)
compiled_fn(a1, b1)  # ç¬¬ä¸€æ¬¡ç¼–è¯‘

a2 = torch.randn(100, 200)
b2 = torch.randn(200, 300)
compiled_fn(a2, b2)  # âœ… å¤ç”¨ï¼ˆshape æ•°å€¼å˜åŒ–ï¼Œä½†éƒ½æ˜¯ 2Dï¼‰

# åœºæ™¯ 2ï¼šç»´åº¦æ•°é‡å˜åŒ–ï¼ˆéœ€è¦é‡æ–°ç¼–è¯‘ï¼‰
a3 = torch.randn(5, 10, 20)
b3 = torch.randn(20, 30)
compiled_fn(a3, b3)  # âŒ é‡æ–°ç¼–è¯‘ï¼ˆa3 ä» 2D å˜ä¸º 3Dï¼‰
```

#### 6.2.3 é™æ€ Shape ä¼˜åŒ–

æŸäº›æƒ…å†µä¸‹ï¼Œå›ºå®š shape å¯ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ï¼š

```python
import torch

def static_shape_fn(x):
    return x.sum(dim=1)

# æ–¹æ³• 1ï¼šåŠ¨æ€ Shapeï¼ˆé»˜è®¤ï¼‰
dynamic_compiled = torch.compile(static_shape_fn)

# æ–¹æ³• 2ï¼šå›ºå®š Shapeï¼ˆé€šè¿‡ dynamic=Falseï¼‰
static_compiled = torch.compile(static_shape_fn, dynamic=False)

x1 = torch.randn(10, 20)
dynamic_compiled(x1)  # ç”Ÿæˆæ”¯æŒä»»æ„ shape çš„ä»£ç 
static_compiled(x1)   # ç”Ÿæˆé’ˆå¯¹ (10, 20) ä¼˜åŒ–çš„ä»£ç 

x2 = torch.randn(100, 200)
dynamic_compiled(x2)  # âœ… å¤ç”¨
static_compiled(x2)   # âŒ é‡æ–°ç¼–è¯‘ï¼ˆshape ä¸ (10, 20) ä¸åŒ¹é…ï¼‰
```

**æ€§èƒ½å¯¹æ¯”**ï¼š
- åŠ¨æ€ Shapeï¼šçµæ´»ä½†å¯èƒ½ç•¥æ…¢ï¼ˆéœ€è¦è¿è¡Œæ—¶è®¡ç®—ç´¢å¼•ï¼‰
- é™æ€ Shapeï¼šæ€§èƒ½æ›´ä¼˜ï¼ˆå¯ä»¥åšæ›´æ¿€è¿›çš„ç¼–è¯‘æ—¶ä¼˜åŒ–ï¼‰

#### 6.2.4 ç¬¦å·åŒ– Shape çš„å®ç°

åœ¨ PyTorch å†…éƒ¨ï¼Œä½¿ç”¨ `SymInt` ç±»å‹è¡¨ç¤ºç¬¦å·åŒ–æ•´æ•°ï¼š

```python
# TorchInductor å†…éƒ¨è¡¨ç¤º
from torch.fx.experimental.symbolic_shapes import ShapeEnv

env = ShapeEnv()
s0 = env.create_symbol(10, source="input_x_dim0")   # ç¬¦å·å˜é‡ s0ï¼Œåˆå§‹å€¼ 10
s1 = env.create_symbol(20, source="input_x_dim1")   # ç¬¦å·å˜é‡ s1ï¼Œåˆå§‹å€¼ 20

# ä½¿ç”¨ç¬¦å·å˜é‡è¿›è¡Œæ¨å¯¼
output_shape = (s0,)  # sum(dim=1) åçš„ shape

# ç¬¬äºŒæ¬¡è°ƒç”¨æ—¶ï¼Œåªéœ€ç»‘å®šæ–°çš„å€¼
env.bind(s0, 100)
env.bind(s1, 200)
# æ— éœ€é‡æ–°ç”Ÿæˆä»£ç 
```

---

## 7. è°ƒè¯•ä¸è°ƒä¼˜

### 7.1 æŸ¥çœ‹ç¼–è¯‘æ—¥å¿—

```python
import torch
import logging

# å¯ç”¨è¯¦ç»†æ—¥å¿—
torch._logging.set_logs(dynamo=logging.INFO, inductor=logging.DEBUG)

compiled_fn = torch.compile(your_function)
compiled_fn(input_data)
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
[INFO] TorchInductor: Compiling function 'your_function'
[DEBUG] Fusion: Merged 3 pointwise ops into 1 kernel
[DEBUG] Code generation: Generated 1 Triton kernel
[INFO] Compilation time: 1.23s
```

### 7.2 æ€§èƒ½åˆ†æ

```python
import torch
from torch.profiler import profile, ProfilerActivity

model = torch.compile(model)

with profile(activities=[ProfilerActivity.CUDA]) as prof:
    output = model(input_data)

print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
```

### 7.3 å¼ºåˆ¶é‡æ–°ç¼–è¯‘

```python
# æ¸…ç©ºç¼–è¯‘ç¼“å­˜
torch._dynamo.reset()

# é‡æ–°ç¼–è¯‘
compiled_fn = torch.compile(your_function)
```

---

## 8. é™åˆ¶ä¸æœ€ä½³å®è·µ

### 8.1 å½“å‰é™åˆ¶

1.  **é¦–æ¬¡ç¼–è¯‘å¼€é”€**ï¼šå¯èƒ½éœ€è¦ 1-5 ç§’ï¼ˆå–å†³äºæ¨¡å‹å¤æ‚åº¦ï¼‰ã€‚
2.  **åŠ¨æ€æ§åˆ¶æµ**ï¼šè¿‡äºå¤æ‚çš„æ•°æ®ä¾èµ–æ§åˆ¶æµå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
3.  **ç¨€ç–æ“ä½œ**ï¼šç¨€ç– Tensor çš„æ”¯æŒæœ‰é™ã€‚

### 8.2 æœ€ä½³å®è·µ

**âœ… æ¨è**ï¼š
```python
# 1. å›ºå®š Batch Size
dataloader = DataLoader(dataset, batch_size=32, drop_last=True)

# 2. é¢„çƒ­ç¼–è¯‘
compiled_model = torch.compile(model)
with torch.no_grad():
    _ = compiled_model(dummy_input)  # é¢„çƒ­

# 3. ä½¿ç”¨ reduce-overhead æ¨¡å¼ï¼ˆé€‚åˆå°æ¨¡å‹ï¼‰
model = torch.compile(model, mode="reduce-overhead")
```

**âŒ é¿å…**ï¼š
```python
# ä¸è¦åœ¨è®­ç»ƒå¾ªç¯ä¸­é¢‘ç¹æ”¹å˜è¾“å…¥å½¢çŠ¶
for batch in dataloader:
    x = batch[torch.randperm(len(batch))]  # åŠ¨æ€å½¢çŠ¶ï¼Œè§¦å‘é‡ç¼–è¯‘
    compiled_model(x)
```

---

## 9. æºç å¯¼è¯»

*   **`torch/_inductor/`**ï¼šTorchInductor çš„æ ¸å¿ƒå®ç°
    *   `codegen/triton.py`ï¼šTriton ä»£ç ç”Ÿæˆå™¨
    *   `fx_passes/`ï¼šå›¾ä¼˜åŒ– Passï¼ˆèåˆã€åˆ†è§£ç­‰ï¼‰
    *   `graph.py`ï¼šå›¾åˆ†æä¸è°ƒåº¦
    *   `scheduler.py`ï¼šæ“ä½œè°ƒåº¦ä¸å†…å­˜è§„åˆ’

*   **Triton ä»“åº“**ï¼š[https://github.com/openai/triton](https://github.com/openai/triton)

---

## 10. æ€»ç»“

TorchInductor æ˜¯ PyTorch 2.0 ç¼–è¯‘æ ˆçš„"ç»ˆææ­¦å™¨"ï¼Œå®ƒå°†é«˜å±‚æ¬¡çš„è®¡ç®—å›¾è½¬æ¢ä¸ºé«˜æ•ˆçš„æœºå™¨ç ã€‚

**æ ¸å¿ƒè¦ç‚¹**ï¼š
*   **ä»£ç ç”Ÿæˆ**ï¼šä½¿ç”¨ Triton è‡ªåŠ¨ç”Ÿæˆä¼˜åŒ–çš„ GPU Kernelã€‚
*   **ç®—å­èåˆ**ï¼šå‡å°‘å†…å­˜è®¿é—®å’Œ Kernel å¯åŠ¨æ¬¡æ•°ã€‚
*   **è‡ªåŠ¨ä¼˜åŒ–**ï¼šå¾ªç¯åˆ†å—ã€å†…å­˜å¸ƒå±€ã€å¸¸é‡æŠ˜å ç­‰ã€‚
*   **æ€§èƒ½æå‡**ï¼šå…¸å‹åœºæ™¯ä¸‹ **1.5-3x** åŠ é€Ÿã€‚

**æŠ€æœ¯æ ˆæ€»è§ˆ**ï¼š
```
ç”¨æˆ· Python ä»£ç 
    â†“ (Dynamo æ•è·)
FX Graph (å‰å‘å›¾)
    â†“ (AOTAutograd ç”Ÿæˆåå‘å›¾)
Optimized FX Graph (å‰å‘ + åå‘)
    â†“ (TorchInductor ä»£ç ç”Ÿæˆ)
Triton / C++ ä»£ç 
    â†“ (ç¼–è¯‘)
CUDA / CPU æœºå™¨ç 
    â†“ (æ‰§è¡Œ)
é«˜æ•ˆè®¡ç®—ç»“æœ ğŸš€
```

**ä¸‹ä¸€ç« é¢„å‘Š**ï¼š
æˆ‘ä»¬å·²ç»äº†è§£äº† PyTorch 2.0 çš„å®Œæ•´ç¼–è¯‘æ ˆã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬æ·±å…¥ PyTorch çš„åº•å±‚åŸºç¡€è®¾æ–½â€”â€”**Dispatcherï¼ˆè°ƒåº¦å™¨ï¼‰**ï¼Œç†è§£ç®—å­å¦‚ä½•è·¯ç”±åˆ°ä¸åŒçš„ç¡¬ä»¶åç«¯ã€‚è¯·çœ‹ [ç¬¬å…«ç« ï¼šPyTorch Dispatcher è°ƒåº¦æœºåˆ¶](./08_PyTorchè°ƒåº¦æœºåˆ¶.md)ã€‚

