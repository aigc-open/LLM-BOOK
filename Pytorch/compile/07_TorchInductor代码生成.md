# ç¬¬ä¸ƒç« ï¼šTorchInductor ä»£ç ç”Ÿæˆ â€”â€” ä»è®¡ç®—å›¾åˆ°æœºå™¨ç 

## æœ¬ç« ç›®æ ‡

- **æ ¸å¿ƒåŸç†**ï¼šç†è§£ TorchInductor å¦‚ä½•å°† FX Graph ç¼–è¯‘ä¸ºé«˜æ•ˆçš„æœºå™¨ç ã€‚
- **ä»£ç ç”Ÿæˆ**ï¼šæŒæ¡ Triton è¯­è¨€å’Œ GPU Kernel ç”Ÿæˆæœºåˆ¶ã€‚
- **ä¼˜åŒ–æŠ€æœ¯**ï¼šå­¦ä¹ ç®—å­èåˆã€å†…å­˜è§„åˆ’ã€å¾ªç¯ä¼˜åŒ–ç­‰ç¼–è¯‘å™¨æŠ€æœ¯ã€‚
- **å®æˆ˜æ€§èƒ½**ï¼šäº†è§£å¦‚ä½•è°ƒä¼˜å’Œè°ƒè¯•ç”Ÿæˆçš„ä»£ç ã€‚

---

## 1. ä¸ºä»€ä¹ˆéœ€è¦ TorchInductorï¼Ÿ

### 1.1 ç¼–è¯‘æ ˆçš„æœ€åä¸€ç¯

åœ¨ `torch.compile` çš„ä¸‰å±‚æ¶æ„ä¸­ï¼š
1.  **Dynamo**ï¼šæ•è·å‰å‘å›¾ï¼ˆPython å­—èŠ‚ç  â†’ FX Graphï¼‰
2.  **AOTAutograd**ï¼šç”Ÿæˆåå‘å›¾å¹¶è”åˆä¼˜åŒ–
3.  **TorchInductor**ï¼š**å°† FX Graph ç¼–è¯‘ä¸º GPU/CPU çš„æœºå™¨ç **

**å½¢è±¡æ¯”å–»**ï¼š
*   **Dynamo** æ˜¯å»ºç­‘è®¾è®¡å¸ˆï¼Œç”»å‡ºæˆ¿å­çš„è“å›¾ï¼ˆè®¡ç®—å›¾ï¼‰ã€‚
*   **AOTAutograd** æ˜¯ç»“æ„å·¥ç¨‹å¸ˆï¼Œç¡®ä¿æˆ¿å­ç»“æ„åˆç†ï¼ˆå‰å‘+åå‘ä¼˜åŒ–ï¼‰ã€‚
*   **TorchInductor** æ˜¯æ–½å·¥é˜Ÿï¼ŒæŠŠè“å›¾å˜æˆçœŸæ­£çš„æˆ¿å­ï¼ˆé«˜æ•ˆæœºå™¨ç ï¼‰ã€‚

### 1.2 ä¼ ç»Ÿæ–¹å¼çš„å±€é™

**PyTorch Eager æ¨¡å¼**ï¼š
```python
# æ¯ä¸ªæ“ä½œéƒ½æ˜¯ç‹¬ç«‹çš„ Python è°ƒç”¨
y = x + 1      # è°ƒç”¨ aten::add
z = y * 2      # è°ƒç”¨ aten::mul
out = z.sin()  # è°ƒç”¨ aten::sin

# é—®é¢˜ï¼š
# 1. æ¯ä¸ªæ“ä½œå¯åŠ¨ä¸€ä¸ªç‹¬ç«‹çš„ GPU Kernelï¼ˆå¯åŠ¨å¼€é”€å¤§ï¼‰
# 2. ä¸­é—´ç»“æœï¼ˆy, zï¼‰éƒ½éœ€è¦å†™å›æ˜¾å­˜ï¼ˆå†…å­˜å¸¦å®½æµªè´¹ï¼‰
# 3. æ— æ³•è¿›è¡Œè·¨æ“ä½œçš„ä¼˜åŒ–
```

**TorchInductor çš„ä¼˜åŒ–**ï¼š
```python
compiled_fn = torch.compile(lambda x: ((x + 1) * 2).sin())

# TorchInductor ç”Ÿæˆèåˆåçš„ Kernelï¼š
# out = sin((x + 1) * 2)  # å•ä¸ª Kernelï¼Œä¸€æ¬¡å†…å­˜è¯»å†™ï¼
```

**æ€§èƒ½æå‡**ï¼š
*   **å‡å°‘ Kernel å¯åŠ¨æ¬¡æ•°**ï¼š3 ä¸ª Kernel â†’ 1 ä¸ª
*   **å‡å°‘å†…å­˜è®¿é—®**ï¼š3 æ¬¡è¯»å†™ â†’ 1 æ¬¡è¯»å†™
*   **ç†è®ºåŠ é€Ÿæ¯”**ï¼šçº¦ **3-5x**ï¼ˆå®é™…å–å†³äºç®—å­ç±»å‹ï¼‰

---

## 2. æ ¸å¿ƒåŸç†ï¼šä»£ç ç”Ÿæˆæµç¨‹

### 2.1 æ•´ä½“æ¶æ„

```mermaid
graph TD
    A["FX Graph (æ¥è‡ª AOTAutograd)"] --> B["å›¾åˆ†æä¸ä¼˜åŒ–"]
    B --> C["ç®—å­èåˆ (Fusion)"]
    C --> D["å†…å­˜è§„åˆ’ (Memory Planning)"]
    D --> E{ç›®æ ‡è®¾å¤‡?}
    E -- GPU --> F["Triton ä»£ç ç”Ÿæˆ"]
    E -- CPU --> G["C++/OpenMP ä»£ç ç”Ÿæˆ"]
    F --> H["ç¼–è¯‘ä¸º CUDA Kernel"]
    G --> I["ç¼–è¯‘ä¸ºäºŒè¿›åˆ¶ä»£ç "]
    H --> J["ç¼“å­˜ç¼–è¯‘ç»“æœ"]
    I --> J
    J --> K["æ‰§è¡Œä¼˜åŒ–åçš„ä»£ç "]
```

### 2.2 å…³é”®æ­¥éª¤è¯¦è§£

#### æ­¥éª¤ 1ï¼šå›¾åˆ†æ

TorchInductor é¦–å…ˆåˆ†æ FX Graphï¼Œè¯†åˆ«ï¼š
*   **å“ªäº›æ“ä½œå¯ä»¥èåˆ**ï¼Ÿï¼ˆå¦‚é€å…ƒç´ æ“ä½œï¼‰
*   **æ•°æ®ä¾èµ–å…³ç³»**ï¼Ÿï¼ˆç¡®ä¿æ­£ç¡®æ€§ï¼‰
*   **å†…å­˜è®¿é—®æ¨¡å¼**ï¼Ÿï¼ˆä¼˜åŒ–å†…å­˜å¸ƒå±€ï¼‰

#### æ­¥éª¤ 2ï¼šç®—å­èåˆ

**æœªèåˆçš„è®¡ç®—**ï¼š
```python
# å‰å‘å›¾
x1 = x + 1       # Kernel 1: read x, write x1
x2 = x1 * 2      # Kernel 2: read x1, write x2
x3 = torch.sin(x2)  # Kernel 3: read x2, write x3
```

**èåˆåçš„è®¡ç®—**ï¼š
```python
# èåˆ Kernelï¼šread x, write x3
x3 = torch.sin((x + 1) * 2)
```

**å†…å­˜å¸¦å®½èŠ‚çœè®¡ç®—**ï¼š
```
æœªèåˆï¼š3 æ¬¡è¯» + 3 æ¬¡å†™ = 6 æ¬¡å†…å­˜è®¿é—®
èåˆåï¼š1 æ¬¡è¯» + 1 æ¬¡å†™ = 2 æ¬¡å†…å­˜è®¿é—®
èŠ‚çœï¼š(6 - 2) / 6 = 66.7% çš„å†…å­˜å¸¦å®½
```

#### æ­¥éª¤ 3ï¼šä»£ç ç”Ÿæˆ

TorchInductor ä½¿ç”¨ **Triton**ï¼ˆä¸€ç§ Python DSLï¼‰ç”Ÿæˆ GPU ä»£ç ã€‚

**Triton ä»£ç ç¤ºä¾‹**ï¼š
```python
import triton
import triton.language as tl

@triton.jit
def fused_kernel(x_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):
    # è·å–å½“å‰çº¿ç¨‹å—çš„èµ·å§‹ä½ç½®
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    
    # è¾¹ç•Œæ£€æŸ¥
    mask = offsets < n_elements
    
    # ä»å…¨å±€å†…å­˜åŠ è½½æ•°æ®
    x = tl.load(x_ptr + offsets, mask=mask)
    
    # èåˆè®¡ç®—ï¼š(x + 1) * 2 ç„¶å sin
    temp = (x + 1.0) * 2.0
    out = tl.sin(temp)
    
    # å†™å›å…¨å±€å†…å­˜
    tl.store(out_ptr + offsets, out, mask=mask)
```

**å…³é”®ç‰¹æ€§**ï¼š
*   **è‡ªåŠ¨å¹¶è¡Œ**ï¼šTriton ç¼–è¯‘å™¨è‡ªåŠ¨ç”Ÿæˆ CUDA çº¿ç¨‹ç½‘æ ¼ã€‚
*   **å†…å­˜åˆå¹¶**ï¼šä¼˜åŒ–å…¨å±€å†…å­˜è®¿é—®æ¨¡å¼ã€‚
*   **å¯„å­˜å™¨ä¼˜åŒ–**ï¼šä¸­é—´å˜é‡ï¼ˆ`temp`ï¼‰ä¿å­˜åœ¨å¯„å­˜å™¨ä¸­ï¼Œä¸å†™å›æ˜¾å­˜ã€‚

---

## 3. æ·±å…¥åº•å±‚ï¼šTriton vs æ‰‹å†™ CUDA

### 3.1 ä¸ºä»€ä¹ˆé€‰æ‹© Tritonï¼Ÿ

**æ‰‹å†™ CUDA çš„ç—›ç‚¹**ï¼š
1.  **å¤æ‚æ€§é«˜**ï¼šéœ€è¦ç®¡ç†çº¿ç¨‹å—ã€å…±äº«å†…å­˜ã€å¯„å­˜å™¨åˆ†é…ã€‚
2.  **å¯ç§»æ¤æ€§å·®**ï¼šä¸åŒ GPU æ¶æ„ï¼ˆå¦‚ A100 vs V100ï¼‰éœ€è¦æ‰‹åŠ¨è°ƒä¼˜ã€‚
3.  **å¼€å‘æ•ˆç‡ä½**ï¼šä¸€ä¸ªç®€å•çš„èåˆ Kernel å¯èƒ½éœ€è¦æ•°ç™¾è¡Œä»£ç ã€‚

**Triton çš„ä¼˜åŠ¿**ï¼š
1.  **Python è¯­æ³•**ï¼šæ˜“äºå­¦ä¹ å’Œç¼–å†™ã€‚
2.  **è‡ªåŠ¨ä¼˜åŒ–**ï¼šç¼–è¯‘å™¨è‡ªåŠ¨å¤„ç†å†…å­˜è®¿é—®ã€çº¿ç¨‹è°ƒåº¦ç­‰ã€‚
3.  **æ€§èƒ½æ¥è¿‘æ‰‹å†™ CUDA**ï¼šé€šå¸¸èƒ½è¾¾åˆ°æ‰‹å†™ CUDA çš„ 90-95% æ€§èƒ½ã€‚

### 3.2 Triton ç¼–è¯‘æµç¨‹

```mermaid
graph LR
    A["Triton Python ä»£ç "] --> B["Triton IR"]
    B --> C["LLVM IR"]
    C --> D["PTX (GPU æ±‡ç¼–)"]
    D --> E["CUDA Binary"]
    E --> F["GPU æ‰§è¡Œ"]
    
    style D fill:#f96
```

**ç¤ºä¾‹å¯¹æ¯”**ï¼š

<details>
<summary>æ‰‹å†™ CUDAï¼ˆçº¦ 50 è¡Œä»£ç ï¼‰</summary>

```cuda
__global__ void fused_kernel(float* x, float* out, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        float temp = (x[idx] + 1.0f) * 2.0f;
        out[idx] = sinf(temp);
    }
}

// å¯åŠ¨é…ç½®
int threads = 256;
int blocks = (n + threads - 1) / threads;
fused_kernel<<<blocks, threads>>>(x, out, n);
```
</details>

<details>
<summary>Triton ä»£ç ï¼ˆçº¦ 10 è¡Œæ ¸å¿ƒé€»è¾‘ï¼‰</summary>

```python
@triton.jit
def fused_kernel(x_ptr, out_ptr, n, BLOCK_SIZE: tl.constexpr):
    pid = tl.program_id(0)
    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n
    x = tl.load(x_ptr + offsets, mask=mask)
    out = tl.sin((x + 1.0) * 2.0)
    tl.store(out_ptr + offsets, out, mask=mask)
```
</details>

**ä»£ç é‡å¯¹æ¯”**ï¼šTriton å‡å°‘ **80% çš„ä»£ç **ï¼ŒåŒæ—¶æ€§èƒ½ç›¸å½“ï¼

---

## 4. å®æˆ˜ç¤ºä¾‹ï¼šæŸ¥çœ‹ç”Ÿæˆçš„ä»£ç 

### 4.1 åŸºæœ¬ç¤ºä¾‹

```python
import torch

def simple_fn(x):
    return ((x + 1) * 2).sin()

# ç¼–è¯‘å¹¶æŸ¥çœ‹ç”Ÿæˆçš„ä»£ç 
compiled_fn = torch.compile(simple_fn)

# å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œä¿å­˜ç”Ÿæˆçš„ä»£ç 
import torch._inductor.config as config
config.debug = True
config.trace.enabled = True

x = torch.randn(1024, device='cuda')
output = compiled_fn(x)

# ç”Ÿæˆçš„ä»£ç ä¼šä¿å­˜åœ¨ï¼š
# /tmp/torchinductor_<username>/
print("ç”Ÿæˆçš„ä»£ç è·¯å¾„:", config.trace.debug_dir)
```

### 4.2 æŸ¥çœ‹ Triton æºä»£ç 

**æ–¹æ³• 1ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡æ‰“å°åˆ°æ§åˆ¶å°ï¼ˆæ¨èï¼‰**

```python
import torch
import os

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œæ‰“å°ç”Ÿæˆçš„ä»£ç 
os.environ['TORCH_LOGS'] = '+output_code'

def simple_fn(x):
    return ((x + 1) * 2).sin()

compiled_fn = torch.compile(simple_fn)
x = torch.randn(1024, device='cuda')
output = compiled_fn(x)
```

**è¾“å‡ºï¼ˆä¼šç›´æ¥æ‰“å°åˆ°æ§åˆ¶å°ï¼‰ï¼š**

```python
# è‡ªåŠ¨ç”Ÿæˆçš„ Triton Kernel
@triton.jit
def triton_poi_fused_add_mul_sin_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = 1.0
    tmp2 = tmp0 + tmp1  # x + 1
    tmp3 = 2.0
    tmp4 = tmp2 * tmp3  # * 2
    tmp5 = tl.sin(tmp4)  # sin
    tl.store(out_ptr0 + (x0), tmp5, xmask)
```

**æ–¹æ³• 2ï¼šä¿å­˜åˆ°æ–‡ä»¶**

```python
import torch
import torch._inductor.config as config

# é…ç½®ä¿å­˜ç”Ÿæˆçš„ä»£ç 
config.debug = True
config.trace.enabled = True
config.trace.debug_dir = "/tmp/inductor_debug"  # è‡ªå®šä¹‰è·¯å¾„

def simple_fn(x):
    return ((x + 1) * 2).sin()

compiled_fn = torch.compile(simple_fn)
x = torch.randn(1024, device='cuda')
output = compiled_fn(x)

# æŸ¥çœ‹ç”Ÿæˆçš„ Python ä»£ç æ–‡ä»¶
# åœ¨ /tmp/inductor_debug/ ç›®å½•ä¸‹æ‰¾ output_code.py
print(f"ä»£ç ä¿å­˜åœ¨: {config.trace.debug_dir}")
```

**æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶ï¼š**

```bash
# åœ¨ç”Ÿæˆçš„ç›®å½•ä¸­æŸ¥æ‰¾ output_code.py
ls /tmp/inductor_debug/
cat /tmp/inductor_debug/*/output_code.py
```

**æ‚¨ä¹‹å‰æ‰¾åˆ°çš„æ–‡ä»¶è¯´æ˜**ï¼š
```bash
/tmp/torchinductor_root/triton/0/LBHSQGOAZD44.../
â”œâ”€â”€ triton_poi_fused_add_mul_sin_0.cubin   # CUDA äºŒè¿›åˆ¶ï¼ˆGPU ç›´æ¥æ‰§è¡Œçš„æœºå™¨ç ï¼‰
â”œâ”€â”€ triton_poi_fused_add_mul_sin_0.ptx     # PTX æ±‡ç¼–ä»£ç ï¼ˆGPU æ±‡ç¼–è¯­è¨€ï¼‰
â”œâ”€â”€ triton_poi_fused_add_mul_sin_0.ttir    # Triton IRï¼ˆTriton ä¸­é—´è¡¨ç¤ºï¼‰
â”œâ”€â”€ triton_poi_fused_add_mul_sin_0.ttgir   # Triton GPU IR
â”œâ”€â”€ triton_poi_fused_add_mul_sin_0.llir    # LLVM IR
â””â”€â”€ *.json                                 # å…ƒæ•°æ®
```

è¿™äº›éƒ½æ˜¯**ç¼–è¯‘äº§ç‰©**ï¼Œä¸æ˜¯æºä»£ç ã€‚è¦æŸ¥çœ‹ Triton Python æºä»£ç ï¼Œå¿…é¡»ä½¿ç”¨ä¸Šè¿°æ–¹æ³• 1 æˆ– 2ã€‚

**å…³é”®è§‚å¯Ÿï¼ˆä»æºä»£ç ä¸­ï¼‰**ï¼š
*   æ‰€æœ‰ä¸­é—´å€¼ï¼ˆ`tmp2`, `tmp4`ï¼‰éƒ½ä¿å­˜åœ¨å¯„å­˜å™¨ä¸­ã€‚
*   åªæœ‰ä¸€æ¬¡ `tl.load`ï¼ˆè¯»å– `x`ï¼‰å’Œä¸€æ¬¡ `tl.store`ï¼ˆå†™å…¥ç»“æœï¼‰ã€‚
*   å®Œç¾çš„ç®—å­èåˆï¼

### 4.3 æ€§èƒ½å¯¹æ¯”

```python
import torch
import time

x = torch.randn(10000000, device='cuda')

# Eager æ¨¡å¼
torch.cuda.synchronize()
start = time.time()
for _ in range(100):
    y = ((x + 1) * 2).sin()
torch.cuda.synchronize()
eager_time = time.time() - start

# Compiled æ¨¡å¼
compiled_fn = torch.compile(lambda x: ((x + 1) * 2).sin())
torch.cuda.synchronize()
start = time.time()
for _ in range(100):
    y = compiled_fn(x)
torch.cuda.synchronize()
compiled_time = time.time() - start

print(f"Eager æ¨¡å¼: {eager_time*1000:.2f} ms")
print(f"Compiled æ¨¡å¼: {compiled_time*1000:.2f} ms")
print(f"åŠ é€Ÿæ¯”: {eager_time/compiled_time:.2f}x")
```

**å…¸å‹ç»“æœ**ï¼š
```
Eager æ¨¡å¼: 45.23 ms
Compiled æ¨¡å¼: 15.67 ms
åŠ é€Ÿæ¯”: 2.89x
```

---

## 5. é«˜çº§ä¼˜åŒ–æŠ€æœ¯

### 5.1 å¾ªç¯ä¼˜åŒ–ï¼ˆLoop Tilingï¼‰

å¯¹äºå¤§å‹ Tensorï¼ŒTorchInductor ä¼šè‡ªåŠ¨è¿›è¡Œå¾ªç¯åˆ†å—ï¼ˆTilingï¼‰ï¼Œæé«˜ç¼“å­˜å‘½ä¸­ç‡ã€‚

**æœªä¼˜åŒ–çš„è®¿é—®**ï¼š
```python
# é€è¡Œå¤„ç†ï¼ˆç¼“å­˜ä¸å‹å¥½ï¼‰
for i in range(N):
    for j in range(M):
        out[i][j] = compute(A[i][j])
```

**ä¼˜åŒ–åçš„è®¿é—®**ï¼š
```python
# åˆ†å—å¤„ç†ï¼ˆç¼“å­˜å‹å¥½ï¼‰
for ii in range(0, N, TILE_SIZE):
    for jj in range(0, M, TILE_SIZE):
        for i in range(ii, min(ii+TILE_SIZE, N)):
            for j in range(jj, min(jj+TILE_SIZE, M)):
                out[i][j] = compute(A[i][j])
```

### 5.2 å†…å­˜å¸ƒå±€ä¼˜åŒ–

TorchInductor ä¼šåˆ†æå†…å­˜è®¿é—®æ¨¡å¼ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å¸ƒå±€ï¼š

```python
# åŸå§‹å¸ƒå±€ï¼šNCHW (Batch, Channel, Height, Width)
x = torch.randn(64, 3, 224, 224)

# å·ç§¯æ“ä½œæ›´é€‚åˆ NHWC å¸ƒå±€
# TorchInductor å¯èƒ½ä¼šè‡ªåŠ¨è½¬æ¢å¸ƒå±€ä»¥æå‡æ€§èƒ½
compiled_conv = torch.compile(conv_layer)
```

### 5.3 å¸¸é‡æŠ˜å ä¸ä¼ æ’­

```python
def compute(x):
    a = 2 * 3      # å¸¸é‡è¡¨è¾¾å¼
    b = x + a      # å¯ä»¥ä¼˜åŒ–ä¸º x + 6
    return b * a   # å¯ä»¥ä¼˜åŒ–ä¸º b * 6

# TorchInductor ä¼˜åŒ–åï¼š
def compute_optimized(x):
    return (x + 6) * 6  # å¸¸é‡åœ¨ç¼–è¯‘æ—¶å·²è®¡ç®—
```

---

## 6. TorchInductor çš„è¿è¡Œæœºåˆ¶

### 6.1 ç¼–è¯‘ç¼“å­˜

TorchInductor ä¼šç¼“å­˜ç¼–è¯‘ç»“æœï¼Œé¿å…é‡å¤ç¼–è¯‘ï¼š

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·ä»£ç 
    participant Cache as ç¼–è¯‘ç¼“å­˜
    participant Inductor as TorchInductor
    participant GPU as GPU
    
    User->>Cache: è°ƒç”¨ compiled_fn(x)
    Cache->>Cache: è®¡ç®— Cache Key (å›¾ + Shape)
    alt ç¼“å­˜å‘½ä¸­
        Cache->>GPU: ç›´æ¥æ‰§è¡Œç¼“å­˜çš„ Kernel
    else ç¼“å­˜æœªå‘½ä¸­
        Cache->>Inductor: è§¦å‘ç¼–è¯‘
        Inductor->>Inductor: ç”Ÿæˆ Triton ä»£ç 
        Inductor->>Inductor: ç¼–è¯‘ä¸º CUDA Binary
        Inductor->>Cache: ä¿å­˜åˆ°ç¼“å­˜
        Cache->>GPU: æ‰§è¡Œæ–°ç¼–è¯‘çš„ Kernel
    end
    GPU-->>User: è¿”å›ç»“æœ
```

**ç¼“å­˜ä½ç½®**ï¼š
```bash
~/.triton/cache/  # Triton ç¼–è¯‘ç¼“å­˜
/tmp/torchinductor_<user>/  # TorchInductor ä¸´æ—¶æ–‡ä»¶
```

### 6.2 åŠ¨æ€ Shape å¤„ç†

å¯¹äºåŠ¨æ€å½¢çŠ¶ï¼ŒTorchInductor ä½¿ç”¨ **ç¬¦å·åŒ– Shape**ï¼š

```python
def dynamic_fn(x):
    # x.shape å¯èƒ½æ˜¯ (10, 20) æˆ– (100, 200)
    return x.sum(dim=1)

compiled_fn = torch.compile(dynamic_fn)

# ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼šshape = (10, 20)
# TorchInductor ç”Ÿæˆæ”¯æŒ "ä»»æ„ M Ã— N" çš„ä»£ç 
x1 = torch.randn(10, 20)
compiled_fn(x1)  # ç¼–è¯‘

# ç¬¬äºŒæ¬¡è°ƒç”¨ï¼šshape = (100, 200)
x2 = torch.randn(100, 200)
compiled_fn(x2)  # å¤ç”¨ç¼–è¯‘ç»“æœï¼ˆæ— éœ€é‡æ–°ç¼–è¯‘ï¼‰
```

---

## 7. è°ƒè¯•ä¸è°ƒä¼˜

### 7.1 æŸ¥çœ‹ç¼–è¯‘æ—¥å¿—

```python
import torch
import logging

# å¯ç”¨è¯¦ç»†æ—¥å¿—
torch._logging.set_logs(dynamo=logging.INFO, inductor=logging.DEBUG)

compiled_fn = torch.compile(your_function)
compiled_fn(input_data)
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
[INFO] TorchInductor: Compiling function 'your_function'
[DEBUG] Fusion: Merged 3 pointwise ops into 1 kernel
[DEBUG] Code generation: Generated 1 Triton kernel
[INFO] Compilation time: 1.23s
```

### 7.2 æ€§èƒ½åˆ†æ

```python
import torch
from torch.profiler import profile, ProfilerActivity

model = torch.compile(model)

with profile(activities=[ProfilerActivity.CUDA]) as prof:
    output = model(input_data)

print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
```

### 7.3 å¼ºåˆ¶é‡æ–°ç¼–è¯‘

```python
# æ¸…ç©ºç¼–è¯‘ç¼“å­˜
torch._dynamo.reset()

# é‡æ–°ç¼–è¯‘
compiled_fn = torch.compile(your_function)
```

---

## 8. é™åˆ¶ä¸æœ€ä½³å®è·µ

### 8.1 å½“å‰é™åˆ¶

1.  **é¦–æ¬¡ç¼–è¯‘å¼€é”€**ï¼šå¯èƒ½éœ€è¦ 1-5 ç§’ï¼ˆå–å†³äºæ¨¡å‹å¤æ‚åº¦ï¼‰ã€‚
2.  **åŠ¨æ€æ§åˆ¶æµ**ï¼šè¿‡äºå¤æ‚çš„æ•°æ®ä¾èµ–æ§åˆ¶æµå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
3.  **ç¨€ç–æ“ä½œ**ï¼šç¨€ç– Tensor çš„æ”¯æŒæœ‰é™ã€‚

### 8.2 æœ€ä½³å®è·µ

**âœ… æ¨è**ï¼š
```python
# 1. å›ºå®š Batch Size
dataloader = DataLoader(dataset, batch_size=32, drop_last=True)

# 2. é¢„çƒ­ç¼–è¯‘
compiled_model = torch.compile(model)
with torch.no_grad():
    _ = compiled_model(dummy_input)  # é¢„çƒ­

# 3. ä½¿ç”¨ reduce-overhead æ¨¡å¼ï¼ˆé€‚åˆå°æ¨¡å‹ï¼‰
model = torch.compile(model, mode="reduce-overhead")
```

**âŒ é¿å…**ï¼š
```python
# ä¸è¦åœ¨è®­ç»ƒå¾ªç¯ä¸­é¢‘ç¹æ”¹å˜è¾“å…¥å½¢çŠ¶
for batch in dataloader:
    x = batch[torch.randperm(len(batch))]  # åŠ¨æ€å½¢çŠ¶ï¼Œè§¦å‘é‡ç¼–è¯‘
    compiled_model(x)
```

---

## 9. æºç å¯¼è¯»

*   **`torch/_inductor/`**ï¼šTorchInductor çš„æ ¸å¿ƒå®ç°
    *   `codegen/triton.py`ï¼šTriton ä»£ç ç”Ÿæˆå™¨
    *   `fx_passes/`ï¼šå›¾ä¼˜åŒ– Passï¼ˆèåˆã€åˆ†è§£ç­‰ï¼‰
    *   `graph.py`ï¼šå›¾åˆ†æä¸è°ƒåº¦
    *   `scheduler.py`ï¼šæ“ä½œè°ƒåº¦ä¸å†…å­˜è§„åˆ’

*   **Triton ä»“åº“**ï¼š[https://github.com/openai/triton](https://github.com/openai/triton)

---

## 10. æ€»ç»“

TorchInductor æ˜¯ PyTorch 2.0 ç¼–è¯‘æ ˆçš„"ç»ˆææ­¦å™¨"ï¼Œå®ƒå°†é«˜å±‚æ¬¡çš„è®¡ç®—å›¾è½¬æ¢ä¸ºé«˜æ•ˆçš„æœºå™¨ç ã€‚

**æ ¸å¿ƒè¦ç‚¹**ï¼š
*   **ä»£ç ç”Ÿæˆ**ï¼šä½¿ç”¨ Triton è‡ªåŠ¨ç”Ÿæˆä¼˜åŒ–çš„ GPU Kernelã€‚
*   **ç®—å­èåˆ**ï¼šå‡å°‘å†…å­˜è®¿é—®å’Œ Kernel å¯åŠ¨æ¬¡æ•°ã€‚
*   **è‡ªåŠ¨ä¼˜åŒ–**ï¼šå¾ªç¯åˆ†å—ã€å†…å­˜å¸ƒå±€ã€å¸¸é‡æŠ˜å ç­‰ã€‚
*   **æ€§èƒ½æå‡**ï¼šå…¸å‹åœºæ™¯ä¸‹ **1.5-3x** åŠ é€Ÿã€‚

**æŠ€æœ¯æ ˆæ€»è§ˆ**ï¼š
```
ç”¨æˆ· Python ä»£ç 
    â†“ (Dynamo æ•è·)
FX Graph (å‰å‘å›¾)
    â†“ (AOTAutograd ç”Ÿæˆåå‘å›¾)
Optimized FX Graph (å‰å‘ + åå‘)
    â†“ (TorchInductor ä»£ç ç”Ÿæˆ)
Triton / C++ ä»£ç 
    â†“ (ç¼–è¯‘)
CUDA / CPU æœºå™¨ç 
    â†“ (æ‰§è¡Œ)
é«˜æ•ˆè®¡ç®—ç»“æœ ğŸš€
```

**ä¸‹ä¸€ç« é¢„å‘Š**ï¼š
æˆ‘ä»¬å·²ç»äº†è§£äº† PyTorch 2.0 çš„å®Œæ•´ç¼–è¯‘æ ˆã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬æ·±å…¥ PyTorch çš„åº•å±‚åŸºç¡€è®¾æ–½â€”â€”**Dispatcherï¼ˆè°ƒåº¦å™¨ï¼‰**ï¼Œç†è§£ç®—å­å¦‚ä½•è·¯ç”±åˆ°ä¸åŒçš„ç¡¬ä»¶åç«¯ã€‚è¯·çœ‹ [ç¬¬å…«ç« ï¼šPyTorch Dispatcher è°ƒåº¦æœºåˆ¶](./08_PyTorchè°ƒåº¦æœºåˆ¶.md)ã€‚

