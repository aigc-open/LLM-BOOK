# ç¬¬å…«ç« ï¼šå›½äº§è®¾å¤‡é€‚é…ä¸ Triton åç«¯æ‰©å±•

## ğŸ“– é€‚ç”¨åœºæ™¯

æœ¬ç« é€‚ç”¨äºå·²æœ‰èŠ¯ç‰‡ä¾›åº”å•†æä¾›çš„ Triton Backend åŒ…çš„æƒ…å†µï¼š
- âœ… å·²æœ‰ `triton-xxx.whl` å’Œè¿è¡Œæ—¶åº“ï¼ˆå¦‚ `triton_gxu.deb`ï¼‰
- âœ… éœ€è¦è®© TorchInductor ä½¿ç”¨ç°æœ‰çš„ Triton åç«¯
- âœ… éœ€è¦é’ˆå¯¹ç¡¬ä»¶ç‰¹æ€§è¿›è¡Œæ€§èƒ½ä¼˜åŒ–ï¼ˆå¦‚ grid sizeã€num_warpsï¼‰

## ç›®å½•

**ä¸»ä½“ç« èŠ‚**
1. [æ¦‚è¿°](#1-æ¦‚è¿°)
2. [TorchInductor è®¾å¤‡æ³¨å†Œæ¶æ„](#2-torchinductor-è®¾å¤‡æ³¨å†Œæ¶æ„)
3. [æ ¸å¿ƒç»„ä»¶è¯¦è§£](#3-æ ¸å¿ƒç»„ä»¶è¯¦è§£)
4. [å®æˆ˜ï¼šä¸º GXU å®ç° TorchInductor åç«¯](#4-å®æˆ˜ä¸º-gxu-å®ç°-torchinductor-åç«¯) â­ **æ ¸å¿ƒ**
5. [Triton Heuristics è‡ªå®šä¹‰](#5-triton-heuristics-è‡ªå®šä¹‰)
6. [è¿›é˜¶è°ƒè¯•ä¸æ€§èƒ½åˆ†æ](#6-è¿›é˜¶è°ƒè¯•ä¸æ€§èƒ½åˆ†æ)
7. [å®Œæ•´ç¤ºä¾‹](#7-å®Œæ•´ç¤ºä¾‹)
8. [å¸¸è§é—®é¢˜](#8-å¸¸è§é—®é¢˜)
9. [æ€»ç»“ä¸å±•æœ›](#9-æ€»ç»“ä¸å±•æœ›)

**é™„å½•**
- [é™„å½• Aï¼šå®Œæ•´ä»£ç æ¸…å•](#é™„å½•-aå®Œæ•´ä»£ç æ¸…å•)
- [é™„å½• Bï¼šå‚è€ƒå®ç°é“¾æ¥](#é™„å½•-bå‚è€ƒå®ç°é“¾æ¥)

---

## 1. æ¦‚è¿°

### 1.1 ä¸ºä»€ä¹ˆéœ€è¦è‡ªå®šä¹‰åç«¯

åœ¨å­¦ä¹ äº† TorchInductor å’Œ Triton ä¹‹åï¼Œæ‚¨å¯èƒ½å¸Œæœ›ï¼š
- æ”¯æŒå›½äº§ AI èŠ¯ç‰‡ï¼ˆå¦‚æ˜†ä»‘èŠ¯ã€æµ·å…‰ DCUã€åä¸ºæ˜‡è…¾ã€æ‘©å°”çº¿ç¨‹ç­‰ï¼‰
- åˆ©ç”¨ `torch.compile` çš„è‡ªåŠ¨ä¼˜åŒ–èƒ½åŠ›
- å¤ç”¨ TorchInductor çš„å›¾ä¼˜åŒ–å’Œç®—å­èåˆèƒ½åŠ›
- é¿å…ä»é›¶å®ç°æ•´ä¸ªç¼–è¯‘æ ˆ

### 1.2 æŠ€æœ¯æ ˆæ¦‚è§ˆ

```
ç”¨æˆ·æ¨¡å‹
    â†“
torch.compile
    â†“
TorchDynamo (å­—èŠ‚ç æ‹¦æˆª)
    â†“
FX Graph (è®¡ç®—å›¾)
    â†“
AOTAutograd (è‡ªåŠ¨å¾®åˆ†)
    â†“
TorchInductor (ä»£ç ç”Ÿæˆå™¨)
    â†“
    â”œâ”€â†’ Scheduling (å†…æ ¸ä»£ç ç”Ÿæˆè°ƒåº¦)
    â”‚       â”œâ”€â†’ TritonScheduling (Triton å†…æ ¸)
    â”‚       â”œâ”€â†’ CppScheduling (C++/OpenMP å†…æ ¸)
    â”‚       â””â”€â†’ è‡ªå®šä¹‰ Scheduling
    â”‚
    â””â”€â†’ WrapperCodegen (å®¿ä¸»ä»£ç ç”Ÿæˆ)
            â”œâ”€â†’ PythonWrapperCodegen (Python wrapper)
            â”œâ”€â†’ CppWrapperGpu (AOTInductor C++ wrapper)
            â””â”€â†’ è‡ªå®šä¹‰ WrapperCodegen
```

### 1.3 æœ¬ç« ç›®æ ‡

- ç†è§£ TorchInductor çš„è®¾å¤‡æ³¨å†Œæ¶æ„
- æŒæ¡ `DeviceOpOverrides` çš„ä½œç”¨å’Œå®ç°
- å­¦ä¼šä½¿ç”¨ `register_backend_for_device` æ³¨å†Œè‡ªå®šä¹‰åç«¯
- ç†è§£ `PrivateUse1` æœºåˆ¶å’Œè‡ªå®šä¹‰è®¾å¤‡æ¨¡å—æ³¨å†Œ
- é’ˆå¯¹ç¡¬ä»¶ç‰¹æ€§è¿›è¡Œæ€§èƒ½ä¼˜åŒ–ï¼ˆgrid sizeã€num_warpsï¼‰

---

## 2. TorchInductor è®¾å¤‡æ³¨å†Œæ¶æ„

### 2.1 æ¶æ„æ€»è§ˆ

TorchInductor ç”Ÿæˆçš„ä»£ç åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼š**å†…æ ¸ä»£ç **å’Œ**åŒ…è£…ä»£ç **ã€‚

```mermaid
graph TB
    subgraph "TorchInductor ä»£ç ç”Ÿæˆ"
        A[FX Graph] --> B[Lowering]
        B --> C{è®¾å¤‡ç±»å‹}
        
        C -->|CUDA/XPU| D[Scheduling]
        C -->|CPU| E[CppScheduling]
        C -->|è‡ªå®šä¹‰| F[CustomScheduling]
        
        D --> G[Triton Kernel Code]
        E --> H[C++ Kernel Code]
        F --> I[Custom Kernel Code]
        
        G --> J[WrapperCodegen]
        H --> J
        I --> J
        
        J --> K[Python/C++ Wrapper]
    end
    
    subgraph "è®¾å¤‡æ³¨å†Œ"
        L[register_backend_for_device] --> M[device_codegens dict]
        N[register_device_op_overrides] --> O[device_op_overrides_dict]
    end
```

### 2.2 æ ¸å¿ƒæ³¨å†Œå‡½æ•°

TorchInductor æä¾›äº†ä¸¤ä¸ªæ ¸å¿ƒæ³¨å†Œ APIï¼š

#### 2.2.1 `register_backend_for_device`

```python
# torch/_inductor/codegen/common.py
def register_backend_for_device(
    device: str,                                        # è®¾å¤‡åç§°ï¼Œå¦‚ "gxu"
    device_scheduling: SchedulingConstructor,           # è°ƒåº¦å™¨æ„é€ å‡½æ•°
    device_wrapper_codegen: WrapperConstructor,         # Python wrapper ä»£ç ç”Ÿæˆå™¨
    device_cpp_wrapper_codegen: Optional[WrapperConstructor] = None,  # C++ wrapper
    device_fx_wrapper_codegen: Optional[WrapperConstructor] = None,   # FX wrapper
    device_custom_pass: Optional[CustomGraphModulePass] = None,       # è‡ªå®šä¹‰ Pass
    device_custom_config: Optional[ConfigModule] = None,              # è‡ªå®šä¹‰é…ç½®
) -> None:
```

#### 2.2.2 `register_device_op_overrides`

```python
# torch/_inductor/codegen/common.py
def register_device_op_overrides(
    device: str, 
    device_op_overrides: DeviceOpOverrides
) -> None:
    """æ³¨å†Œè®¾å¤‡ç‰¹å®šçš„æ“ä½œè¦†ç›–"""
    device_op_overrides_dict[device] = device_op_overrides
```

### 2.3 å†…ç½®è®¾å¤‡æ³¨å†Œç¤ºä¾‹

```python
# torch/_inductor/codegen/common.py - init_backend_registration()

# CPU åç«¯
register_backend_for_device(
    "cpu",
    lambda scheduling: cpu_backends[config.cpu_backend](scheduling),
    PythonWrapperCodegen,
    CppWrapperCpuArrayRef if config.aot_inductor.allow_stack_allocation else CppWrapperCpu,
    WrapperFxCodegen,
)

# CUDA åç«¯
register_backend_for_device(
    "cuda",
    lambda scheduling: cuda_backends[config.cuda_backend](scheduling),
    PythonWrapperCodegen,
    CppWrapperGpu,
    WrapperFxCodegen,
)

# XPU åç«¯ (Intel GPU)
register_backend_for_device(
    "xpu",
    TritonScheduling,
    PythonWrapperCodegen,
    CppWrapperGpu,
    WrapperFxCodegen,
)
```

---

## 3. æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 3.1 DeviceOpOverrides ç±»

`DeviceOpOverrides` å®šä¹‰äº†è®¾å¤‡ç‰¹å®šçš„ä»£ç ç‰‡æ®µï¼Œç”¨äºç”Ÿæˆ wrapper ä»£ç ï¼š

```python
# torch/_inductor/codegen/common.py

class DeviceOpOverrides:
    """è®¾å¤‡æ“ä½œè¦†ç›–åŸºç±»ï¼Œå®šä¹‰è®¾å¤‡ç‰¹å®šçš„ä»£ç ç”Ÿæˆæ–¹æ³•"""
    
    def import_get_raw_stream_as(self, name: str) -> str:
        """ç”Ÿæˆè·å–åŸå§‹æµçš„å¯¼å…¥è¯­å¥"""
        raise NotImplementedError

    def set_device(self, device_idx: int) -> str:
        """ç”Ÿæˆè®¾ç½®è®¾å¤‡çš„ä»£ç """
        raise NotImplementedError

    def synchronize(self) -> str:
        """ç”ŸæˆåŒæ­¥ä»£ç """
        raise NotImplementedError

    def device_guard(self, device_idx: int) -> str:
        """ç”Ÿæˆè®¾å¤‡ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä»£ç """
        raise NotImplementedError

    def cpp_device_guard(self) -> str:
        """C++ è®¾å¤‡ Guard ç±»å"""
        raise NotImplementedError

    def cpp_aoti_device_guard(self) -> str:
        """AOTInductor è®¾å¤‡ Guard ç±»å"""
        raise NotImplementedError

    def cpp_stream_guard(self) -> str:
        """C++ Stream Guard ç±»å"""
        raise NotImplementedError

    def cpp_aoti_stream_guard(self) -> str:
        """AOTInductor Stream Guard ç±»å"""
        raise NotImplementedError

    def cpp_getStreamFromExternal(self) -> str:
        """è·å–å¤–éƒ¨æµçš„ C++ å‡½æ•°"""
        raise NotImplementedError

    def kernel_header(self) -> str:
        """å†…æ ¸å¤´æ–‡ä»¶åŒ…å«"""
        raise NotImplementedError

    def kernel_driver(self) -> str:
        """å†…æ ¸é©±åŠ¨ä»£ç ï¼ˆåŠ è½½/å¯åŠ¨å†…æ ¸ï¼‰"""
        raise NotImplementedError

    def cpp_stream_type(self) -> str:
        """C++ æµç±»å‹"""
        raise NotImplementedError

    def aoti_get_stream(self) -> str:
        """AOTInductor è·å–æµçš„å‡½æ•°"""
        raise NotImplementedError

    def cpp_kernel_type(self) -> str:
        """C++ å†…æ ¸ç±»å‹"""
        raise NotImplementedError

    def cpp_device_ptr(self) -> str:
        """C++ è®¾å¤‡æŒ‡é’ˆç±»å‹"""
        raise NotImplementedError
```

### 3.2 ç°æœ‰å®ç°å‚è€ƒ

#### 3.2.1 CUDA DeviceOpOverrides

```python
# torch/_inductor/codegen/cuda/device_op_overrides.py

class CUDADeviceOpOverrides(DeviceOpOverrides):
    def import_get_raw_stream_as(self, name: str) -> str:
        return f"from torch._C import _cuda_getCurrentRawStream as {name}"

    def set_device(self, device_idx: int) -> str:
        return f"torch.cuda.set_device({device_idx})"

    def synchronize(self) -> str:
        return "torch.cuda.synchronize()"

    def device_guard(self, device_idx: int) -> str:
        return f"torch.cuda._DeviceGuard({device_idx})"

    def cpp_device_guard(self) -> str:
        return "at::cuda::CUDAGuard"

    def cpp_aoti_device_guard(self) -> str:
        return "AOTICudaGuard"

    def cpp_stream_guard(self) -> str:
        return "at::cuda::CUDAStreamGuard"

    def cpp_aoti_stream_guard(self) -> str:
        return "AOTICudaStreamGuard"

    def cpp_getStreamFromExternal(self) -> str:
        return "at::cuda::getStreamFromExternal"

    def kernel_header(self) -> str:
        return """
        #include <c10/cuda/CUDAGuard.h>
        #include <c10/cuda/CUDAStream.h>
        #include <ATen/cuda/EmptyTensor.h>
        """

    def kernel_driver(self) -> str:
        # åŒ…å« CUDA driver API è°ƒç”¨ä»£ç 
        # loadKernel, launchKernel ç­‰å‡½æ•°
        return "..."  # è§å®Œæ•´æºç 

    def cpp_stream_type(self) -> str:
        return "cudaStream_t"

    def aoti_get_stream(self) -> str:
        return "aoti_torch_get_current_cuda_stream"

    def cpp_kernel_type(self) -> str:
        return "CUfunction"

    def cpp_device_ptr(self) -> str:
        return "CUdeviceptr"

# æ³¨å†Œ
register_device_op_overrides("cuda", CUDADeviceOpOverrides())
```

#### 3.2.2 XPU DeviceOpOverrides

```python
# torch/_inductor/codegen/xpu/device_op_overrides.py

class XPUDeviceOpOverrides(DeviceOpOverrides):
    def import_get_raw_stream_as(self, name: str) -> str:
        return f"from torch._C import _xpu_getCurrentRawStream as {name}"

    def set_device(self, device_idx: int) -> str:
        return f"torch.xpu.set_device({device_idx})"

    def synchronize(self) -> str:
        return "torch.xpu.synchronize()"

    def device_guard(self, device_idx: int) -> str:
        return f"torch.xpu._DeviceGuard({device_idx})"

    def cpp_device_guard(self) -> str:
        return "at::DeviceGuard"

    def cpp_stream_guard(self) -> str:
        return "at::xpu::XPUStreamGuard"

    def kernel_header(self) -> str:
        return """
        #include <torch/csrc/inductor/aoti_runtime/sycl_runtime_wrappers.h>
        """

    def cpp_stream_type(self) -> str:
        return "sycl::queue*"

    def cpp_kernel_type(self) -> str:
        return "std::unique_ptr<sycl::kernel>"

    def cpp_device_ptr(self) -> str:
        return "void *"

register_device_op_overrides("xpu", XPUDeviceOpOverrides())
```

### 3.3 PrivateUse1 è‡ªåŠ¨å‘ç°æœºåˆ¶

å¯¹äºè‡ªå®šä¹‰è®¾å¤‡ï¼ŒTorchInductor ä¼šå°è¯•è‡ªåŠ¨å‘ç°ï¼š

```python
# torch/_inductor/codegen/common.py - init_backend_registration()

private_backend = torch._C._get_privateuse1_backend_name()
if (
    private_backend != "privateuseone"
    and get_scheduling_for_device(private_backend) is None
):
    from torch.utils.backend_registration import _get_custom_mod_func

    try:
        # ä»è‡ªå®šä¹‰è®¾å¤‡æ¨¡å—è·å–å¿…è¦ç»„ä»¶
        device_scheduling = _get_custom_mod_func("Scheduling")
        wrapper_codegen = _get_custom_mod_func("PythonWrapperCodegen")
        cpp_wrapper_codegen = _get_custom_mod_func("CppWrapperCodegen")
        fx_wrapper_codegen = _get_custom_mod_func("WrapperFxCodegen")
        
        if device_scheduling and wrapper_codegen and cpp_wrapper_codegen:
            register_backend_for_device(
                private_backend,
                device_scheduling,
                wrapper_codegen,
                cpp_wrapper_codegen,
                fx_wrapper_codegen,
            )
    except RuntimeError:
        pass
```

---

## 4. å®æˆ˜ï¼šä¸º GXU å®ç° TorchInductor åç«¯

### 4.1 æ•´ä½“æ­¥éª¤

```mermaid
graph LR
    A[1. å®‰è£… Triton GXU] --> B[2. æ³¨å†Œ PrivateUse1]
    B --> C[3. å®ç° DeviceOpOverrides]
    C --> D[4. å®ç°è®¾å¤‡æ¨¡å—]
    D --> E[5. æ³¨å†Œåˆ° TorchInductor]
    E --> F[6. æµ‹è¯•éªŒè¯]
```

### 4.2 æ­¥éª¤ 1ï¼šå®‰è£… Triton GXU åŒ…

```bash
# å®‰è£… GXU è¿è¡Œæ—¶åº“
sudo dpkg -i triton_gxu.deb

# å®‰è£… Triton GXU Python åŒ…
pip install triton-gxu.whl

# éªŒè¯å®‰è£…
python -c "import triton; print(triton.__version__)"
python -c "import triton.backends.gxu; print('GXU backend loaded')"
```

### 4.3 æ­¥éª¤ 2ï¼šæ³¨å†Œ PrivateUse1 åç«¯åç§°

```python
# gxu/__init__.py
import torch

# æ³¨å†Œ PrivateUse1 åç«¯åç§°ä¸º "gxu"
torch.utils.rename_privateuse1_backend("gxu")
```

### 4.4 æ­¥éª¤ 3ï¼šå®ç° GXU DeviceOpOverrides

åˆ›å»ºæ–‡ä»¶ `gxu/device_op_overrides.py`ï¼š

```python
# gxu/device_op_overrides.py
from __future__ import annotations

from typing import Optional

from torch._inductor.codegen.common import (
    DeviceOpOverrides,
    register_device_op_overrides,
    TritonScratchWorkspace,
)


class GXUDeviceOpOverrides(DeviceOpOverrides):
    """GXU è®¾å¤‡çš„æ“ä½œè¦†ç›–å®ç°"""
    
    def import_get_raw_stream_as(self, name: str) -> str:
        """è·å–åŸå§‹æµçš„å¯¼å…¥è¯­å¥"""
        # éœ€è¦ GXU æä¾›ç±»ä¼¼ CUDA çš„ getCurrentRawStream API
        return f"from torch._C import _gxu_getCurrentRawStream as {name}"

    def set_device(self, device_idx: int) -> str:
        """è®¾ç½®è®¾å¤‡"""
        return f"torch.gxu.set_device({device_idx})"

    def synchronize(self) -> str:
        """åŒæ­¥è®¾å¤‡"""
        return "torch.gxu.synchronize()"

    def device_guard(self, device_idx: int) -> str:
        """è®¾å¤‡ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        return f"torch.gxu._DeviceGuard({device_idx})"

    def cpp_device_guard(self) -> str:
        """C++ è®¾å¤‡ Guard"""
        return "at::gxu::GXUGuard"

    def cpp_aoti_device_guard(self) -> str:
        """AOTInductor Guard"""
        return "AOTIGcuGuard"

    def cpp_stream_guard(self) -> str:
        """C++ Stream Guard"""
        return "at::gxu::GXUStreamGuard"

    def cpp_aoti_stream_guard(self) -> str:
        """AOTInductor Stream Guard"""
        return "AOTIGcuStreamGuard"

    def cpp_getStreamFromExternal(self) -> str:
        """ä»å¤–éƒ¨è·å–æµ"""
        return "at::gxu::getStreamFromExternal"

    def kernel_header(self) -> str:
        """å†…æ ¸å¤´æ–‡ä»¶"""
        return """
        #include <c10/gxu/GXUGuard.h>
        #include <c10/gxu/GXUStream.h>
        #include <ATen/gxu/EmptyTensor.h>
        """

    def kernel_driver(self) -> str:
        """å†…æ ¸é©±åŠ¨ä»£ç """
        return """
            #define GXU_DRIVER_CHECK(EXPR)                    \\
            do {                                               \\
                gxuError_t code = EXPR;                        \\
                if (code != GXU_SUCCESS) {                     \\
                    const char *msg = gxuGetErrorString(code); \\
                    throw std::runtime_error(                  \\
                        std::string("GXU driver error: ") +    \\
                        std::string(msg));                     \\
                }                                              \\
            } while (0);

            static inline gxuFunction loadKernel(
                    std::string filePath,
                    const std::string &funcName,
                    uint32_t sharedMemBytes,
                    const std::optional<std::string> &cubinDir = std::nullopt) {
                if (cubinDir) {
                    std::filesystem::path p1{*cubinDir};
                    std::filesystem::path p2{filePath};
                    filePath = (p1 / p2.filename()).string();
                }

                gxuModule mod;
                gxuFunction func;
                GXU_DRIVER_CHECK(gxuModuleLoad(&mod, filePath.c_str()));
                GXU_DRIVER_CHECK(gxuModuleGetFunction(&func, mod, funcName.c_str()));
                if (sharedMemBytes > 0) {
                    GXU_DRIVER_CHECK(gxuFuncSetAttribute(
                        func,
                        GXU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES,
                        sharedMemBytes
                    ))
                }
                return func;
            }

            static inline void launchKernel(
                    gxuFunction func,
                    uint32_t gridX,
                    uint32_t gridY,
                    uint32_t gridZ,
                    uint32_t numWarps,
                    uint32_t sharedMemBytes,
                    void* args[],
                    gxuStream_t stream) {
                // GXU ç‰¹æ€§ï¼šwarp size å¯èƒ½ä¸åŒäº 32
                const uint32_t GXU_WARP_SIZE = 32;  // æ ¹æ®å®é™…ç¡¬ä»¶è°ƒæ•´
                GXU_DRIVER_CHECK(gxuLaunchKernel(
                    func, gridX, gridY, gridZ, 
                    GXU_WARP_SIZE * numWarps, 1, 1, 
                    sharedMemBytes, stream, args, nullptr
                ));
            }
        """

    def cpp_stream_type(self) -> str:
        """æµç±»å‹"""
        return "gxuStream_t"

    def aoti_get_stream(self) -> str:
        """AOTInductor è·å–æµå‡½æ•°"""
        return "aoti_torch_get_current_gxu_stream"

    def cpp_kernel_type(self) -> str:
        """å†…æ ¸ç±»å‹"""
        return "gxuFunction"

    def cpp_device_ptr(self) -> str:
        """è®¾å¤‡æŒ‡é’ˆç±»å‹"""
        return "gxuDevicePtr"

    def cpp_scratch(
        self, idx: int, workspace: TritonScratchWorkspace, prefix: Optional[str] = None
    ) -> Optional[tuple[list[str], str]]:
        """ä¸´æ—¶ç©ºé—´åˆ†é…"""
        prefix = f"{prefix}_" if prefix else ""
        var_name = f"{prefix}scratch_{idx}"
        if workspace.size > 0:
            size_array = f"int64_t {var_name}_size[] = {{{workspace.size}}};"
            stride_array = f"int64_t {var_name}_stride[] = {{1}};"
            device_type = "cached_torch_device_type_gxu"
            device_idx = "device_idx_"

            return (
                [
                    f"{size_array}",
                    f"{stride_array}",
                    f"AtenTensorHandle {var_name}_handle;",
                    (
                        f"AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_empty_strided(1, {var_name}_size, {var_name}_stride, "
                        f"{workspace.generate_dtype_str()}, {device_type}, {device_idx}, &{var_name}_handle));"
                    ),
                    f"RAIIAtenTensorHandle {var_name}_tensor({var_name}_handle);",
                    f"gxuDevicePtr {var_name} = reinterpret_cast<gxuDevicePtr>({var_name}_tensor.data_ptr());",
                ],
                var_name,
            )
        else:
            return [f"gxuDevicePtr {var_name} = 0;"], var_name


# æ³¨å†Œ GXU DeviceOpOverrides
register_device_op_overrides("gxu", GXUDeviceOpOverrides())
```

### 4.5 æ­¥éª¤ 4ï¼šå®ç° GXU è®¾å¤‡æ¨¡å—

åˆ›å»ºæ–‡ä»¶ `gxu/module.py`ï¼š

```python
# gxu/module.py
import torch
import ctypes
from typing import Optional

# åŠ è½½ GXU è¿è¡Œæ—¶åº“
try:
    _libgxu = ctypes.CDLL("libgxu_runtime.so")
except OSError:
    _libgxu = None


class GXUModule:
    """GXU è®¾å¤‡æ¨¡å—ï¼Œæä¾›è®¾å¤‡ç®¡ç†åŠŸèƒ½"""
    
    @staticmethod
    def is_available() -> bool:
        """æ£€æŸ¥ GXU æ˜¯å¦å¯ç”¨"""
        if _libgxu is None:
            return False
        try:
            count = GXUModule.device_count()
            return count > 0
        except Exception:
            return False
    
    @staticmethod
    def is_initialized() -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²åˆå§‹åŒ–"""
        return _libgxu is not None
    
    @staticmethod
    def device_count() -> int:
        """è·å– GXU è®¾å¤‡æ•°é‡"""
        if _libgxu is None:
            return 0
        count = ctypes.c_int()
        ret = _libgxu.gxuGetDeviceCount(ctypes.byref(count))
        if ret != 0:
            return 0
        return count.value
    
    @staticmethod
    def current_device() -> int:
        """è·å–å½“å‰è®¾å¤‡ç´¢å¼•"""
        device = ctypes.c_int()
        _libgxu.gxuGetDevice(ctypes.byref(device))
        return device.value
    
    @staticmethod
    def set_device(device: int) -> None:
        """è®¾ç½®å½“å‰è®¾å¤‡"""
        ret = _libgxu.gxuSetDevice(device)
        if ret != 0:
            raise RuntimeError(f"Failed to set GXU device {device}")
    
    @staticmethod
    def synchronize(device: Optional[int] = None) -> None:
        """åŒæ­¥è®¾å¤‡"""
        if device is not None:
            old_device = GXUModule.current_device()
            GXUModule.set_device(device)
            _libgxu.gxuDeviceSynchronize()
            GXUModule.set_device(old_device)
        else:
            _libgxu.gxuDeviceSynchronize()
    
    @staticmethod
    def _is_in_bad_fork() -> bool:
        """æ£€æŸ¥æ˜¯å¦åœ¨ fork åçš„åçŠ¶æ€"""
        return False
    
    # ========== TorchInductor éœ€è¦çš„ç»„ä»¶ ==========
    
    @staticmethod
    def Scheduling(scheduler):
        """è¿”å›è°ƒåº¦å™¨ç±»"""
        from torch._inductor.codegen.triton import TritonScheduling
        return TritonScheduling(scheduler)
    
    @staticmethod
    def PythonWrapperCodegen():
        """è¿”å› Python Wrapper ä»£ç ç”Ÿæˆå™¨"""
        from torch._inductor.codegen.wrapper import PythonWrapperCodegen
        return PythonWrapperCodegen
    
    @staticmethod
    def CppWrapperCodegen():
        """è¿”å› C++ Wrapper ä»£ç ç”Ÿæˆå™¨"""
        from torch._inductor.codegen.cpp_wrapper_gpu import CppWrapperGpu
        return CppWrapperGpu
    
    @staticmethod
    def WrapperFxCodegen():
        """è¿”å› FX Wrapper ä»£ç ç”Ÿæˆå™¨"""
        from torch._inductor.codegen.wrapper_fxir import WrapperFxCodegen
        return WrapperFxCodegen


class _DeviceGuard:
    """GXU è®¾å¤‡ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    
    def __init__(self, device_idx: int):
        self.device_idx = device_idx
        self.prev_device = None
    
    def __enter__(self):
        self.prev_device = GXUModule.current_device()
        GXUModule.set_device(self.device_idx)
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.prev_device is not None:
            GXUModule.set_device(self.prev_device)
        return False


# æ³¨å†Œè®¾å¤‡æ¨¡å—
torch._register_device_module("gxu", GXUModule)

# æ·»åŠ å¿«æ·è®¿é—®
torch.gxu = GXUModule
torch.gxu._DeviceGuard = _DeviceGuard
```

### 4.6 æ­¥éª¤ 5ï¼šå®ç°è‡ªå®šä¹‰ Schedulingï¼ˆå¯é€‰ï¼‰

å¦‚æœéœ€è¦é’ˆå¯¹ GXU ä¼˜åŒ– Triton å†…æ ¸é…ç½®ï¼Œå¯ä»¥åˆ›å»ºè‡ªå®šä¹‰ Schedulingï¼š

```python
# gxu/scheduling.py
from torch._inductor.codegen.triton import TritonScheduling
from torch._inductor.codegen.common import BackendFeature
from torch.utils._ordered_set import OrderedSet


class GXUTritonScheduling(TritonScheduling):
    """GXU ä¸“ç”¨çš„ Triton è°ƒåº¦å™¨"""
    
    backend_features = OrderedSet([
        BackendFeature.FOREACH,
        BackendFeature.BUCKETIZE,
        BackendFeature.INPLACE_BUFFERS,
        BackendFeature.MASKED_SCATTER_WITH_INDEX,
        BackendFeature.SCAN,
        BackendFeature.SORT,
        BackendFeature.TRITON_TEMPLATES,
        BackendFeature.TUPLE_REDUCTION,
    ])
    
    def __init__(self, scheduler):
        super().__init__(scheduler)
        # GXU ç‰¹å®šçš„é…ç½®
        self.gxu_max_grid_size = 48
        self.gxu_optimal_num_warps = 1
    
    def get_backend_features(self, device):
        """è¿”å›åç«¯æ”¯æŒçš„ç‰¹æ€§"""
        return self.backend_features
```

### 4.7 æ­¥éª¤ 6ï¼šå®Œæ•´åˆå§‹åŒ–è„šæœ¬

åˆ›å»º `gxu/__init__.py`ï¼š

```python
# gxu/__init__.py
"""
GXU TorchInductor åç«¯
ç”¨æ³•ï¼š
    import gxu  # è‡ªåŠ¨å®Œæˆæ‰€æœ‰æ³¨å†Œ
    
    model = MyModel().to("gxu")
    compiled = torch.compile(model)
"""
import torch

# 1. æ³¨å†Œåç«¯åç§°
torch.utils.rename_privateuse1_backend("gxu")

# 2. å¯¼å…¥å¹¶æ³¨å†Œè®¾å¤‡æ¨¡å—
from . import module  # æ³¨å†Œ torch.gxu

# 3. å¯¼å…¥å¹¶æ³¨å†Œ DeviceOpOverrides
from . import device_op_overrides

# 4. å¯é€‰ï¼šå¯¼å…¥è‡ªå®šä¹‰ Scheduling
# from . import scheduling

# 5. å¯¼å‡ºå…¬å…± API
__all__ = ["is_available", "device_count", "set_device", "synchronize"]

is_available = module.GXUModule.is_available
device_count = module.GXUModule.device_count
set_device = module.GXUModule.set_device
synchronize = module.GXUModule.synchronize

print(f"GXU backend initialized. {device_count()} device(s) available.")
```

---

## 5. Triton Heuristics è‡ªå®šä¹‰

### 5.1 ç†è§£ Triton Heuristics

TorchInductor ä½¿ç”¨ heuristics å†³å®š Triton kernel çš„å¯åŠ¨é…ç½®ï¼š

```python
# torch/_inductor/runtime/triton_heuristics.py

def triton_config(
    size_hints,
    x,
    y=None,
    z=None,
    num_stages=1,
    num_elements_per_warp=256,
    min_elem_per_thread=0,
    num_warps=None,
    matrix_instr=None,
    waves_per_eu=None,
) -> Config:
    """æ„é€  pointwise Triton é…ç½®"""
    # é»˜è®¤ grid å’Œ block å¤§å°é™åˆ¶
    maxGridSize = [2147483647, 65535, 65535]
    # ...
```

### 5.2 è®¾å¤‡å±æ€§æ„ŸçŸ¥

heuristics é€šè¿‡ `DeviceProperties` è·å–è®¾å¤‡ä¿¡æ¯ï¼š

```python
# torch/_inductor/runtime/hints.py

class DeviceProperties(typing.NamedTuple):
    type: str
    index: int
    multi_processor_count: int
    cc: int  # compute capability
    major: int | None = None
    regs_per_multiprocessor: int | None = None
    max_threads_per_multi_processor: int | None = None
    max_threads_per_block: int | None = None
    warp_size: int | None = None

    @classmethod
    @functools.cache
    def create(cls, device) -> DeviceProperties:
        """ä»è®¾å¤‡è·å–å±æ€§"""
        device_interface = get_interface_for_device(device)
        props = device_interface.get_device_properties(device)
        
        # ç‰¹æ®Šå¤„ç†ä¸åŒè®¾å¤‡ç±»å‹
        if device_type == "xpu":
            multi_processor_count = props.gpu_subslice_count
        elif device_type == "mtia":
            multi_processor_count = 64
        # ...
```

### 5.3 ä¸º GXU è‡ªå®šä¹‰ Heuristics

```python
# gxu/heuristics.py
import functools
from torch._inductor.runtime.triton_heuristics import triton_config
from torch._inductor.runtime.hints import DeviceProperties


class GXUHeuristics:
    """GXU ä¸“ç”¨çš„å¯å‘å¼é…ç½®"""
    
    # GXU ç¡¬ä»¶é™åˆ¶
    MAX_GRID_SIZE = 48  # Grid æœ€å¥½ < 48 æˆ– 48 çš„å€æ•°
    OPTIMAL_NUM_WARPS = 1  # Warp æ•°æœ€å¥½ä¸º 1
    WARP_SIZE = 32  # å¯èƒ½éœ€è¦æ ¹æ®å®é™…ç¡¬ä»¶è°ƒæ•´
    
    @staticmethod
    def adjust_grid(grid):
        """è°ƒæ•´ grid å¤§å°ä»¥é€‚åº” GXU"""
        if isinstance(grid, (list, tuple)):
            adjusted = []
            for dim in grid:
                if dim <= GXUHeuristics.MAX_GRID_SIZE:
                    adjusted.append(dim)
                elif dim % GXUHeuristics.MAX_GRID_SIZE == 0:
                    adjusted.append(dim)
                else:
                    # å‘ä¸Šå–æ•´åˆ° 48 çš„å€æ•°
                    new_dim = ((dim + GXUHeuristics.MAX_GRID_SIZE - 1) 
                               // GXUHeuristics.MAX_GRID_SIZE 
                               * GXUHeuristics.MAX_GRID_SIZE)
                    adjusted.append(new_dim)
            return tuple(adjusted)
        else:
            if grid <= GXUHeuristics.MAX_GRID_SIZE:
                return grid
            elif grid % GXUHeuristics.MAX_GRID_SIZE == 0:
                return grid
            else:
                return ((grid + GXUHeuristics.MAX_GRID_SIZE - 1) 
                        // GXUHeuristics.MAX_GRID_SIZE 
                        * GXUHeuristics.MAX_GRID_SIZE)
    
    @staticmethod
    def get_config(size_hints, **kwargs):
        """è·å– GXU ä¼˜åŒ–çš„é…ç½®"""
        # å¼ºåˆ¶ä½¿ç”¨æœ€ä¼˜ num_warps
        kwargs['num_warps'] = GXUHeuristics.OPTIMAL_NUM_WARPS
        return triton_config(size_hints, **kwargs)


# Monkey patch ç¤ºä¾‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
def patch_triton_heuristics():
    """ä¿®æ”¹ TorchInductor çš„é»˜è®¤ heuristics"""
    import torch._inductor.runtime.triton_heuristics as th
    
    _original_triton_config = th.triton_config
    
    @functools.wraps(_original_triton_config)
    def gxu_triton_config(*args, **kwargs):
        # å¼ºåˆ¶ num_warps=1 for GXU
        import torch
        if torch.gxu.is_available():
            kwargs['num_warps'] = GXUHeuristics.OPTIMAL_NUM_WARPS
        return _original_triton_config(*args, **kwargs)
    
    th.triton_config = gxu_triton_config
```

### 5.4 è‡ªå®šä¹‰ DeviceProperties

```python
# gxu/device_properties.py
from torch._inductor.runtime.hints import DeviceProperties


def create_gxu_device_properties(device) -> DeviceProperties:
    """åˆ›å»º GXU è®¾å¤‡å±æ€§"""
    import torch
    
    # ä» GXU runtime è·å–å±æ€§
    props = get_gxu_device_properties(device.index)
    
    return DeviceProperties(
        type="gxu",
        index=device.index,
        multi_processor_count=props.multi_processor_count,
        cc=props.compute_capability,
        major=props.major,
        regs_per_multiprocessor=props.regs_per_multiprocessor,
        max_threads_per_multi_processor=props.max_threads_per_sm,
        max_threads_per_block=props.max_threads_per_block,
        warp_size=32,  # æˆ– GXU å®é™…çš„ warp size
    )
```

---

## 6. è¿›é˜¶è°ƒè¯•ä¸æ€§èƒ½åˆ†æ

### 6.1 æŸ¥çœ‹ç”Ÿæˆçš„ä»£ç 

```python
import torch
import os

# å¯ç”¨ä»£ç è¾“å‡º
os.environ["TORCH_LOGS"] = "+output_code"
torch._inductor.config.debug = True
torch._inductor.config.trace.enabled = True
torch._inductor.config.trace.output_dir = "/tmp/inductor_gxu"

# ç¼–è¯‘æ¨¡å‹
model = MyModel().to("gxu")
compiled = torch.compile(model, backend="inductor")

# è¿è¡Œ
x = torch.randn(32, 128, device="gxu")
output = compiled(x)

print(f"ç”Ÿæˆçš„ä»£ç ä¿å­˜åœ¨: /tmp/inductor_gxu")
```

### 6.2 è°ƒè¯• DeviceOpOverrides

```python
# éªŒè¯ DeviceOpOverrides æ³¨å†Œ
from torch._inductor.codegen.common import get_device_op_overrides

try:
    overrides = get_device_op_overrides("gxu")
    print("DeviceOpOverrides å·²æ³¨å†Œ")
    print(f"  set_device: {overrides.set_device(0)}")
    print(f"  synchronize: {overrides.synchronize()}")
    print(f"  device_guard: {overrides.device_guard(0)}")
except KeyError:
    print("é”™è¯¯: GXU DeviceOpOverrides æœªæ³¨å†Œ")
```

### 6.3 è°ƒè¯• Backend æ³¨å†Œ

```python
from torch._inductor.codegen.common import (
    get_scheduling_for_device,
    get_wrapper_codegen_for_device,
)

# æ£€æŸ¥è°ƒåº¦å™¨
scheduling = get_scheduling_for_device("gxu")
print(f"Scheduling: {scheduling}")

# æ£€æŸ¥ wrapper codegen
wrapper = get_wrapper_codegen_for_device("gxu")
print(f"Wrapper Codegen: {wrapper}")

cpp_wrapper = get_wrapper_codegen_for_device("gxu", cpp_wrapper=True)
print(f"C++ Wrapper Codegen: {cpp_wrapper}")
```

### 6.4 å¸¸è§é—®é¢˜è¯Šæ–­

#### é—®é¢˜ 1ï¼šè®¾å¤‡æœªæ‰¾åˆ°

```python
# æ£€æŸ¥è®¾å¤‡æ³¨å†Œ
import torch
print(f"PrivateUse1 åç«¯å: {torch._C._get_privateuse1_backend_name()}")
print(f"GXU æ¨¡å—æ˜¯å¦å­˜åœ¨: {hasattr(torch, 'gxu')}")
print(f"è®¾å¤‡æ•°é‡: {torch.gxu.device_count()}")
```

#### é—®é¢˜ 2ï¼šTriton ç¼–è¯‘å¤±è´¥

```python
# æ£€æŸ¥ Triton åç«¯
import triton
print(f"Triton ç‰ˆæœ¬: {triton.__version__}")

# æ£€æŸ¥ GXU åç«¯æ˜¯å¦åŠ è½½
try:
    import triton.backends.gxu
    print("Triton GXU åç«¯å·²åŠ è½½")
except ImportError:
    print("é”™è¯¯: Triton GXU åç«¯æœªå®‰è£…")
```

---

## 7. å®Œæ•´ç¤ºä¾‹

### 7.1 å®Œæ•´ç›®å½•ç»“æ„

```
gxu/
â”œâ”€â”€ __init__.py           # åˆå§‹åŒ–å’Œå¯¼å‡º
â”œâ”€â”€ module.py             # è®¾å¤‡æ¨¡å—
â”œâ”€â”€ device_op_overrides.py # DeviceOpOverrides å®ç°
â”œâ”€â”€ scheduling.py         # è‡ªå®šä¹‰ Scheduling (å¯é€‰)
â”œâ”€â”€ heuristics.py         # è‡ªå®šä¹‰ heuristics (å¯é€‰)
â””â”€â”€ device_properties.py  # è®¾å¤‡å±æ€§ (å¯é€‰)
```

### 7.2 ä½¿ç”¨ç¤ºä¾‹

```python
import torch
import torch.nn as nn

# å¯¼å…¥ GXU åç«¯ï¼ˆè‡ªåŠ¨å®Œæˆæ³¨å†Œï¼‰
import gxu

# å®šä¹‰æ¨¡å‹
class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear1 = nn.Linear(512, 1024)
        self.linear2 = nn.Linear(1024, 512)
        self.linear3 = nn.Linear(512, 10)
    
    def forward(self, x):
        x = torch.relu(self.linear1(x))
        x = torch.relu(self.linear2(x))
        x = self.linear3(x)
        return x

# åˆ›å»ºæ¨¡å‹å¹¶ç§»åˆ° GXU
model = SimpleModel().to("gxu")

# ç¼–è¯‘æ¨¡å‹
compiled_model = torch.compile(model, backend="inductor")

# è¿è¡Œæ¨ç†
x = torch.randn(64, 512, device="gxu")
with torch.no_grad():
    output = compiled_model(x)

print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
print(f"è¾“å‡ºè®¾å¤‡: {output.device}")

# æ€§èƒ½å¯¹æ¯”
import time

def benchmark(model, x, num_runs=100):
    # é¢„çƒ­
    for _ in range(10):
        _ = model(x)
    torch.gxu.synchronize()
    
    # è®¡æ—¶
    start = time.time()
    for _ in range(num_runs):
        _ = model(x)
    torch.gxu.synchronize()
    
    return (time.time() - start) / num_runs * 1000  # ms

eager_time = benchmark(model, x)
compiled_time = benchmark(compiled_model, x)

print(f"\næ€§èƒ½å¯¹æ¯”:")
print(f"  Eager æ¨¡å¼:    {eager_time:.2f} ms")
print(f"  Compiled æ¨¡å¼: {compiled_time:.2f} ms")
print(f"  åŠ é€Ÿæ¯”:        {eager_time / compiled_time:.2f}x")
```

---

## 8. å¸¸è§é—®é¢˜

### Q1: DeviceOpOverrides å’Œ register_backend_for_device çš„åŒºåˆ«ï¼Ÿ

**DeviceOpOverrides**ï¼šå®šä¹‰è®¾å¤‡ç‰¹å®šçš„**ä»£ç ç‰‡æ®µ**ï¼Œç”¨äº wrapper ä»£ç ç”Ÿæˆ
- `set_device()` è¿”å›çš„æ˜¯ä»£ç å­—ç¬¦ä¸²ï¼Œå¦‚ `"torch.cuda.set_device(0)"`
- ç”¨äºç”Ÿæˆ Python/C++ wrapper ä¸­çš„è®¾å¤‡ç®¡ç†ä»£ç 

**register_backend_for_device**ï¼šæ³¨å†Œ**ä»£ç ç”Ÿæˆå™¨ç±»**
- `Scheduling` å†³å®šå¦‚ä½•ç”Ÿæˆå†…æ ¸ä»£ç 
- `WrapperCodegen` å†³å®šå¦‚ä½•ç”Ÿæˆ wrapper ä»£ç æ¡†æ¶

### Q2: å¦‚ä½•å¤„ç†ä¸åŒçš„ warp sizeï¼Ÿ

```python
# åœ¨ DeviceOpOverrides.kernel_driver() ä¸­è°ƒæ•´
def kernel_driver(self) -> str:
    return f"""
        static inline void launchKernel(...) {{
            // GXU warp size å¯èƒ½ä¸æ˜¯ 32
            const uint32_t GXU_WARP_SIZE = {self.warp_size};
            gxuLaunchKernel(
                func, gridX, gridY, gridZ,
                GXU_WARP_SIZE * numWarps, 1, 1,
                ...
            );
        }}
    """
```

### Q3: å¦‚ä½•ä½¿ç”¨è‡ªå®šä¹‰çš„ Triton åç«¯ï¼Ÿ

Triton ä¼šé€šè¿‡ç¯å¢ƒå˜é‡æˆ– `triton.runtime.driver.set_active()` é€‰æ‹©åç«¯ï¼š

```python
import os
os.environ["TRITON_BACKEND"] = "gxu"

# æˆ–
import triton
triton.runtime.driver.set_active("gxu")
```

### Q4: AOTInductor æ”¯æŒéœ€è¦ä»€ä¹ˆé¢å¤–å·¥ä½œï¼Ÿ

1. å®ç° C++ å¤´æ–‡ä»¶ï¼ˆ`c10/gxu/GXUGuard.h` ç­‰ï¼‰
2. å®ç° AOTInductor runtime wrapperï¼ˆ`AOTIGcuGuard` ç­‰ï¼‰
3. åœ¨ `cpp_aoti_*` æ–¹æ³•ä¸­è¿”å›æ­£ç¡®çš„ç±»å

---

## 9. æ€»ç»“ä¸å±•æœ›

### 9.1 å®ç°æ€»ç»“

é€šè¿‡æœ¬ç« å­¦ä¹ ï¼Œæ‚¨å·²ç»æŒæ¡ï¼š

| ç»„ä»¶ | ä½œç”¨ | æ–‡ä»¶ä½ç½® |
|------|------|----------|
| `DeviceOpOverrides` | è®¾å¤‡ç‰¹å®šä»£ç ç‰‡æ®µ | `codegen/common.py` |
| `register_device_op_overrides` | æ³¨å†Œ DeviceOpOverrides | `codegen/common.py` |
| `register_backend_for_device` | æ³¨å†Œåç«¯ä»£ç ç”Ÿæˆå™¨ | `codegen/common.py` |
| `PrivateUse1` æœºåˆ¶ | è‡ªå®šä¹‰è®¾å¤‡æ”¯æŒ | `torch.utils.rename_privateuse1_backend` |
| `_get_custom_mod_func` | è‡ªåŠ¨å‘ç°è®¾å¤‡æ¨¡å— | `torch.utils.backend_registration` |

### 9.2 æŠ€æœ¯è·¯çº¿å›¾

```mermaid
graph TB
    A[1. å®‰è£… Triton GXU åŒ…] --> B[2. æ³¨å†Œ PrivateUse1 åç«¯]
    B --> C[3. å®ç° DeviceOpOverrides]
    C --> D[4. å®ç°è®¾å¤‡æ¨¡å—]
    D --> E[5. éªŒè¯åŸºç¡€åŠŸèƒ½]
    E --> F[6. ä¼˜åŒ– Heuristics]
    F --> G[7. æ”¯æŒ AOTInductor]
    G --> H[8. ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²]
```

### 9.3 å‚è€ƒèµ„æº

- [Intel IPEX Inductor å®ç°](https://github.com/intel/intel-extension-for-pytorch/blob/main/intel_extension_for_pytorch/_inductor/__init__.py)
- [PyTorch Device Extension Guide](https://pytorch.org/tutorials/advanced/extend_device.html)
- [TorchInductor æºç ](https://github.com/pytorch/pytorch/tree/main/torch/_inductor)

---

## é™„å½• Aï¼šå®Œæ•´ä»£ç æ¸…å•

å®Œæ•´ä»£ç è§ï¼š

```
gxu/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ module.py
â”œâ”€â”€ device_op_overrides.py
â”œâ”€â”€ scheduling.py (å¯é€‰)
â”œâ”€â”€ heuristics.py (å¯é€‰)
â””â”€â”€ device_properties.py (å¯é€‰)
```

## é™„å½• Bï¼šå‚è€ƒå®ç°é“¾æ¥

| é¡¹ç›® | é“¾æ¥ | è¯´æ˜ |
|------|------|------|
| Intel IPEX | [GitHub](https://github.com/intel/intel-extension-for-pytorch) | XPU åç«¯å‚è€ƒ |
| AMD ROCm | [GitHub](https://github.com/ROCmSoftwarePlatform/triton) | Triton AMD åç«¯ |
| PyTorch XPU | `torch/_inductor/codegen/xpu/` | XPU DeviceOpOverrides |
| PyTorch CUDA | `torch/_inductor/codegen/cuda/` | CUDA DeviceOpOverrides |

---

