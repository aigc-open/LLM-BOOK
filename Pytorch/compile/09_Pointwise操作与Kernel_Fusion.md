# ç¬¬ä¹ç« ï¼šPointwise æ“ä½œä¸ Kernel Fusion

## ğŸ“– æœ¬ç« æ¦‚è¦

æœ¬ç« æ·±å…¥è®²è§£ TorchInductor ä¸­æœ€æ ¸å¿ƒçš„ä¼˜åŒ–æŠ€æœ¯ä¹‹ä¸€ï¼š**Kernel Fusionï¼ˆç®—å­èåˆï¼‰**ã€‚é€šè¿‡ç†è§£ Pointwise æ“ä½œçš„æœ¬è´¨ï¼Œæ‚¨å°†æ˜ç™½ï¼š
- ä¸ºä»€ä¹ˆ `torch.compile` èƒ½å¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡
- TorchInductor å¦‚ä½•å†³å®šç”Ÿæˆ Triton Kernel è¿˜æ˜¯è°ƒç”¨å¤–éƒ¨åº“
- å¦‚ä½•è®¾è®¡æ¨¡å‹ä»¥æœ€å¤§åŒ–ç¼–è¯‘ä¼˜åŒ–æ•ˆæœ

## ç›®å½•

1. [Pointwise æ“ä½œè¯¦è§£](#1-pointwise-æ“ä½œè¯¦è§£)
2. [TorchInductor çš„ Kernel ç­–ç•¥](#2-torchinductor-çš„-kernel-ç­–ç•¥)
3. [Kernel Fusion åŸç†](#3-kernel-fusion-åŸç†)
4. [å®æˆ˜ï¼šè§‚å¯Ÿ Kernel Fusion æ•ˆæœ](#4-å®æˆ˜è§‚å¯Ÿ-kernel-fusion-æ•ˆæœ)
5. [ä¼˜åŒ–å»ºè®®](#5-ä¼˜åŒ–å»ºè®®)
6. [å¸¸è§é—®é¢˜](#6-å¸¸è§é—®é¢˜)

---

## 1. Pointwise æ“ä½œè¯¦è§£

### 1.1 ä»€ä¹ˆæ˜¯ Pointwise æ“ä½œ

**Pointwiseï¼ˆé€ç‚¹ï¼‰æ“ä½œ** æ˜¯æŒ‡å¯¹å¼ é‡ä¸­çš„æ¯ä¸ªå…ƒç´ **ç‹¬ç«‹**è¿›è¡Œè®¡ç®—çš„æ“ä½œã€‚å…³é”®ç‰¹å¾æ˜¯ï¼š**æ¯ä¸ªè¾“å‡ºå…ƒç´ åªä¾èµ–å¯¹åº”ä½ç½®çš„è¾“å…¥å…ƒç´ **ã€‚

```python
# Pointwise æ“ä½œç¤ºä¾‹
y[i] = f(x[i])           # ä¸€å…ƒæ“ä½œ
z[i] = f(x[i], y[i])     # äºŒå…ƒæ“ä½œ

# å…·ä½“ä¾‹å­
y = torch.relu(x)        # y[i] = max(0, x[i])
y = x + 1                # y[i] = x[i] + 1
y = x * scale            # y[i] = x[i] * scale[i]
y = torch.sigmoid(x)     # y[i] = 1 / (1 + exp(-x[i]))
```

### 1.2 æ“ä½œç±»å‹åˆ†ç±»

| æ“ä½œç±»å‹ | å®šä¹‰ | ç¤ºä¾‹ | æ•°æ®ä¾èµ– |
|---------|------|------|----------|
| **Pointwise** | é€å…ƒç´ ç‹¬ç«‹è®¡ç®— | `relu`, `sigmoid`, `+`, `*` | æ—  |
| **Reduction** | å¤šå…ƒç´ å½’çº¦ä¸ºä¸€ä¸ª | `sum`, `mean`, `max` | åŒç»´åº¦æ‰€æœ‰å…ƒç´  |
| **Matmul** | çŸ©é˜µä¹˜æ³• | `@`, `mm`, `Linear` | æ•´è¡Œ/æ•´åˆ— |
| **Scatter/Gather** | ç´¢å¼•æ“ä½œ | `index_select`, `scatter_` | ç´¢å¼•ä½ç½® |

### 1.3 æ•°æ®ä¾èµ–å›¾ç¤º

```
Pointwise æ“ä½œï¼ˆæ— æ•°æ®ä¾èµ–ï¼‰ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  x: [a, b, c, d, e]                 â”‚
â”‚      â†“  â†“  â†“  â†“  â†“   (ç‹¬ç«‹è®¡ç®—)     â”‚
â”‚  y: [f(a), f(b), f(c), f(d), f(e)]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Reduction æ“ä½œï¼ˆè·¨å…ƒç´ ä¾èµ–ï¼‰ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  x: [a, b, c, d, e]                 â”‚
â”‚      â†˜  â†“  â†“  â†“  â†™   (å…¨éƒ¨å‚ä¸)     â”‚
â”‚  y:      sum(x)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Matmul æ“ä½œï¼ˆè¡Œåˆ—ä¾èµ–ï¼‰ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  A[i,:] @ B[:,j] = C[i,j]           â”‚
â”‚  éœ€è¦è®¿é—® A çš„ç¬¬ i è¡Œå’Œ B çš„ç¬¬ j åˆ—   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.4 å¸¸è§ Pointwise æ“ä½œåˆ—è¡¨

```python
# ä¸€å…ƒ Pointwise æ“ä½œ
torch.relu(x)
torch.sigmoid(x)
torch.tanh(x)
torch.gelu(x)
torch.exp(x)
torch.log(x)
torch.sqrt(x)
torch.abs(x)
torch.neg(x)
x.pow(2)
x.clamp(min=0, max=1)

# äºŒå…ƒ Pointwise æ“ä½œ
x + y
x - y
x * y
x / y
torch.maximum(x, y)
torch.minimum(x, y)
torch.where(cond, x, y)

# å¸¦å¹¿æ’­çš„ Pointwise æ“ä½œ
x + scalar
x * scale  # scale å¯ä»¥æ˜¯æ ‡é‡æˆ–å¯å¹¿æ’­çš„å¼ é‡
x + bias   # bias å¯ä»¥æ˜¯å¯å¹¿æ’­çš„å¼ é‡
```

---

## 2. TorchInductor çš„ Kernel ç­–ç•¥

### 2.1 ç­–ç•¥æ¦‚è§ˆ

TorchInductor ä¼šæ ¹æ®æ“ä½œç±»å‹é€‰æ‹©ä¸åŒçš„å®ç°æ–¹å¼ï¼š

```
                     TorchInductor
                          â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â–¼              â–¼              â–¼
      Pointwise      Reduction       Matmul
           â”‚              â”‚              â”‚
           â–¼              â–¼              â–¼
     Triton Kernel   Triton Kernel   extern_kernels
     (è‡ªåŠ¨ç”Ÿæˆ)      (è‡ªåŠ¨ç”Ÿæˆ)      (è°ƒç”¨åº“å‡½æ•°)
```

### 2.2 ä¸ºä»€ä¹ˆ Matmul ä½¿ç”¨å¤–éƒ¨åº“ï¼Ÿ

**åŸå›  1ï¼šcuBLAS/cuDNN å·²æåº¦ä¼˜åŒ–**

```python
# cuBLAS GEMM çš„ä¼˜åŒ–å†ç¨‹
# - 1998: ç¬¬ä¸€ç‰ˆ BLAS
# - 2007: cuBLAS å‘å¸ƒ
# - è‡³ä»Š: 20+ å¹´çš„æŒç»­ä¼˜åŒ–
# - åŒ…å«: æ‰‹å†™æ±‡ç¼–ã€ç¡¬ä»¶ç‰¹å®šä¼˜åŒ–ã€å¤šç§ tiling ç­–ç•¥
```

**åŸå›  2ï¼šMatmul çš„å¤æ‚æ€§**

```python
# Matmul çš„é«˜æ•ˆå®ç°éœ€è¦è€ƒè™‘ï¼š
# 1. Tilingï¼ˆåˆ†å—ï¼‰ç­–ç•¥
# 2. å…±äº«å†…å­˜ä½¿ç”¨
# 3. å¯„å­˜å™¨åˆ†é…
# 4. å†…å­˜è®¿é—®åˆå¹¶
# 5. æµæ°´çº¿éšè—å»¶è¿Ÿ
# 6. Tensor Core åˆ©ç”¨ï¼ˆå¦‚æœæ”¯æŒï¼‰

# ä¸€ä¸ªç®€å•çš„ Triton matmul å®ç°ï¼ˆæ€§èƒ½é€šå¸¸ä¸å¦‚ cuBLASï¼‰
@triton.jit
def matmul_kernel(A, B, C, M, N, K, ...):
    # éœ€è¦å¤æ‚çš„ tiling å’Œå†…å­˜ç®¡ç†
    # å³ä½¿ç²¾å¿ƒä¼˜åŒ–ï¼Œä¹Ÿå¾ˆéš¾è¶…è¶Š cuBLAS
    pass
```

**åŸå›  3ï¼šæˆæœ¬æ•ˆç›Š**

| æ–¹æ¡ˆ | å¼€å‘æˆæœ¬ | æ€§èƒ½ |
|------|---------|------|
| è°ƒç”¨ cuBLAS | ä½ï¼ˆå·²æœ‰ï¼‰ | æœ€ä¼˜ |
| Triton ç”Ÿæˆ | é«˜ï¼ˆéœ€ä¼˜åŒ–ï¼‰ | é€šå¸¸è¾ƒå·® |

### 2.3 ä¸ºä»€ä¹ˆ Pointwise ä½¿ç”¨ Tritonï¼Ÿ

**åŸå›  1ï¼šæ²¡æœ‰ç°æˆçš„èåˆåº“**

```python
# æ²¡æœ‰ cuBLAS å‡½æ•°å¯ä»¥åšè¿™ä¸ªï¼š
y = relu(x * scale + bias)

# åªèƒ½æ‹†æˆå¤šä¸ªè°ƒç”¨ï¼š
tmp1 = x * scale     # ä¸€æ¬¡ kernel
tmp2 = tmp1 + bias   # åˆä¸€æ¬¡ kernel
y = relu(tmp2)       # å†ä¸€æ¬¡ kernel
```

**åŸå›  2ï¼šPointwise æ“ä½œæ˜“äºå¹¶è¡Œ**

```python
# Pointwise æ“ä½œå¤©ç„¶å¹¶è¡Œï¼Œæ¯ä¸ªçº¿ç¨‹ç‹¬ç«‹å¤„ç†
@triton.jit
def pointwise_kernel(x, scale, bias, out, N):
    idx = tl.program_id(0) * BLOCK + tl.arange(0, BLOCK)
    mask = idx < N
    
    val = tl.load(x + idx, mask=mask)
    s = tl.load(scale + idx, mask=mask)
    b = tl.load(bias + idx, mask=mask)
    
    # ç®€å•ç›´æ¥ï¼Œæ— éœ€å¤æ‚çš„åŒæ­¥æˆ–é€šä¿¡
    result = tl.maximum(val * s + b, 0)  # fused relu(x*s+b)
    
    tl.store(out + idx, result, mask=mask)
```

**åŸå›  3ï¼šFusion å¸¦æ¥çš„æ”¶ç›Šå·¨å¤§**

```python
# æœªèåˆï¼ˆ5 æ¬¡å†…å­˜å¾€è¿”ï¼‰
tmp1 = x * scale     # è¯» x, scale -> å†™ tmp1
tmp2 = tmp1 + bias   # è¯» tmp1, bias -> å†™ tmp2
tmp3 = relu(tmp2)    # è¯» tmp2 -> å†™ tmp3
tmp4 = tmp3 * 2.0    # è¯» tmp3 -> å†™ tmp4
y = sigmoid(tmp4)    # è¯» tmp4 -> å†™ y

# èåˆåï¼ˆ1 æ¬¡å†…å­˜å¾€è¿”ï¼‰
y = fused_kernel(x, scale, bias)  # è¯» x, scale, bias -> å†™ y
# ä¸­é—´ç»“æœå…¨éƒ¨åœ¨å¯„å­˜å™¨ä¸­ï¼Œä¸å†™å›æ˜¾å­˜ï¼
```

### 2.4 Kernel ç±»å‹å†³ç­–æ ‘

```
                    æ“ä½œåˆ†æ
                       â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                       â”‚
      æ˜¯å¦ä¸º Matmul?          æ˜¯å¦ä¸º Pointwise?
           â”‚                       â”‚
      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
      Yes       No           Yes       No
      â”‚         â”‚            â”‚         â”‚
  extern     æ£€æŸ¥å…¶ä»–     ç”Ÿæˆ Triton  æ£€æŸ¥å…¶ä»–
  (cuBLAS)   æ“ä½œç±»å‹     Kernel       æ“ä½œç±»å‹
```

---

## 3. Kernel Fusion åŸç†

### 3.1 ä»€ä¹ˆæ˜¯ Kernel Fusion

**Kernel Fusionï¼ˆç®—å­èåˆï¼‰** æ˜¯å°†å¤šä¸ªè¿ç»­çš„æ“ä½œåˆå¹¶æˆä¸€ä¸ª GPU Kernel æ‰§è¡Œçš„ä¼˜åŒ–æŠ€æœ¯ã€‚

```python
# æ¦‚å¿µå›¾ç¤º
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æœªèåˆ                            â”‚
â”‚  Kernel1    Kernel2    Kernel3    Kernel4           â”‚
â”‚  x â†’ tmp1 â†’ tmp1 â†’ tmp2 â†’ tmp2 â†’ tmp3 â†’ tmp3 â†’ y   â”‚
â”‚  [å…¨å±€å†…å­˜å†™å…¥/è¯»å–]  [å…¨å±€å†…å­˜å†™å…¥/è¯»å–]  ...        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                         â†“ Fusion

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    èåˆå                            â”‚
â”‚              Fused Kernel                           â”‚
â”‚  x â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ y  â”‚
â”‚        [ä¸­é—´ç»“æœä¿æŒåœ¨å¯„å­˜å™¨/å…±äº«å†…å­˜ä¸­]              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Fusion çš„æ€§èƒ½æ”¶ç›Š

#### 3.2.1 å‡å°‘å†…å­˜å¸¦å®½æ¶ˆè€—

```python
# å‡è®¾å¼ é‡å¤§å°ä¸º Nï¼Œæ¯ä¸ªå…ƒç´  4 å­—èŠ‚

# æœªèåˆï¼š5 ä¸ªæ“ä½œ
# å†…å­˜è¯»å†™é‡ = 5 * 2 * N * 4 = 40N å­—èŠ‚ï¼ˆæ¯ä¸ªæ“ä½œè¯»+å†™ï¼‰

# èåˆåï¼š1 ä¸ªæ“ä½œ
# å†…å­˜è¯»å†™é‡ = 2 * N * 4 = 8N å­—èŠ‚ï¼ˆåªè¯»ä¸€æ¬¡ï¼Œå†™ä¸€æ¬¡ï¼‰

# å¸¦å®½èŠ‚çœ = (40N - 8N) / 40N = 80%
```

#### 3.2.2 å‡å°‘ Kernel Launch å¼€é”€

```python
# Kernel Launch å¼€é”€ï¼ˆå…¸å‹å€¼ï¼‰
# - CPU ç«¯è°ƒåº¦ï¼š~5-10 Î¼s
# - GPU ç«¯å¯åŠ¨ï¼š~2-5 Î¼s
# - æ€»è®¡ï¼š~7-15 Î¼s / æ¬¡

# 5 ä¸ªç‹¬ç«‹ Kernelï¼š5 * 10 = 50 Î¼s å¼€é”€
# 1 ä¸ªèåˆ Kernelï¼š1 * 10 = 10 Î¼s å¼€é”€

# å¯¹äºå°å¼ é‡ï¼ˆæ‰§è¡Œæ—¶é—´ < 50Î¼sï¼‰ï¼Œlaunch å¼€é”€å¯èƒ½å ä¸»å¯¼ï¼
```

#### 3.2.3 æé«˜ç¼“å­˜åˆ©ç”¨ç‡

```python
# èåˆåï¼Œæ•°æ®ä¿æŒåœ¨ï¼š
# 1. å¯„å­˜å™¨ï¼ˆæœ€å¿«ï¼Œ~0 å»¶è¿Ÿï¼‰
# 2. L1 Cacheï¼ˆ~20 cyclesï¼‰
# 3. L2 Cacheï¼ˆ~200 cyclesï¼‰

# è€Œéï¼š
# å…¨å±€æ˜¾å­˜ï¼ˆ~400-800 cyclesï¼‰
```

### 3.3 TorchInductor çš„ Fusion ç­–ç•¥

#### 3.3.1 Pointwise Fusion

```python
# è¿ç»­çš„ Pointwise æ“ä½œä¼šè¢«è‡ªåŠ¨èåˆ
x = input
x = x * scale      # â”
x = x + bias       # â”‚ è¿™äº›ä¼šè¢«èåˆæˆä¸€ä¸ª Kernel
x = torch.relu(x)  # â”‚
x = x * 2.0        # â”‚
x = torch.tanh(x)  # â”˜
```

#### 3.3.2 Reduction + Pointwise Fusion

```python
# LayerNorm çš„å®ç°ä¼šèåˆï¼š
# 1. mean è®¡ç®—ï¼ˆreductionï¼‰
# 2. variance è®¡ç®—ï¼ˆreductionï¼‰
# 3. å½’ä¸€åŒ–ï¼ˆpointwiseï¼‰
# 4. scale + biasï¼ˆpointwiseï¼‰

x = F.layer_norm(x, normalized_shape)
# å†…éƒ¨ç”Ÿæˆ 1-2 ä¸ªèåˆçš„ Triton Kernel
```

#### 3.3.3 Fusion è¾¹ç•Œ

æŸäº›æ“ä½œä¼šé˜»æ­¢ fusionï¼š

```python
# ä¸èƒ½èåˆçš„æƒ…å†µ
x = pointwise_op1(x)
y = matmul(x, weight)    # â† æ‰“æ–­ fusionï¼ˆè°ƒç”¨ extern kernelï¼‰
y = pointwise_op2(y)

# ç»“æœï¼š
# Kernel 1: pointwise_op1
# Kernel 2: matmul (extern)
# Kernel 3: pointwise_op2
```

### 3.4 Fusion å¯è§†åŒ–

```
åŸå§‹è®¡ç®—å›¾ï¼š
â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”
â”‚ * â”‚ â†’ â”‚ + â”‚ â†’ â”‚reluâ”‚ â†’ â”‚ * â”‚ â†’ â”‚sigâ”‚
â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜
  â†“       â†“       â†“       â†“       â†“
[mem]   [mem]   [mem]   [mem]   [mem]

èåˆåï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  triton_poi_fused_mul_add_relu_...  â”‚
â”‚  * â†’ + â†’ relu â†’ * â†’ sigmoid         â”‚
â”‚  (å…¨éƒ¨åœ¨å¯„å­˜å™¨ä¸­å®Œæˆ)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
              [mem]
```

---

## 4. å®æˆ˜ï¼šè§‚å¯Ÿ Kernel Fusion æ•ˆæœ

### 4.1 åˆ›å»ºæµ‹è¯•æ¨¡å‹

```python
import torch
import torch.nn as nn

class PointwiseModel(nn.Module):
    """çº¯ Pointwise æ“ä½œæ¨¡å‹ - å±•ç¤º Kernel Fusion"""
    
    def __init__(self, dim=1024):
        super().__init__()
        self.scale1 = nn.Parameter(torch.randn(dim))
        self.bias1 = nn.Parameter(torch.randn(dim))
        self.scale2 = nn.Parameter(torch.randn(dim))
        self.bias2 = nn.Parameter(torch.randn(dim))
    
    def forward(self, x):
        # è¿™äº›æ“ä½œä¼šè¢«èåˆæˆ 1 ä¸ª Triton Kernel
        x = x * self.scale1 + self.bias1
        x = torch.relu(x)
        x = x * self.scale2 + self.bias2
        x = torch.sigmoid(x)
        x = x * 2.0 + 1.0
        x = torch.tanh(x)
        return x
```

### 4.2 ç¼–è¯‘å¹¶è§‚å¯Ÿ

```python
# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥æŸ¥çœ‹ç”Ÿæˆçš„ä»£ç 
import os
os.environ['TORCH_LOGS'] = 'output_code'

model = PointwiseModel().cuda()
compiled_model = torch.compile(model, backend="inductor")

# è¿è¡Œä»¥è§¦å‘ç¼–è¯‘
x = torch.randn(128, 1024, device='cuda')
y = compiled_model(x)
```

### 4.3 ç”Ÿæˆçš„ Triton Kernel

TorchInductor ä¼šç”Ÿæˆç±»ä¼¼è¿™æ ·çš„èåˆ Kernelï¼š

```python
@triton.jit
def triton_poi_fused_add_mul_relu_sigmoid_tanh_0(
    in_ptr0,   # x
    in_ptr1,   # scale1
    in_ptr2,   # bias1
    in_ptr3,   # scale2
    in_ptr4,   # bias2
    out_ptr0,  # output
    xnumel,
    XBLOCK: tl.constexpr
):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    
    # åŠ è½½æ‰€æœ‰è¾“å…¥
    x = tl.load(in_ptr0 + xindex, xmask)
    scale1 = tl.load(in_ptr1 + (xindex % 1024), xmask)
    bias1 = tl.load(in_ptr2 + (xindex % 1024), xmask)
    scale2 = tl.load(in_ptr3 + (xindex % 1024), xmask)
    bias2 = tl.load(in_ptr4 + (xindex % 1024), xmask)
    
    # æ‰€æœ‰è®¡ç®—åœ¨å¯„å­˜å™¨ä¸­å®Œæˆ
    tmp0 = x * scale1 + bias1
    tmp1 = tl.maximum(tmp0, 0)  # relu
    tmp2 = tmp1 * scale2 + bias2
    tmp3 = tl.sigmoid(tmp2)
    tmp4 = tmp3 * 2.0 + 1.0
    tmp5 = tl.libdevice.tanh(tmp4)
    
    # åªå†™ä¸€æ¬¡
    tl.store(out_ptr0 + xindex, tmp5, xmask)
```

### 4.4 æ€§èƒ½å¯¹æ¯”

```python
import time

def benchmark(fn, x, warmup=50, runs=100):
    # Warmup
    for _ in range(warmup):
        fn(x)
    torch.cuda.synchronize()
    
    # Benchmark
    start = time.time()
    for _ in range(runs):
        fn(x)
    torch.cuda.synchronize()
    
    return (time.time() - start) / runs * 1000  # ms

# æµ‹è¯•
x = torch.randn(1024, 1024, device='cuda')

eager_time = benchmark(model, x)
compiled_time = benchmark(compiled_model, x)

print(f"Eager:    {eager_time:.3f} ms")
print(f"Compiled: {compiled_time:.3f} ms")
print(f"Speedup:  {eager_time / compiled_time:.2f}x")
```

å…¸å‹ç»“æœï¼š

```
Eager:    0.245 ms
Compiled: 0.052 ms
Speedup:  4.71x
```

---

## 5. ä¼˜åŒ–å»ºè®®

### 5.1 æœ€å¤§åŒ– Fusion æ•ˆæœ

```python
# âœ… å¥½çš„æ¨¡å¼ï¼šè¿ç»­çš„ Pointwise æ“ä½œ
def good_forward(x, scale, bias):
    x = x * scale + bias
    x = torch.relu(x)
    x = x * 0.5 + 0.5
    return torch.sigmoid(x)

# âŒ é¿å…æ‰“æ–­ Fusion
def bad_forward(x, weight, scale, bias):
    x = x * scale + bias
    x = torch.relu(x)
    x = x @ weight        # â† Matmul æ‰“æ–­äº† Fusion
    x = x * 0.5 + 0.5
    return torch.sigmoid(x)
```

### 5.2 é€‰æ‹©åˆé€‚çš„æ¿€æ´»å‡½æ•°

æ‰€æœ‰æ¿€æ´»å‡½æ•°éƒ½æ˜¯ Pointwiseï¼Œä½†å¤æ‚åº¦ä¸åŒï¼š

```python
# è®¡ç®—å¤æ‚åº¦ï¼ˆç›¸å¯¹ï¼‰
torch.relu(x)      # 1x  - æœ€ç®€å•
torch.sigmoid(x)   # 3x  - exp è®¡ç®—
torch.tanh(x)      # 3x  - exp è®¡ç®—
torch.gelu(x)      # 5x  - åŒ…å« erf
torch.silu(x)      # 4x  - x * sigmoid(x)
```

### 5.3 ä½¿ç”¨ torch.compile çš„é«˜çº§é€‰é¡¹

```python
# æ›´æ¿€è¿›çš„ä¼˜åŒ–
compiled_model = torch.compile(
    model,
    backend="inductor",
    mode="max-autotune",  # å°è¯•æ›´å¤šé…ç½®
    fullgraph=True,       # å¼ºåˆ¶æ•´å›¾ç¼–è¯‘
)

# å‡å°‘ç¼–è¯‘æ—¶é—´ï¼ˆç‰ºç‰²éƒ¨åˆ†æ€§èƒ½ï¼‰
compiled_model = torch.compile(
    model,
    backend="inductor",
    mode="reduce-overhead",  # å‡å°‘ launch å¼€é”€
)
```

### 5.4 æ¨¡å‹è®¾è®¡å»ºè®®

```python
class OptimizedBlock(nn.Module):
    """é’ˆå¯¹ Fusion ä¼˜åŒ–çš„æ¨¡å—è®¾è®¡"""
    
    def __init__(self, dim):
        super().__init__()
        self.linear = nn.Linear(dim, dim)
        self.scale = nn.Parameter(torch.ones(dim))
        self.bias = nn.Parameter(torch.zeros(dim))
    
    def forward(self, x):
        # Pointwise æ“ä½œç»„ 1ï¼ˆä¼šè¢«èåˆï¼‰
        residual = x * self.scale + self.bias
        
        # Matmulï¼ˆå•ç‹¬çš„ kernelï¼‰
        x = self.linear(x)
        
        # Pointwise æ“ä½œç»„ 2ï¼ˆä¼šè¢«èåˆï¼‰
        x = torch.gelu(x)
        x = x + residual  # æ®‹å·®è¿æ¥
        
        return x
```

---

## 6. å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆæˆ‘çš„æ¨¡å‹ç¼–è¯‘ååè€Œå˜æ…¢äº†ï¼Ÿ

**å¯èƒ½åŸå› **ï¼š

1. **æ¨¡å‹å¤ªå°** - ç¼–è¯‘å¼€é”€ > ä¼˜åŒ–æ”¶ç›Š
2. **ä¸»è¦æ˜¯ Matmul** - æ²¡æœ‰ Pointwise å¯ä»¥ä¼˜åŒ–
3. **æµ‹é‡åŒ…å«äº†ç¼–è¯‘æ—¶é—´** - éœ€è¦é¢„çƒ­

```python
# æ­£ç¡®çš„æµ‹é‡æ–¹å¼
compiled_model = torch.compile(model)

# 1. é¢„çƒ­ï¼ˆè§¦å‘ç¼–è¯‘ï¼‰
for _ in range(10):
    _ = compiled_model(x)
torch.cuda.synchronize()

# 2. ç„¶åå†æµ‹é‡
start = time.time()
...
```

### Q2: å¦‚ä½•åˆ¤æ–­æ˜¯å¦ç”Ÿæˆäº† Triton Kernelï¼Ÿ

```python
# æ–¹æ³• 1ï¼šè®¾ç½®ç¯å¢ƒå˜é‡
os.environ['TORCH_LOGS'] = 'output_code'

# æ–¹æ³• 2ï¼šä½¿ç”¨ CUDA profiler
# nsys profile python script.py

# æ–¹æ³• 3ï¼šæ£€æŸ¥ç¼“å­˜ç›®å½•
# ls /tmp/torchinductor_*/
```

### Q3: Linear å±‚èƒ½å¦ç”¨ Triton å®ç°ï¼Ÿ

å¯ä»¥ï¼Œä½†**ä¸æ¨è**ï¼š

```python
# å¼ºåˆ¶ä½¿ç”¨ Triton matmulï¼ˆé€šå¸¸æ›´æ…¢ï¼‰
import torch._inductor.config as config
config.triton.mm = "triton"  # é»˜è®¤æ˜¯ "aten"
```

### Q4: å“ªäº›æ“ä½œä¼šæ‰“æ–­ Fusionï¼Ÿ

```python
# ä¼šæ‰“æ–­ Fusion çš„æ“ä½œï¼š
torch.matmul(x, w)       # Matmul
F.conv2d(x, w)           # å·ç§¯
x.view(-1)               # æŸäº› reshape
x[idx]                   # å¤æ‚çš„ç´¢å¼•
torch.sort(x)            # æ’åº
torch.unique(x)          # å»é‡
```

### Q5: å¦‚ä½•æŸ¥çœ‹ Fusion åçš„ Kernel åç§°ï¼Ÿ

Kernel åç§°åŒ…å«äº†èåˆçš„æ“ä½œï¼š

```
triton_poi_fused_add_mul_relu_sigmoid_tanh_0
       â”‚     â”‚     â”‚   â”‚    â”‚       â”‚     â”‚
       â”‚     â”‚     â”‚   â”‚    â”‚       â”‚     â””â”€ åºå·
       â”‚     â”‚     â”‚   â”‚    â”‚       â””â”€ tanh
       â”‚     â”‚     â”‚   â”‚    â””â”€ sigmoid  
       â”‚     â”‚     â”‚   â””â”€ relu
       â”‚     â”‚     â””â”€ mul (ä¹˜æ³•)
       â”‚     â””â”€ add (åŠ æ³•)
       â””â”€ poi = pointwise
```

---

## 7. æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **Pointwise æ“ä½œ** = é€å…ƒç´ ç‹¬ç«‹è®¡ç®—ï¼Œæ— æ•°æ®ä¾èµ–
2. **TorchInductor** å¯¹ Pointwise ç”Ÿæˆ Triton Kernelï¼Œå¯¹ Matmul è°ƒç”¨ cuBLAS
3. **Kernel Fusion** æ˜¯ `torch.compile` æ€§èƒ½æå‡çš„å…³é”®
4. **Fusion æ”¶ç›Š**ï¼šå‡å°‘å†…å­˜è®¿é—®ã€å‡å°‘ launch å¼€é”€ã€æé«˜ç¼“å­˜åˆ©ç”¨

### è®°å¿†å£è¯€

```
Pointwise ç”¨ Tritonï¼ŒMatmul ç”¨ cuBLASï¼›
è¿ç»­ Pointwise è‡ªåŠ¨èåˆï¼Œä¸­é—´ç»“æœä¸å†™å›ï¼›
æƒ³è¦åŠ é€Ÿçœ‹ Fusionï¼Œæ¨¡å‹è®¾è®¡è¦é…åˆã€‚
```

### æ¨èé˜…è¯»

- [PyTorch å®˜æ–¹æ–‡æ¡£ï¼štorch.compile](https://pytorch.org/docs/stable/torch.compiler.html)
- [Triton å®˜æ–¹æ•™ç¨‹](https://triton-lang.org/main/getting-started/tutorials/)
- [TorchInductor è®¾è®¡æ–‡æ¡£](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747)

---

## é™„å½• Aï¼šå®Œæ•´ç¤ºä¾‹ä»£ç 

```python
#!/usr/bin/env python3
"""
Pointwise æ“ä½œä¸ Kernel Fusion ç¤ºä¾‹
æ¼”ç¤º TorchInductor å¦‚ä½•èåˆ Pointwise æ“ä½œ
"""

import os
import time
import torch
import torch.nn as nn

# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥æŸ¥çœ‹ç”Ÿæˆçš„ä»£ç ï¼ˆå¯é€‰ï¼‰
# os.environ['TORCH_LOGS'] = 'output_code'


class PointwiseModel(nn.Module):
    """çº¯ Pointwise æ“ä½œæ¨¡å‹"""
    
    def __init__(self, dim=1024):
        super().__init__()
        self.scale1 = nn.Parameter(torch.randn(dim))
        self.bias1 = nn.Parameter(torch.randn(dim))
        self.scale2 = nn.Parameter(torch.randn(dim))
        self.bias2 = nn.Parameter(torch.randn(dim))
    
    def forward(self, x):
        x = x * self.scale1 + self.bias1
        x = torch.relu(x)
        x = x * self.scale2 + self.bias2
        x = torch.sigmoid(x)
        x = x * 2.0 + 1.0
        x = torch.tanh(x)
        return x


class MixedModel(nn.Module):
    """æ··åˆæ¨¡å‹ï¼šLinear + Pointwise"""
    
    def __init__(self, dim=512):
        super().__init__()
        self.linear1 = nn.Linear(dim, dim)
        self.linear2 = nn.Linear(dim, dim)
        self.scale = nn.Parameter(torch.randn(dim))
        self.bias = nn.Parameter(torch.randn(dim))
    
    def forward(self, x):
        # Linear (extern kernel - cuBLAS)
        x = self.linear1(x)
        
        # Pointwise (Triton kernel - ä¼šè¢«èåˆ)
        x = x * self.scale + self.bias
        x = torch.relu(x)
        x = x * 0.5 + 0.5
        x = torch.gelu(x)
        
        # Another Linear
        x = self.linear2(x)
        
        # More Pointwise
        x = torch.sigmoid(x)
        
        return x


def benchmark(fn, x, warmup=50, runs=100):
    """æ€§èƒ½æµ‹è¯•"""
    # Warmup
    for _ in range(warmup):
        fn(x)
    torch.cuda.synchronize()
    
    # Benchmark
    start = time.time()
    for _ in range(runs):
        fn(x)
    torch.cuda.synchronize()
    
    return (time.time() - start) / runs * 1000


def main():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Device: {device}")
    
    # æµ‹è¯• Pointwise æ¨¡å‹
    print("\n" + "="*60)
    print("Test 1: Pure Pointwise Model")
    print("="*60)
    
    model1 = PointwiseModel(1024).to(device)
    compiled1 = torch.compile(model1, backend="inductor")
    
    x1 = torch.randn(256, 1024, device=device)
    
    eager_time1 = benchmark(model1, x1)
    compiled_time1 = benchmark(compiled1, x1)
    
    print(f"Eager:    {eager_time1:.3f} ms")
    print(f"Compiled: {compiled_time1:.3f} ms")
    print(f"Speedup:  {eager_time1 / compiled_time1:.2f}x")
    
    # æµ‹è¯•æ··åˆæ¨¡å‹
    print("\n" + "="*60)
    print("Test 2: Mixed Model (Linear + Pointwise)")
    print("="*60)
    
    model2 = MixedModel(512).to(device)
    compiled2 = torch.compile(model2, backend="inductor")
    
    x2 = torch.randn(256, 512, device=device)
    
    eager_time2 = benchmark(model2, x2)
    compiled_time2 = benchmark(compiled2, x2)
    
    print(f"Eager:    {eager_time2:.3f} ms")
    print(f"Compiled: {compiled_time2:.3f} ms")
    print(f"Speedup:  {eager_time2 / compiled_time2:.2f}x")
    
    # éªŒè¯æ­£ç¡®æ€§
    print("\n" + "="*60)
    print("Correctness Verification")
    print("="*60)
    
    with torch.no_grad():
        y_eager = model1(x1)
        y_compiled = compiled1(x1)
        max_diff = (y_eager - y_compiled).abs().max().item()
        print(f"Max difference: {max_diff:.2e}")
        print(f"âœ“ Results match" if max_diff < 1e-5 else "âœ— Results differ")


if __name__ == "__main__":
    main()
```

---

## é™„å½• Bï¼šKernel Fusion å†³ç­–æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TorchInductor ç¼–è¯‘æµç¨‹                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FX Graph åˆ†æ                          â”‚
â”‚  è¯†åˆ«æ“ä½œç±»å‹ï¼šPointwise / Reduction / Matmul / å…¶ä»–        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼               â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Pointwise      â”‚ â”‚   Reduction   â”‚ â”‚     Matmul        â”‚
â”‚  (element-wise)   â”‚ â”‚  (sum, mean)  â”‚ â”‚  (mm, linear)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                    â”‚                    â”‚
         â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å°è¯•ä¸ç›¸é‚»çš„     â”‚ â”‚  æ£€æŸ¥æ˜¯å¦å¯ä¸ â”‚ â”‚  è°ƒç”¨ extern      â”‚
â”‚  Pointwise èåˆ   â”‚ â”‚  Pointwise    â”‚ â”‚  kernel (cuBLAS)  â”‚
â”‚                   â”‚ â”‚  èåˆ         â”‚ â”‚                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                    â”‚                    â”‚
         â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç”Ÿæˆèåˆçš„       â”‚ â”‚  ç”Ÿæˆèåˆçš„   â”‚ â”‚  ç”Ÿæˆ extern      â”‚
â”‚  Triton Kernel    â”‚ â”‚  Triton       â”‚ â”‚  è°ƒç”¨ä»£ç          â”‚
â”‚  triton_poi_...   â”‚ â”‚  Kernel       â”‚ â”‚  aten.mm.default  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æœ€ç»ˆç”Ÿæˆçš„ä»£ç                             â”‚
â”‚  = è‹¥å¹² Triton Kernel + è‹¥å¹² extern kernel è°ƒç”¨             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

