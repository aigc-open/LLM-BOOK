# ç¬¬äº”ç« ï¼šTorch Dynamo åŠ¨æ€å›¾ç¼–è¯‘

## ğŸ¯ æœ¬ç« ç›®æ ‡

- æ·±å…¥ç†è§£ Dynamo çš„æ ¸å¿ƒä½œç”¨
- æŒæ¡å­—èŠ‚ç æ•è·æŠ€æœ¯
- ç†è§£ Guards æœºåˆ¶
- å­¦ä¹  Dynamo å¦‚ä½•å®ç°åŠ¨æ€å›¾åˆ°é™æ€å›¾çš„è½¬æ¢

## 1. Torch Dynamo æ˜¯ä»€ä¹ˆï¼Ÿ

### 1.1 ç®€å•ç†è§£

**é—®é¢˜ï¼šå¦‚ä½•æ•è· Python åŠ¨æ€å›¾ï¼Ÿ**

```python
# Python ä»£ç å¾ˆçµæ´»
def model_forward(x, use_dropout=True):
    x = self.linear1(x)
    if use_dropout:                    # åŠ¨æ€æ§åˆ¶æµ
        x = self.dropout(x)
    x = torch.relu(x)
    return x

# å¦‚ä½•æŠŠè¿™ä¸ªè½¬æˆé™æ€å›¾ä¼˜åŒ–ï¼Ÿ
# TorchScriptï¼šè¦æ±‚ç”¨æˆ·æ”¹ä»£ç  âŒ
# Dynamoï¼šè‡ªåŠ¨æ•è·ï¼Œæ— éœ€æ”¹ä»£ç  âœ…
```

**Dynamo çš„magicï¼š**
```python
# ç”¨æˆ·åªéœ€ä¸€è¡Œ
compiled_fn = torch.compile(model_forward)

# Dynamo è‡ªåŠ¨ï¼š
# 1. æ‹¦æˆª Python å­—èŠ‚ç 
# 2. è¯†åˆ« PyTorch æ“ä½œ
# 3. æ„å»ºè®¡ç®—å›¾
# 4. äº¤ç»™åç«¯ä¼˜åŒ–
```

### 1.2 Dynamo åœ¨ torch.compile ä¸­çš„ä½ç½®

```
torch.compile(model)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Torch Dynamo (æœ¬ç« é‡ç‚¹)             â”‚
â”‚  - æ‹¦æˆª Python å­—èŠ‚ç                 â”‚
â”‚  - æå– PyTorch æ“ä½œ                 â”‚
â”‚  - æ„å»º FX å›¾                        â”‚
â”‚  - å¤„ç†æ§åˆ¶æµå’ŒåŠ¨æ€æ€§                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
         [FX Graph]
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AOTAutograd (ä¸‹ä¸€ç« )                â”‚
â”‚  - ç”Ÿæˆå‰å‘/åå‘å›¾                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Backend Compiler                    â”‚
â”‚  - TorchInductor                     â”‚
â”‚  - ç”Ÿæˆä¼˜åŒ–ä»£ç                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 2. å­—èŠ‚ç æ•è·ï¼šDynamo çš„æ ¸å¿ƒæŠ€æœ¯

### 2.1 Python å­—èŠ‚ç åŸºç¡€

Python ä»£ç ä¼šè¢«ç¼–è¯‘æˆå­—èŠ‚ç ï¼š

```python
import dis

def simple_add(a, b):
    return a + b

# æŸ¥çœ‹å­—èŠ‚ç 
dis.dis(simple_add)
```

**è¾“å‡ºï¼š**
```
  2           0 LOAD_FAST                0 (a)
              2 LOAD_FAST                1 (b)
              4 BINARY_ADD
              6 RETURN_VALUE
```

### 2.2 Dynamo å¦‚ä½•æ‹¦æˆªå­—èŠ‚ç 

**æ­£å¸¸æ‰§è¡Œæµç¨‹ï¼š**
```
Python æºç  â†’ å­—èŠ‚ç  â†’ Python è§£é‡Šå™¨æ‰§è¡Œ
```

**Dynamo æ‹¦æˆªï¼š**
```
Python æºç  â†’ å­—èŠ‚ç  â†’ Dynamo Frame Evaluation Hook â†’ 
    â†“
åˆ†æ PyTorch æ“ä½œ â†’ æ„å»º FX å›¾ â†’ ç¼–è¯‘ä¼˜åŒ–
    â†“
æ‰§è¡Œä¼˜åŒ–åçš„ä»£ç 
```

### 2.3 å®é™…ç¤ºä¾‹

```python
import torch

def example_function(x, y):
    a = x + y           # PyTorch æ“ä½œ
    b = a * 2           # PyTorch æ“ä½œ
    c = torch.sin(b)    # PyTorch æ“ä½œ
    return c

# ç¼–è¯‘
compiled_fn = torch.compile(example_function)

# ç¬¬ä¸€æ¬¡æ‰§è¡Œæ—¶ï¼ŒDynamo ä¼šï¼š
x = torch.randn(10)
y = torch.randn(10)
result = compiled_fn(x, y)  # è§¦å‘æ•è·å’Œç¼–è¯‘

# æŸ¥çœ‹ Dynamo æ•è·çš„å›¾
import torch._dynamo as dynamo
explanation = dynamo.explain(example_function)(x, y)
print(explanation)
```

## 3. Guards æœºåˆ¶

### 3.1 ä»€ä¹ˆæ˜¯ Guardsï¼Ÿ

**é—®é¢˜ï¼šåŠ¨æ€ Python ä»£ç çš„æŒ‘æˆ˜**

```python
def dynamic_fn(x, threshold):
    if x.sum() > threshold:   # æ•°æ®ä¾èµ–çš„åˆ†æ”¯
        return x * 2
    else:
        return x + 1

# ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼šx.sum() = 10, threshold = 5 â†’ èµ°ç¬¬ä¸€ä¸ªåˆ†æ”¯
# ç¬¬äºŒæ¬¡è°ƒç”¨ï¼šx.sum() = 2,  threshold = 5 â†’ èµ°ç¬¬äºŒä¸ªåˆ†æ”¯
# å¦‚ä½•å¤„ç†ï¼Ÿ
```

**Guards = å‡è®¾çš„æ¡ä»¶æ£€æŸ¥**

Dynamo ä¼šç”Ÿæˆç±»ä¼¼è¿™æ ·çš„ä»£ç ï¼š

```python
# Guard: æ£€æŸ¥æ˜¯å¦èµ°ç¬¬ä¸€ä¸ªåˆ†æ”¯
if x.sum() > threshold:  # âœ… Guard é€šè¿‡
    return optimized_branch1(x)
else:  # âŒ Guard å¤±è´¥ï¼Œé‡æ–°ç¼–è¯‘
    return optimized_branch2(x)
```

### 3.2 Guards çš„ç±»å‹

**1. Shape Guardsï¼ˆå½¢çŠ¶æ£€æŸ¥ï¼‰**

```python
def shape_dependent(x):
    if x.shape[0] > 100:
        return x.sum()
    return x.mean()

# Dynamo ç”Ÿæˆ Guard:
# if x.shape[0] == 128:  # å‡è®¾ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶ batch_size=128
#     return compiled_version_for_128(x)
# else:
#     recompile()
```

**2. Type Guardsï¼ˆç±»å‹æ£€æŸ¥ï¼‰**

```python
# Guard: æ£€æŸ¥ x æ˜¯å¦æ˜¯ Tensor
assert isinstance(x, torch.Tensor)

# Guard: æ£€æŸ¥ dtype
assert x.dtype == torch.float32
```

**3. Value Guardsï¼ˆå€¼æ£€æŸ¥ï¼‰**

```python
def value_dependent(x, mode):
    if mode == "train":
        return dropout(x)
    else:
        return x

# Dynamo Guard:
# if mode == "train":  # è®°ä½ç¬¬ä¸€æ¬¡çš„å€¼
#     return compiled_train_version(x)
```

### 3.3 Guards å¤±è´¥å’Œé‡æ–°ç¼–è¯‘

```python
import torch

def example(x):
    return x + 1

compiled_example = torch.compile(example)

# ç¬¬ä¸€æ¬¡ï¼šshape = (10,)
x1 = torch.randn(10)
compiled_example(x1)  # ç¼–è¯‘ï¼Œç”Ÿæˆ Guard: shape == (10,)

# ç¬¬äºŒæ¬¡ï¼šshape = (10,)
x2 = torch.randn(10)
compiled_example(x2)  # Guard é€šè¿‡ï¼Œä½¿ç”¨ç¼“å­˜çš„ç¼–è¯‘ç»“æœ

# ç¬¬ä¸‰æ¬¡ï¼šshape = (20,) âš ï¸
x3 = torch.randn(20)
compiled_example(x3)  # Guard å¤±è´¥ï¼Œé‡æ–°ç¼–è¯‘ï¼
```

**æŸ¥çœ‹ Guardsï¼š**

```python
import torch._dynamo as dynamo

# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
torch._dynamo.config.log_level = logging.INFO
torch._dynamo.config.verbose = True

compiled_fn = torch.compile(example)
compiled_fn(torch.randn(10))

# å¯ä»¥çœ‹åˆ°ç”Ÿæˆçš„ Guards
```

## 4. Graph Breaksï¼ˆå›¾ä¸­æ–­ï¼‰

### 4.1 ä»€ä¹ˆæ˜¯ Graph Breakï¼Ÿ

æœ‰äº› Python ä»£ç æ— æ³•è¢«æ•è·åˆ°å›¾ä¸­ï¼š

```python
def example_with_break(x):
    a = x + 1              # âœ… å¯ä»¥æ•è·
    b = a * 2              # âœ… å¯ä»¥æ•è·
    
    print(b.shape)         # âŒ Graph Breakï¼print æ— æ³•æ•è·
    
    c = b - 1              # âœ… å¯ä»¥æ•è·ï¼ˆæ–°çš„å›¾ï¼‰
    return c
```

**Dynamo ä¼šåˆ†æˆå¤šä¸ªå›¾ï¼š**

```
å›¾ 1: x â†’ +1 â†’ *2 â†’ b
â†“ (Graph Break)
Python æ‰§è¡Œ: print(b.shape)
â†“
å›¾ 2: b â†’ -1 â†’ c
```

### 4.2 å¸¸è§çš„ Graph Breaks

```python
# 1. print è¯­å¥
print(x)  # âŒ

# 2. è®¿é—®å…¨å±€å˜é‡ï¼ˆæŸäº›æƒ…å†µï¼‰
global_var = []
global_var.append(x)  # âŒ

# 3. ä¸æ”¯æŒçš„åº“
import numpy as np
np_array = x.numpy()  # âŒ

# 4. åŠ¨æ€å±æ€§è®¿é—®
attr_name = "weight"
value = getattr(model, attr_name)  # âŒ (æŸäº›æƒ…å†µ)

# 5. å¤æ‚çš„æ§åˆ¶æµ
for i in range(x.item()):  # âŒ x.item() éœ€è¦å®é™…æ‰§è¡Œ
    ...
```

### 4.3 æŸ¥çœ‹ Graph Breaks

```python
import torch._dynamo as dynamo

def function_with_breaks(x):
    a = x + 1
    print("Debug:", a.shape)  # Graph break
    b = a * 2
    return b

# æŸ¥çœ‹ graph breaks
explanation = dynamo.explain(function_with_breaks)(torch.randn(10))
print(explanation)
```

**è¾“å‡ºç¤ºä¾‹ï¼š**
```
Dynamo produced 2 graphs:
  Graph 1: 1 node
    LOAD_FAST, LOAD_CONST, BINARY_ADD
  [Graph Break: print]
  Graph 2: 1 node
    LOAD_FAST, LOAD_CONST, BINARY_MULTIPLY
```

### 4.4 å‡å°‘ Graph Breaks

```python
# âŒ ä¸å¥½ï¼šé¢‘ç¹ Graph Break
def bad_example(x):
    a = x + 1
    print(a.shape)     # Break 1
    b = a * 2
    print(b.shape)     # Break 2
    c = b - 1
    return c

# âœ… å¥½ï¼šé¿å…ä¸­é—´çš„ print
def good_example(x):
    a = x + 1
    b = a * 2
    c = b - 1
    return c
    # æœ€åå† print è°ƒè¯•ä¿¡æ¯
```

## 5. æ§åˆ¶æµå¤„ç†

### 5.1 ç®€å•çš„æ¡ä»¶è¯­å¥

```python
def conditional_example(x, flag: bool):
    if flag:
        return x * 2
    else:
        return x + 1

# Dynamo å¤„ç†æ–¹å¼ï¼š
# 1. ç¬¬ä¸€æ¬¡è°ƒç”¨ flag=Trueï¼Œç¼–è¯‘ç¬¬ä¸€ä¸ªåˆ†æ”¯
# 2. æ·»åŠ  Guard: if flag == True
# 3. å¦‚æœåç»­ flag=Falseï¼ŒGuard å¤±è´¥ï¼Œé‡æ–°ç¼–è¯‘
```

### 5.2 æ•°æ®ä¾èµ–çš„æ§åˆ¶æµ

```python
def data_dependent(x):
    if x.sum() > 0:  # ä¾èµ–æ•°æ®çš„å€¼
        return x * 2
    else:
        return x - 1

# Dynamo çš„æŒ‘æˆ˜ï¼š
# - x.sum() éœ€è¦å®é™…è®¡ç®—
# - å¯èƒ½å¯¼è‡´ Graph Break æˆ–è€…é‡æ–°ç¼–è¯‘
```

**Dynamo çš„ç­–ç•¥ï¼š**

1. **Specializeï¼ˆç‰¹åŒ–ï¼‰ï¼š** ä¸ºæ¯ä¸ªåˆ†æ”¯ç¼–è¯‘ä¸åŒçš„ç‰ˆæœ¬
2. **Guardï¼š** è®°ä½æ¡ä»¶ï¼Œå½“æ¡ä»¶å˜åŒ–æ—¶é‡æ–°ç¼–è¯‘

### 5.3 å¾ªç¯å¤„ç†

```python
# âœ… å›ºå®šæ¬¡æ•°çš„å¾ªç¯
def fixed_loop(x):
    for i in range(10):  # å¯ä»¥å±•å¼€
        x = x + 1
    return x

# âš ï¸ åŠ¨æ€æ¬¡æ•°çš„å¾ªç¯
def dynamic_loop(x, n):
    for i in range(n):  # éš¾ä»¥å¤„ç†
        x = x + 1
    return x

# âŒ æ•°æ®ä¾èµ–çš„å¾ªç¯
def data_loop(x):
    while x.sum() > 0:  # æéš¾å¤„ç†
        x = x - 0.1
    return x
```

## 6. Dynamo çš„é…ç½®å’Œè°ƒè¯•

### 6.1 é…ç½®é€‰é¡¹

```python
import torch._dynamo as dynamo

# æŸ¥çœ‹é…ç½®
print(dynamo.config)

# å¸¸ç”¨é…ç½®
dynamo.config.verbose = True           # è¯¦ç»†æ—¥å¿—
dynamo.config.suppress_errors = False  # ä¸æŠ‘åˆ¶é”™è¯¯
dynamo.config.log_level = logging.DEBUG

# ç¼“å­˜å¤§å°
dynamo.config.cache_size_limit = 64    # æœ€å¤šç¼“å­˜ 64 ä¸ªç¼–è¯‘ç‰ˆæœ¬
```

### 6.2 è°ƒè¯•å·¥å…·

```python
import torch._dynamo as dynamo

# 1. explain: æŸ¥çœ‹ç¼–è¯‘ä¿¡æ¯
def my_function(x):
    return x + 1

explanation = dynamo.explain(my_function)(torch.randn(10))
print(explanation)

# 2. ç¦ç”¨ Dynamoï¼ˆå¯¹æ¯”æµ‹è¯•ï¼‰
@dynamo.disable
def no_compile_function(x):
    return x + 1

# 3. é‡ç½® Dynamo
dynamo.reset()  # æ¸…ç©ºæ‰€æœ‰ç¼“å­˜

# 4. æŸ¥çœ‹ç”Ÿæˆçš„ä»£ç 
compiled_fn = torch.compile(my_function)
print(compiled_fn.__code__)
```

### 6.3 å¸¸è§é—®é¢˜æ’æŸ¥

```python
# é—®é¢˜ 1ï¼šç¼–è¯‘é”™è¯¯
try:
    compiled_fn = torch.compile(problematic_function)
    compiled_fn(input_data)
except Exception as e:
    print(f"ç¼–è¯‘é”™è¯¯: {e}")
    # å°è¯•ç¦ç”¨ç¼–è¯‘è¿è¡Œ
    result = problematic_function(input_data)

# é—®é¢˜ 2ï¼šæ€§èƒ½åè€Œå˜æ…¢
# å¯èƒ½åŸå› ï¼š
# - å¤ªå¤š Graph Breaks
# - é¢‘ç¹é‡æ–°ç¼–è¯‘ï¼ˆGuards å¤±è´¥ï¼‰
# - é¦–æ¬¡ç¼–è¯‘å¼€é”€

# è§£å†³æ–¹æ¡ˆï¼š
# - å‡å°‘ Graph Breaks
# - å›ºå®šè¾“å…¥å½¢çŠ¶
# - é¢„çƒ­ï¼ˆwarm-upï¼‰
```

## 7. æºç åˆ†æ

### 7.1 æ ¸å¿ƒæ–‡ä»¶ä½ç½®

```
pytorch/torch/_dynamo/
â”œâ”€â”€ __init__.py          # å…¬å…± API
â”œâ”€â”€ eval_frame.py        # å­—èŠ‚ç æ‹¦æˆªæ ¸å¿ƒ
â”œâ”€â”€ symbolic_convert.py  # å­—èŠ‚ç è½¬ FX å›¾
â”œâ”€â”€ guards.py            # Guards ç”Ÿæˆå’Œæ£€æŸ¥
â”œâ”€â”€ bytecode_transformation.py  # å­—èŠ‚ç è½¬æ¢
â”œâ”€â”€ output_graph.py      # è¾“å‡ºå›¾ç®¡ç†
â””â”€â”€ variables/           # å˜é‡è¿½è¸ª
    â”œâ”€â”€ tensor.py        # Tensor å˜é‡
    â”œâ”€â”€ functions.py     # å‡½æ•°å˜é‡
    â””â”€â”€ ...
```

### 7.2 å…³é”®ä»£ç ç‰‡æ®µ

**å­—èŠ‚ç æ‹¦æˆªï¼ˆeval_frame.pyï¼‰ï¼š**

```python
# ç®€åŒ–ç‰ˆæœ¬
def _custom_eval_frame(frame, cache_size):
    """Dynamo çš„æ ¸å¿ƒï¼šæ‹¦æˆª Python frame æ‰§è¡Œ"""
    
    # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç¼–è¯‘è¿™ä¸ª frame
    if should_compile(frame):
        # è½¬æ¢å­—èŠ‚ç ä¸º FX å›¾
        graph = convert_frame_to_graph(frame)
        
        # ç¼–è¯‘å›¾
        compiled_fn = compile_graph(graph)
        
        # æ‰§è¡Œç¼–è¯‘åçš„ä»£ç 
        return compiled_fn(*frame.f_locals.values())
    else:
        # æ­£å¸¸æ‰§è¡Œ
        return DEFAULT_EVAL_FRAME(frame)
```

**Guards ç”Ÿæˆï¼ˆguards.pyï¼‰ï¼š**

```python
# ç®€åŒ–ç‰ˆæœ¬
class GuardBuilder:
    def __init__(self):
        self.guards = []
    
    def add_tensor_match_guard(self, tensor, name):
        """æ·»åŠ  Tensor åŒ¹é…çš„ Guard"""
        # æ£€æŸ¥å½¢çŠ¶
        self.guards.append(
            f"{name}.shape == {tensor.shape}"
        )
        # æ£€æŸ¥ dtype
        self.guards.append(
            f"{name}.dtype == {tensor.dtype}"
        )
        # æ£€æŸ¥è®¾å¤‡
        self.guards.append(
            f"{name}.device == {tensor.device}"
        )
```

### 7.3 æ‰§è¡Œæµç¨‹

```python
# æ‰§è¡Œæµç¨‹ï¼ˆä¼ªä»£ç ï¼‰

def torch_compile(fn):
    def wrapper(*args, **kwargs):
        # 1. æ£€æŸ¥ç¼“å­˜
        cache_key = compute_cache_key(args, kwargs)
        if cache_key in CACHE:
            compiled_fn = CACHE[cache_key]
            
            # 2. æ£€æŸ¥ Guards
            if check_guards(compiled_fn.guards, args, kwargs):
                # Guards é€šè¿‡ï¼Œä½¿ç”¨ç¼“å­˜
                return compiled_fn(*args, **kwargs)
        
        # 3. Guards å¤±è´¥æˆ–æ— ç¼“å­˜ï¼Œé‡æ–°ç¼–è¯‘
        graph = capture_graph(fn, args, kwargs)  # Dynamo æ•è·
        guards = extract_guards(graph)
        
        optimized_fn = backend_compile(graph)    # åç«¯ç¼–è¯‘
        optimized_fn.guards = guards
        
        # 4. ç¼“å­˜
        CACHE[cache_key] = optimized_fn
        
        # 5. æ‰§è¡Œ
        return optimized_fn(*args, **kwargs)
    
    return wrapper
```

## 8. Dynamo çš„ä¼˜åŠ¿ä¸é™åˆ¶

### 8.1 ä¼˜åŠ¿

âœ… **æ— ä¾µå…¥æ€§**
```python
# åªéœ€ä¸€è¡Œ
model = torch.compile(model)
# æ— éœ€ä¿®æ”¹æ¨¡å‹ä»£ç ï¼
```

âœ… **æ”¯æŒåŠ¨æ€ Python**
```python
# æ”¯æŒæ§åˆ¶æµ
if condition:
    x = branch1(x)
else:
    x = branch2(x)
```

âœ… **çµæ´»çš„åç«¯**
```python
# å¯ä»¥é€‰æ‹©ä¸åŒçš„ç¼–è¯‘åç«¯
model = torch.compile(model, backend="inductor")
model = torch.compile(model, backend="aot_eager")
```

### 8.2 é™åˆ¶

âŒ **é¦–æ¬¡ç¼–è¯‘å¼€é”€**
```python
# ç¬¬ä¸€æ¬¡è°ƒç”¨å¾ˆæ…¢ï¼ˆç¼–è¯‘ï¼‰
output = compiled_model(input)  # å¯èƒ½éœ€è¦å‡ ç§’

# åç»­è°ƒç”¨å¿«
output = compiled_model(input)  # æ¯«ç§’çº§
```

âŒ **Graph Breaks å½±å“æ€§èƒ½**
```python
# å¤ªå¤š Graph Breaks ä¼šé™ä½æ€§èƒ½
def many_breaks(x):
    x = x + 1
    print(x)      # Break
    x = x * 2
    print(x)      # Break
    x = x - 1
    return x
```

âŒ **åŠ¨æ€å½¢çŠ¶æ”¯æŒæœ‰é™**
```python
# å½¢çŠ¶å˜åŒ–å¯¼è‡´é‡æ–°ç¼–è¯‘
model(torch.randn(32, 128))  # ç¼–è¯‘ç‰ˆæœ¬ 1
model(torch.randn(64, 128))  # é‡æ–°ç¼–è¯‘ï¼
```

## 9. å®æˆ˜æŠ€å·§

### 9.1 é¢„çƒ­ç­–ç•¥

```python
model = torch.compile(model)

# é¢„çƒ­ï¼šè§¦å‘ç¼–è¯‘
dummy_input = torch.randn(batch_size, input_dim, device=device)
with torch.no_grad():
    _ = model(dummy_input)

# ä¹‹åæ­£å¸¸ä½¿ç”¨
for batch in dataloader:
    output = model(batch)  # å·²ç»ç¼–è¯‘å¥½ï¼Œå¾ˆå¿«
```

### 9.2 å›ºå®šè¾“å…¥å½¢çŠ¶

```python
# âŒ ä¸å¥½ï¼šåŠ¨æ€ batch size
for batch in dataloader:  # batch size å¯èƒ½å˜åŒ–
    output = model(batch)

# âœ… å¥½ï¼šå›ºå®š batch size
from torch.utils.data import DataLoader

dataloader = DataLoader(
    dataset, 
    batch_size=32,    # å›ºå®š
    drop_last=True    # ä¸¢å¼ƒæœ€åä¸å®Œæ•´çš„ batch
)
```

### 9.3 é€æ­¥ç¼–è¯‘

```python
# ä¸è¦ä¸€æ¬¡æ€§ç¼–è¯‘æ•´ä¸ªå¤æ‚ç³»ç»Ÿ
# å…ˆç¼–è¯‘æ€§èƒ½ç“¶é¢ˆéƒ¨åˆ†

class ComplexModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = Encoder()
        self.decoder = Decoder()
    
    def forward(self, x):
        # åªç¼–è¯‘ encoderï¼ˆå‡è®¾å®ƒæ˜¯ç“¶é¢ˆï¼‰
        encoded = torch.compile(self.encoder)(x)
        decoded = self.decoder(encoded)
        return decoded
```

## 10. å°ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **Dynamo = å­—èŠ‚ç æ‹¦æˆª + å›¾æ•è·å¼•æ“**
2. **æ ¸å¿ƒæŠ€æœ¯ï¼š**
   - æ‹¦æˆª Python frame execution
   - åˆ†æå­—èŠ‚ç ï¼Œæå– PyTorch æ“ä½œ
   - æ„å»º FX å›¾
   - ç”Ÿæˆ Guards
3. **Guards**ï¼šç¡®ä¿ç¼–è¯‘ç‰ˆæœ¬çš„æ­£ç¡®æ€§
4. **Graph Breaks**ï¼šæ— æ³•æ•è·çš„æ“ä½œä¼šä¸­æ–­å›¾
5. **åŠ¨æ€æ€§æ”¯æŒ**ï¼šé€šè¿‡ Guards å’Œé‡æ–°ç¼–è¯‘

### å…³é”®æ¦‚å¿µ

- **Bytecode Capture**ï¼šå­—èŠ‚ç æ•è·
- **Guards**ï¼šæ¡ä»¶æ£€æŸ¥
- **Graph Break**ï¼šå›¾ä¸­æ–­
- **Specialization**ï¼šç‰¹åŒ–ç¼–è¯‘
- **Recompilation**ï¼šé‡æ–°ç¼–è¯‘

### ä½¿ç”¨å»ºè®®

1. âœ… å‡å°‘ Graph Breaks
2. âœ… å›ºå®šè¾“å…¥å½¢çŠ¶
3. âœ… é¢„çƒ­æ¨¡å‹
4. âœ… ä»ç®€å•åœºæ™¯å¼€å§‹
5. âš ï¸ æ³¨æ„é¦–æ¬¡ç¼–è¯‘å¼€é”€

### ä¸‹ä¸€æ­¥

Dynamo æ•è·äº†è®¡ç®—å›¾åï¼Œæ¥ä¸‹æ¥éœ€è¦ç”Ÿæˆé«˜æ•ˆçš„å‰å‘å’Œåå‘ä¼ æ’­ï¼š

ğŸ“– [ç¬¬å…­ç« ï¼šAOTAutograd è‡ªåŠ¨å¾®åˆ†](./06_aotautograd.md) - æå‰ç¼–è¯‘è‡ªåŠ¨å¾®åˆ†

---

## ğŸ’¡ ç»ƒä¹ é¢˜

1. ä½¿ç”¨ `dynamo.explain()` åˆ†æä¸€ä¸ªå‡½æ•°çš„ç¼–è¯‘è¿‡ç¨‹
2. ç¼–å†™ä¸€ä¸ªæœ‰ Graph Break çš„å‡½æ•°ï¼Œè§‚å¯Ÿç”Ÿæˆçš„å¤šä¸ªå›¾
3. æµ‹è¯•ä¸åŒè¾“å…¥å½¢çŠ¶ä¸‹çš„é‡æ–°ç¼–è¯‘è¡Œä¸º

**ç»§ç»­å­¦ä¹ ** â†’ [AOTAutograd è‡ªåŠ¨å¾®åˆ†](./06_aotautograd.md)

