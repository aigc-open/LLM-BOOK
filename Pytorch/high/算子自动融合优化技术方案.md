# 针对多级存储芯片的算子自动融合优化技术方案

## 1. 什么是"算子自动融合"？

想象你在厨房做菜，需要完成三道工序：
1.  **洗菜**：从冰箱（L3）拿菜到水池洗，洗完放回冰箱
2.  **切菜**：从冰箱拿菜到砧板切，切完放回冰箱
3.  **炒菜**：从冰箱拿菜到锅里炒

**传统做法（无融合）**：
每道工序都要"拿→做→放回"，在厨房和冰箱之间来回跑三次，非常浪费时间。

**融合做法（本方案）**：
既然这三道工序连着做，为什么不一次性把菜拿到厨房，**洗→切→炒**一气呵成，最后再把成品放回去？这样只需要跑一次就够了！

在 AI 芯片中：
*   **算子**就像"洗菜"、"切菜"这样的工序
*   **冰箱（L3）**是慢速的外层存储
*   **砧板/灶台（L2）**是快速的内层存储
*   **融合**就是把多个连续的算子合并执行，让中间结果不用反复搬运

---

## 2. 为什么需要算子融合？（背景与痛点）

### 2.1 多级存储的现实
AI 芯片通常有三级存储：
*   **L3（外层存储）**：容量大（如 32 GB DDR），但速度慢（如 100 GB/s）
*   **L2（内层存储）**：容量小（如 16 MB），但速度快（如 1 TB/s）
*   **L1（寄存器）**：容量极小，速度极快

### 2.2 性能瓶颈
传统的执行方式：
```
算子A：L3 读取 → L2 计算 → L3 写回
算子B：L3 读取 → L2 计算 → L3 写回
算子C：L3 读取 → L2 计算 → L3 写回
```

问题在于：
*   算子 A 的输出是算子 B 的输入（中间结果）
*   这个中间结果**只是临时数据**，最终不会被使用
*   却要经历两次慢速的 L3 访问（A 写回 + B 读取）

### 2.3 目标
通过融合，让中间结果**驻留在 L2**：
```
融合算子{A, B, C}：L3 读取 → L2 中依次执行 A→B→C → L3 写回
```
只访问 L3 两次（首次输入 + 最终输出），大幅减少数据搬运。

---

## 3. 核心技术：怎么实现的？

### 3.1 与传统方案的区别

**传统方案（Pattern Match 图样匹配）**：
*   编译器预先定义好固定的融合模式，比如：
    *   `Conv + BatchNorm + ReLU` → 融合
    *   `MatMul + Add` → 融合
*   **缺点**：
    *   只能识别预定义的模式，遇到新模式就无能为力
    *   需要为每种模式手写融合算子代码（开发成本高）
    *   无法灵活应对各种神经网络结构

**本方案（自动融合引擎 AFE）**：
*   **不关心算子类型**（是 Conv 还是 MatMul？不管！）
*   **只关心数据依赖**（A 的输出是 B 的输入吗？是的话就试试融合！）
*   **自动化一切**：从寻找融合机会到生成执行代码，全自动完成

---

## 4. 详细技术方案

### 4.1 第一步：构建融合结构

#### (1) 筛选目标算子
编译器首先检查每个算子：
*   **存储条件**：算子的输入输出张量能否放入 L2？
*   **支持情况**：算子是否支持输入/输出在 L2 存储？
*   **标记候选**：满足条件的算子标记为"可融合候选"

#### (2) 算子分类（按入度/出度）
根据数据依赖关系，将算子分为四类：

*   **单入单出**：只有一个输入，只有一个输出（如 ReLU）
*   **单入多出**：一个输入，多个输出（如 Split）
*   **多入单出**：多个输入，一个输出（如 Add、Concat）
*   **多入多出**：多个输入，多个输出（如 MultiheadAttention）

**注意**：入度只考虑**非常量输入**（常量如权重张量，已经预加载）

#### (3) 前向搜索（贪心融合）
从每个候选算子出发，**向前**搜索可融合的算子：

```
当前算子（锚点）
    ↑
搜索 单入单出 或 多入单出 的前驱算子
    ↑
尝试融合
```

融合策略：
*   先尝试融合 2 个算子（小范围）
*   如果成功，尝试融合 3 个算子（扩大范围）
*   持续扩大，直到无法再融合为止

#### (4) 后向搜索（补充遗漏）
从 `单入多出` 或 `多入多出` 的算子出发，**向后**搜索：

```
当前算子（锚点）
    ↓
搜索 单入单出 或 单入多出 的后继算子
    ↓
尝试融合
```

这一步是为了捕获前向搜索中遗漏的融合机会。

#### (5) 内存峰值检查（融合判定条件）
尝试融合时，需要模拟执行过程，计算 **L2 内存使用峰值**：

*   **当前算子需要的内存**：非常量输入 + 输出
*   **其他算子驻留的内存**：后续算子还需要用的中间结果

如果峰值 ≤ L2 容量 → 融合成功
如果峰值 > L2 容量 → 调整算子顺序，再次尝试
如果尝试所有顺序都失败 → 放弃融合

#### (6) 算子顺序调整
为什么调整顺序能降低内存峰值？

**示例**：融合 A、B、C、D 四个算子

**顺序 1**：A → B → C → D
*   执行 A 时：需要保留 A 的输出（给 B 用）
*   执行 B 时：需要保留 A 的输出（给 D 用）+ B 的输出（给 C 用）
*   执行 C 时：需要保留 A 的输出（给 D 用）+ C 的输出（给 D 用）
*   峰值出现在执行 C 时

**顺序 2**：A → D → B → C
*   执行 A 时：保留 A 的输出
*   执行 D 时：A 的输出用完，释放；保留 D 的输出
*   执行 B 时：保留 B 的输出
*   执行 C 时：只需 B 和 D 的输出
*   峰值更低！

编译器会遍历所有合法的执行顺序（保持数据依赖的拓扑关系），找到内存峰值最小的那个。

#### (7) 生成融合算子
融合成功后，生成一个新的"超级算子"（**AutoFusionOp**），包含：
*   内部所有子算子的表达式
*   确定的执行顺序
*   内存分配方案

### 4.2 第二步：解析融合结构（运行时执行）

#### (1) 统一接口设计
所有自动融合生成的算子，对上层框架来说都是一个普通算子：`AutoFusionOp`

*   框架无需知道内部有多少子算子
*   框架只管理 L3 外层存储
*   `AutoFusionOp` 内部自己管理 L2 内层存储

#### (2) 自动拆解与执行
当 `AutoFusionOp` 被调用时：
1.  拆解内部的子算子列表
2.  按照预先确定的顺序，逐个调用子算子
3.  中间结果存放在 L2（AutoFusionOp 内部管理）
4.  最终结果写回 L3（返回给框架）

#### (3) 内存管理策略
*   **输入来自融合外部** → 从 L3 读取
*   **输入来自融合内部** → 从 L2 读取
*   **输出不是最终结果** → 写入 L2
*   **输出是最终结果** → 写回 L3

#### (4) Launch 开销优化
通常每个算子调用都需要一次 **Launch**（启动内核），每次有固定开销（如 10 微秒）。

*   **不融合**：3 个算子 = 3 次 Launch = 30 微秒开销
*   **融合后**：1 个 AutoFusionOp = 1 次 Launch = 10 微秒开销

对于计算时间短的轻量算子（如 ReLU），Launch 开销可能占总时间的 50%，融合后性能提升显著。

#### (5) 生命周期分析
编译器分析每个中间张量的生命周期：
*   **创建时刻**：某算子输出时
*   **最后使用时刻**：最后一个依赖它的算子执行后
*   **释放时刻**：生命周期结束立即释放，供后续算子复用

通过精细化管理，进一步降低 L2 内存峰值。

### 4.3 第三步：算子实现适配

融合后，算子的输入输出可能在 L2 而不是 L3，需要调整算子实现：

*   **编译期感知**：算子在编译时知道输入输出在哪个层级
*   **策略调整**：
    *   输入在 L3：需要预加载到 L2（开销大）
    *   输入在 L2：直接使用（开销小）
*   **性能优化**：根据不同情况，生成最优的机器码

---

## 5. 实战案例：ResNet 中的算子融合

### 5.1 场景描述
ResNet 中的一个典型序列：

```
Conv2D (卷积)
    ↓
BatchNorm (归一化)
    ↓
ReLU (激活)
```

假设中间张量大小：
*   Conv2D 输出：`[32, 128, 56, 56]` = 16 MB
*   BatchNorm 输出：`[32, 128, 56, 56]` = 16 MB
*   L2 容量：32 MB

### 5.2 融合前的执行

**算子 1：Conv2D**
1.  从 L3 读取输入：`[32, 128, 56, 56]` = 16 MB
2.  从 L3 读取权重：`[128, 128, 3, 3]` = 0.5 MB
3.  L2 计算，耗时 5 ms
4.  写回 L3：16 MB
5.  Launch 开销：10 μs

**算子 2：BatchNorm**
1.  从 L3 读取输入（Conv2D 输出）：16 MB
2.  L2 计算，耗时 1 ms
3.  写回 L3：16 MB
4.  Launch 开销：10 μs

**算子 3：ReLU**
1.  从 L3 读取输入（BatchNorm 输出）：16 MB
2.  L2 计算，耗时 0.5 ms
3.  写回 L3：16 MB
4.  Launch 开销：10 μs

**总开销**：
*   L3 读写：16 MB × 6 次 = 96 MB 传输
*   计算时间：5 + 1 + 0.5 = 6.5 ms
*   Launch 开销：30 μs
*   **假设 L3 带宽 100 GB/s，L3 传输耗时 ≈ 0.96 ms**
*   **总耗时 ≈ 7.5 ms**

### 5.3 融合后的执行

**AutoFusionOp {Conv2D, BatchNorm, ReLU}**
1.  从 L3 读取输入：16 MB
2.  从 L3 读取权重：0.5 MB
3.  **L2 内部依次执行**：
    *   Conv2D 计算 → 输出到 L2（16 MB）
    *   BatchNorm 计算 → 输出到 L2（16 MB，覆盖 Conv 输出）
    *   ReLU 计算 → 输出到 L2（16 MB，覆盖 BN 输出）
4.  写回 L3：16 MB
5.  Launch 开销：10 μs（只一次！）

**总开销**：
*   L3 读写：16.5 MB × 2 次 = 33 MB 传输
*   计算时间：6.5 ms（不变）
*   Launch 开销：10 μs（省了 20 μs）
*   **L3 传输耗时 ≈ 0.33 ms**
*   **总耗时 ≈ 6.8 ms**

**性能提升**：
$$ 加速比 = \frac{7.5}{6.8} \approx 1.10 \text{倍（提速 10%）} $$

**更重要的是**：如果是更复杂的融合组（如 10 个算子），收益会更显著！

### 5.4 内存峰值分析

**执行 Conv2D 时**：
*   输入张量（L2）：16 MB
*   输出张量（L2）：16 MB
*   权重张量（常驻）：0.5 MB
*   合计：32.5 MB > 32 MB → **超出！**

**调整策略**：输入张量可以边读边释放（流式处理），实际峰值 < 20 MB，满足条件。

---

## 6. 核心创新点总结

### 6.1 无模式感知（Pattern-Free）
*   不需要预定义融合模式（如 Conv+BN+ReLU）
*   只要有数据依赖，就自动尝试融合
*   适应性强，适用于各种网络结构（ResNet、Transformer、YOLO...）

### 6.2 算子类型无关（Type-Agnostic）
*   不关心算子是卷积、矩阵乘法还是激活函数
*   只关心张量大小和数据流向
*   开发成本低，易于维护

### 6.3 极致的软件缓存（Software Cache）
*   不依赖硬件 Cache 机制（硬件 Cache 容量小、策略固定）
*   编译期就确定好哪些数据驻留 L2
*   充分利用 L2 资源，能融尽融

### 6.4 积木化执行（Modular Execution）
*   不需要为每种融合模式手写一个"大算子"
*   复用原子级别的算子实现（积木）
*   避免重复开发，降低维护成本

### 6.5 编译期优化（Zero Runtime Overhead）
*   所有搜索、分析、决策都在编译时完成
*   运行时只是执行预先生成的代码
*   不带来额外的运行时开销

---

## 7. 技术对比

| 特性 | 传统图样匹配 | 本方案（AFE） |
|------|-------------|---------------|
| 融合模式 | 预定义固定模式 | 自动搜索任意模式 |
| 算子类型感知 | 需要 | 不需要 |
| 开发成本 | 高（每种模式手写代码）| 低（自动生成）|
| 适用范围 | 有限（仅支持的模式）| 广泛（所有数据依赖）|
| 内存管理 | 手动 | 自动生命周期分析 |
| 运行开销 | 无 | 无 |

---

## 8. 总结

这项技术就像是给 AI 编译器装了一个**智能管家**：

1.  **自动识别**：不需要人告诉它"这里可以融合"，它自己会找。
2.  **精细计算**：模拟每个执行时刻的内存占用，确保不会爆掉 L2。
3.  **灵活调整**：通过调整算子顺序，榨取最大的融合空间。
4.  **零负担执行**：对上层框架透明，对底层硬件友好。

**最终效果**：在不改变硬件的前提下，通过编译器优化，让 AI 芯片的性能发挥到极致！

