# 针对多级存储芯片的超大规模张量计算图编译优化技术方案

## 1. 什么是“超大规模张量编译优化”？

想象一下，你有一张巨大的拼图（超大规模张量），桌子（L2 高速缓存）太小放不下，只能放在远处的地板（L3 慢速内存）上。

*   **传统做法**：你每次跑去地板拿一块拼图，拼好后再跑回去放好，再拿下一块。把时间都浪费在来回跑路（数据搬运）上了。
*   **本方案的做法（TLF 引擎）**：
    1.  **切分（Tiling）**：先把大拼图切成刚好能放在桌子上的小块。
    2.  **融合（Fusion）**：一次性把那一小块拼图需要做的所有步骤（比如涂色、抛光）都在桌子上做完。
    3.  **循环（Loop）**：用同样的方法处理每一小块，直到全部完成。
    4.  **自动拼接**：做完最后一块时，它们在地板上就已经自动拼成了一张完整的大图，不需要额外整理。

简单来说，这项技术就是**教 AI 芯片如何“化整为零”，少跑路、多干活**。

---

## 2. 为什么要这么做？（背景与痛点）

AI 芯片里有三级存储，就像三个不同的工作台：
*   **L3（大仓库）**：地方大，但离得远，存取慢。
*   **L2（小桌子）**：地方小，离得近，存取快。
*   **L1（手边盘子）**：最小最快，就在手边。

**问题**：现在的 AI 模型越来越大，数据（张量）经常比“小桌子”（L2）还大。
**后果**：只能被迫在“大仓库”（L3）里慢吞吞地干活，或者不停地在仓库和桌子之间搬来搬去，导致芯片性能被“堵”在路上（Memory Wall）。

**目标**：把大张量切开，塞进 L2 里高速处理，榨干芯片性能。

---

## 3. 核心技术：怎么实现的？

### 3.1 算账：怎么切最划算？（Cost Model）

编译器不是瞎切，它会像会计一样算一笔账：

*   **成本 A**：直接在 L3 上慢速跑，虽然慢，但不用切，省事。
*   **成本 B**：切成 10 份在 L2 上跑。虽然快，但要多算切分的开销，还要考虑搬运的油费。

**公式**：
$$ 收益 = (直接在 L3 跑的耗时) - (切成 N 份在 L2 跑的总耗时) $$

只有当**收益 > 0** 时，我们才考虑切分。而且，如果好几个算子（加法、乘法等）连在一起，我们发现中间结果不需要搬回仓库，直接在桌子上接着用，那收益就更大了！

### 3.2 决策：全局最优解（动态规划）

有时候，只看眼前可能会吃亏。
比如：切分算子 A 可能亏了一点点，但为了让后面的算子 B 和 C 能完美配合，整体算下来可能是大赚的。

编译器使用**动态规划（Dynamic Programming）**算法，把整张图的所有可能性都搜索一遍，找到一条**总耗时最短**的切分路径。它不贪图眼前的小利，而是追求全局的胜利。

### 3.3 表达：用“循环”简化指令（Loop Semantics）

如果你要把一个大蛋糕切成 100 份吃掉，笨办法是写 100 行指令：
1. 吃第1块
2. 吃第2块
...
100. 吃第100块

聪明的办法是写一个**循环**：
`While (还有蛋糕): 吃下一块`

本方案引入了 **While 节点**，把成百上千次的切分执行逻辑，折叠成一个简单的循环。这样指令更短，控制更简单。

### 3.4 技巧：零拷贝自动拼接（Zero-Copy Stitching）

通常，切分处理完后，我们会得到一堆零碎的小结果。传统做法是最后再搞一次“大扫除”，把碎片拼起来（Concat 算子），这又是一次搬运。

**本方案的高招**：
在开始干活前，就已经在 L3 仓库里划好了最终结果的停车位。
*   处理第 1 块时，直接告诉它：“做完停在第 1 号位”。
*   处理第 2 块时，直接告诉它：“做完停在第 2 号位”。

等循环结束，大家自动就位，**不需要额外的拼接步骤**，省去了一次巨大的搬运开销。

---

## 4. 实战案例：ResNet 网络的融合优化

为了让技术方案更加直观，我们用经典的 **ResNet-50** 网络结构来演示实际的切分与融合过程。

### 4.1 ResNet 的基本结构

ResNet（残差网络）是 AI 领域的经典模型，其核心单元是 **Residual Block（残差块）**，典型结构如下：

```
输入张量 [Batch, C_in, H, W]
    ↓
Conv1 (1x1 卷积，降维)
    ↓
BatchNorm1 + ReLU
    ↓
Conv2 (3x3 卷积，特征提取)
    ↓
BatchNorm2 + ReLU
    ↓
Conv3 (1x1 卷积，升维)
    ↓
BatchNorm3
    ↓
Add (与输入做残差连接)
    ↓
ReLU
    ↓
输出张量 [Batch, C_out, H, W]
```

### 4.2 超大规模张量的出现场景

假设我们在处理高分辨率输入（如 4K 视频帧）：
*   **输入张量形状**：`[32, 256, 1024, 1024]`（Batch=32, 通道=256, 高宽=1024）
*   **张量大小**：32 × 256 × 1024 × 1024 × 4 bytes = **34 GB**
*   **L2 存储容量**：假设为 **16 MB**

显然，**34 GB 的张量无法放入 16 MB 的 L2**，必须进行切分。

### 4.3 融合方案分析

#### 阶段 1：图区域划分
编译器分析 ResNet 的计算图，识别出可融合的区域：

**融合组 1**：`Conv1 -> BatchNorm1 -> ReLU`
*   这三个算子连续执行，中间结果可驻留 L2。

**融合组 2**：`Conv2 -> BatchNorm2 -> ReLU`
*   同样的模式。

**融合组 3**：`Conv3 -> BatchNorm3 -> Add -> ReLU`
*   包含残差连接，需要同时缓存主路径和跳跃连接的张量。

**不可融合的阻断点**：
*   如果遇到全局池化（Global Average Pooling），需要跨整个特征图计算，无法通过切分 Batch 维度处理，因此会成为区域边界。

#### 阶段 2：切分策略制定

由于张量在 Batch 维度（第 0 维）是独立的，编译器决定沿 **Batch 维度**切分：

**切分方案**：
*   原始 Batch = 32，每次处理 Batch = 4
*   切分次数 N = 32 / 4 = 8 次
*   每次切片张量大小：`[4, 256, 1024, 1024]` ≈ **4.2 GB**

**L2 峰值计算**（以融合组 3 为例）：
1.  Conv3 输入张量：4 MB
2.  Conv3 权重张量（常驻）：2 MB
3.  Conv3 输出 + BatchNorm3 输出：4 MB
4.  跳跃连接张量（从融合组 1 缓存）：4 MB
5.  Add 输出 + ReLU 输出：4 MB

峰值 = 4 + 2 + 4 + 4 + 4 = **18 MB** > 16 MB，仍超出！

**调整方案**：继续切分，改为 Batch = 2，重新计算峰值 ≈ **10 MB**，满足条件。

#### 阶段 3：收益计算

**不切分（直接在 L3 运行）**：
*   单次 Conv1 耗时：100 ms（含 L3 读写）
*   单次 BatchNorm1 耗时：20 ms
*   单次 ReLU 耗时：10 ms
*   总耗时（不融合）：130 ms

**切分+融合（在 L2 运行）**：
*   单次 L3->L2 搬运：5 ms
*   单次融合组执行（L2 内部）：50 ms（计算快，无中间 L3 写入）
*   单次 L2->L3 回写：5 ms
*   单次总耗时：60 ms
*   切分 16 次总耗时：60 × 16 = 960 ms
*   固定调用开销节省：16 次融合 vs 48 次单算子调用

**最终收益**：
$$ 收益 = 130 \times 16 - 960 = 1120 \, \text{ms} $$
相当于**提速 2.2 倍**！

#### 阶段 4：循环语义生成

原始计算图被改写为：

```python
# 伪代码表示
While (batch_offset = 0; batch_offset < 32; batch_offset += 2):
    # Slice: 从 L3 切出当前批次
    input_slice = input[batch_offset:batch_offset+2, :, :, :]
    
    # 融合组 1：在 L2 执行
    x = Conv1_L2(input_slice)
    x = BatchNorm1_L2(x)
    x = ReLU_L2(x)
    
    # 融合组 2：在 L2 执行
    x = Conv2_L2(x)
    x = BatchNorm2_L2(x)
    x = ReLU_L2(x)
    
    # 融合组 3：在 L2 执行
    x = Conv3_L2(x)
    x = BatchNorm3_L2(x)
    x = Add_L2(x, input_slice)  # 残差连接
    x = ReLU_L2(x)
    
    # Deslice: 写回 L3（带偏移）
    output[batch_offset:batch_offset+2, :, :, :] = x
```

#### 阶段 5：零拷贝拼接

在 L3 上预先分配完整的输出张量地址：`output_base_addr`
*   第 1 次循环：写入 `output_base_addr + 0 * slice_size`
*   第 2 次循环：写入 `output_base_addr + 1 * slice_size`
*   ...
*   第 16 次循环：写入 `output_base_addr + 15 * slice_size`

循环结束时，`output_base_addr` 处已自动形成完整的 `[32, C_out, H, W]` 张量，**无需额外的 Concat 操作**。

### 4.4 关键技术点总结

1.  **形状反向推断**：从最后的 ReLU 输出（`[2, 256, 1024, 1024]`）反推到 Conv1 输入，确保每层的切分形状一致。
2.  **跳跃连接处理**：残差块中的 Add 算子需要同时访问两个张量（主路径和跳跃路径），必须确保它们都驻留在 L2 中。
3.  **权重复用**：Conv 的权重张量在每次切分中都被重复读取，这是负收益来源。但通过融合多个算子，正收益远大于权重重读的代价。
4.  **内存对齐优化**：实际实现中，切分大小会对齐硬件的 DMA 传输单元（如 256 字节），进一步提升带宽利用率。

---

## 5. 总结

这项技术就像是给 AI 芯片请了一位**精明的调度管家**：
1.  **精打细算**：用数学模型算出每一分收益（ResNet 中提速 2.2 倍）。
2.  **统筹全局**：不只看一步，而是规划好整条流水线（融合 3 个算子 vs 单独执行）。
3.  **化繁为简**：用循环指令管理复杂的切分任务（16 次循环 vs 48 个独立算子调用）。
4.  **一步到位**：利用地址技巧，避免最后返工拼接（零拷贝机制）。

最终结果就是：**同样的硬件，跑大模型速度更快！**
