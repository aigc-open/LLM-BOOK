# PyTorch 技术栈对比与选型指南

---

## 1. torch.compile - 一键加速神器

### 它是什么？

想象你在餐厅点菜：
- **以前（普通 PyTorch）**：你点一道菜，厨师做一道，上一道。虽然灵活，但效率低。
- **现在（torch.compile）**：你把所有菜单都给厨师看，他们提前规划好顺序，同时准备多道菜，效率高很多！

**一句话总结**：让 PyTorch 自动优化你的模型，只需加一行代码。

### 最简单的使用示例

```python
import torch
import torch.nn as nn

# 1. 定义一个普通的模型
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(100, 50)
        self.layer2 = nn.Linear(50, 10)
    
    def forward(self, x):
        x = torch.relu(self.layer1(x))
        x = self.layer2(x)
        return x

# 2. 创建模型
model = MyModel()

# 3. 神奇的一行代码！
model = torch.compile(model)

# 4. 之后完全正常使用，自动加速
x = torch.randn(32, 100)
output = model(x)  # 就这么简单！
```

### 优点（好处）

#### 1. 超级简单
```python
# 只需要加一行
model = torch.compile(model)

# 不需要：
# - 学习新语法
# - 改写模型代码
# - 修改训练流程
```

#### 2. 性能提升明显
```python
import time

# 测试普通模型
model_normal = MyModel()
start = time.time()
for _ in range(100):
    output = model_normal(x)
print(f"普通模型耗时: {time.time() - start:.2f}秒")

# 测试编译后的模型
model_fast = torch.compile(MyModel())
# 预热（第一次会慢）
_ = model_fast(x)

start = time.time()
for _ in range(100):
    output = model_fast(x)
print(f"编译模型耗时: {time.time() - start:.2f}秒")

# 输出示例：
# 普通模型耗时: 1.50秒
# 编译模型耗时: 0.80秒  <- 快了将近一倍！
```

#### 3. 保持灵活性
```python
# 支持 if/else 控制流
def forward(self, x, use_dropout=True):
    x = self.layer1(x)
    if use_dropout:  # 可以有条件判断
        x = self.dropout(x)
    x = self.layer2(x)
    return x

# 支持循环
def forward(self, x):
    for layer in self.layers:  # 可以用循环
        x = layer(x)
    return x
```

### 缺点（要注意的）

#### 1. 第一次运行会慢
```python
model = torch.compile(model)

# 第一次运行：慢（在编译）
print("第一次...")
start = time.time()
output = model(x)
print(f"耗时: {time.time() - start:.2f}秒")  # 可能 5-10 秒

# 第二次运行：快（用编译好的）
print("第二次...")
start = time.time()
output = model(x)
print(f"耗时: {time.time() - start:.2f}秒")  # 只需 0.01 秒
```

**解决方法**：预热
```python
# 训练开始前预热一次
model = torch.compile(model)
dummy_input = torch.randn(batch_size, input_dim)
_ = model(dummy_input)  # 触发编译
print("预热完成，可以开始训练了！")

# 之后正常训练，速度快
for batch in dataloader:
    output = model(batch)  # 很快
```

#### 2. 输入形状变化会重新编译
```python
model = torch.compile(model)

# 第一次：batch_size=32
x1 = torch.randn(32, 100)
output = model(x1)  # 编译版本1

# 第二次：batch_size=32（形状相同）
x2 = torch.randn(32, 100)
output = model(x2)  # 使用缓存，很快

# 第三次：batch_size=64（形状变了）
x3 = torch.randn(64, 100)
output = model(x3)  # 重新编译！又慢了
```

**解决方法**：固定输入形状
```python
from torch.utils.data import DataLoader

# 固定 batch_size，丢弃不完整的 batch
dataloader = DataLoader(
    dataset,
    batch_size=32,      # 固定大小
    drop_last=True      # 丢弃最后不完整的 batch
)
```

### 什么时候用？

**推荐使用**：
```python
# 1. 生产环境训练
# 训练时间长，首次编译开销可以忽略
model = torch.compile(model)
for epoch in range(100):  # 训练 100 轮
    train(model)

# 2. 推理服务
# 输入形状固定，性能提升明显
model = torch.compile(model)
for request in requests:
    prediction = model(request.data)
```

**不推荐使用**：
```python
# 1. 快速调试
# 频繁改代码时，每次都要重新编译
model = torch.compile(model)  # 不建议
for i in range(10):
    # 改改代码测试一下
    pass

# 2. 输入形状经常变
# 每次形状变化都重新编译，反而更慢
for batch_size in [16, 32, 64, 128]:  # 不建议
    x = torch.randn(batch_size, 100)
    output = model(x)  # 编译 4 次
```

### 实战例子：训练一个分类器

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# 1. 定义模型
class Classifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(784, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 10)
        )
    
    def forward(self, x):
        return self.net(x)

# 2. 准备数据
X_train = torch.randn(10000, 784)
y_train = torch.randint(0, 10, (10000,))
train_dataset = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_dataset, batch_size=32, drop_last=True)

# 3. 创建模型（加上 torch.compile）
model = Classifier()
model = torch.compile(model)  # 就这一行！

# 4. 训练（完全正常的训练代码）
optimizer = optim.Adam(model.parameters())
criterion = nn.CrossEntropyLoss()

print("开始训练...")
for epoch in range(5):
    total_loss = 0
    for X_batch, y_batch in train_loader:
        # 前向传播（自动加速）
        output = model(X_batch)
        loss = criterion(output, y_batch)
        
        # 反向传播（也被加速）
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    print(f"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}")

print("训练完成！")
```

---

## 2. TorchScript - C++ 部署专家

### 它是什么？

想象你写了一个 Python 程序：
- **问题**：只能在安装了 Python 的电脑上运行
- **TorchScript**：把它"翻译"成一个独立的程序，不需要 Python 也能运行

**一句话总结**：让你的模型脱离 Python，可以在 C++、手机、嵌入式设备上运行。

### 两种使用方式

#### 方式1：trace（追踪模式）- 简单但有限制

```python
import torch
import torch.nn as nn

# 定义模型
class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(3, 64, 3)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.conv(x)
        x = self.relu(x)
        return x

# 创建模型和示例输入
model = SimpleModel()
example_input = torch.randn(1, 3, 224, 224)

# trace：运行一次，记录所有操作
traced_model = torch.jit.trace(model, example_input)

# 保存（包含代码+权重，一个文件搞定）
traced_model.save("model.pt")

print("模型已保存到 model.pt，可以在 C++ 中加载了！")
```

**trace 的问题**：无法处理 if/else
```python
class BadForTrace(nn.Module):
    def forward(self, x, use_relu):
        x = self.conv(x)
        if use_relu:  # <- trace 只会记录一个分支！
            x = torch.relu(x)
        return x

# trace 时 use_relu=True
traced = torch.jit.trace(model, (example_input, True))

# 但之后 use_relu=False 也会执行 relu！
# 因为 trace 只记录了 True 的分支
output = traced(example_input, False)  # 还是会执行 relu
```

#### 方式2：script（脚本模式）- 复杂但完整

```python
# 用装饰器标记
@torch.jit.script
class ScriptModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(3, 64, 3)
    
    def forward(self, x, use_relu: bool):  # 注意：需要类型标注
        x = self.conv(x)
        if use_relu:  # <- 两个分支都会保留
            x = torch.relu(x)
        return x

# 直接保存
scripted_model = ScriptModel()
scripted_model.save("model_script.pt")

# 之后在 Python 或 C++ 中加载
loaded = torch.jit.load("model_script.pt")
```

### 在 C++ 中使用（不需要 Python！）

```cpp
// main.cpp
#include <torch/script.h>
#include <iostream>

int main() {
    // 1. 加载模型（不需要 Python 环境）
    torch::jit::script::Module model;
    model = torch::jit::load("model.pt");
    
    // 2. 准备输入
    std::vector<torch::jit::IValue> inputs;
    inputs.push_back(torch::randn({1, 3, 224, 224}));
    
    // 3. 推理
    auto output = model.forward(inputs).toTensor();
    
    std::cout << "推理完成！输出形状: " 
              << output.sizes() << std::endl;
    
    return 0;
}

// 编译：g++ main.cpp -ltorch -o inference
// 运行：./inference
```

### 优点

1. **完全脱离 Python**
```python
# Python 端
model.save("model.pt")

# C++ 端（服务器上没有 Python 也能运行）
# ./inference model.pt
```

2. **一个文件包含所有**
```python
# 保存
scripted_model.save("model.pt")  # 包含：代码 + 权重 + 结构

# 加载（在任何地方）
model = torch.jit.load("model.pt")  # 无需原始代码
```

3. **移动端部署**
```python
# 优化为移动端版本
from torch.utils.mobile_optimizer import optimize_for_mobile

mobile_model = optimize_for_mobile(scripted_model)
mobile_model._save_for_lite_interpreter("model_mobile.ptl")

# 可以在 Android/iOS 上使用
```

### 缺点

1. **需要改代码**
```python
# 有些 Python 特性不支持
import numpy as np  # <- 不支持 numpy

@torch.jit.script
def my_function(x):
    # 不能用 numpy
    arr = np.array([1, 2, 3])  # 错误！
    
    # 要用 torch
    arr = torch.tensor([1, 2, 3])  # 正确
```

2. **调试困难**
```python
# 错误信息不清楚
@torch.jit.script
def buggy(x):
    return x.unknown_method()  # 错误信息很难懂

# 建议：先在普通 Python 中调试好，再转 script
```

### 什么时候用？

**推荐**：
- C++ 服务器部署
- 移动端 App
- 嵌入式设备
- 不允许 Python 的环境

**不推荐**：
- Python 训练（用 torch.compile）
- 快速原型开发（太麻烦）

---

## 3. TorchFX - 图编辑工具

### 它是什么？

想象你的模型是一张流程图：
- **TorchScript**：把流程图变成图片，看得到但改不了
- **TorchFX**：流程图还是可编辑的，你可以随意修改

**一句话总结**：可以查看和修改模型计算图的工具。

### 基本使用

```python
import torch
import torch.nn as nn
import torch.fx as fx

# 1. 定义模型
class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear1 = nn.Linear(10, 20)
        self.linear2 = nn.Linear(20, 5)
    
    def forward(self, x):
        x = self.linear1(x)
        x = torch.relu(x)
        x = self.linear2(x)
        return x

# 2. 符号追踪（不需要实际运行）
model = Model()
traced = fx.symbolic_trace(model)

# 3. 查看生成的代码
print(traced.code)

# 输出：
# def forward(self, x):
#     linear1 = self.linear1(x)
#     relu = torch.relu(linear1)
#     linear2 = self.linear2(relu)
#     return linear2
```

### 实战：修改模型（把 ReLU 换成 GELU）

```python
import torch.nn.functional as F

# 1. 追踪模型
traced = fx.symbolic_trace(model)

# 2. 遍历所有操作
for node in traced.graph.nodes:
    # 找到 relu
    if node.op == 'call_function' and node.target == torch.relu:
        # 替换成 gelu
        node.target = F.gelu
        print(f"已将 {node.name} 从 relu 改为 gelu")

# 3. 重新编译
traced.recompile()

# 4. 查看修改后的代码
print("\n修改后的代码：")
print(traced.code)

# 输出：
# def forward(self, x):
#     linear1 = self.linear1(x)
#     relu = torch.nn.functional.gelu(linear1)  # <- 已改变
#     linear2 = self.linear2(relu)
#     return linear2

# 5. 使用修改后的模型
x = torch.randn(5, 10)
output = traced(x)
print(f"\n输出形状: {output.shape}")
```

### 实战：删除 Dropout（部署时用）

```python
def remove_dropout(traced_model):
    """移除模型中的所有 Dropout 层"""
    
    graph = traced_model.graph
    
    # 找到所有 dropout
    nodes_to_remove = []
    for node in graph.nodes:
        if node.op == 'call_module':
            # 获取实际的模块
            module = traced_model.get_submodule(node.target)
            if isinstance(module, nn.Dropout):
                print(f"找到 Dropout: {node.name}")
                nodes_to_remove.append(node)
    
    # 删除 dropout 节点
    for node in nodes_to_remove:
        # Dropout 的输出就是输入，直接替换
        node.replace_all_uses_with(node.args[0])
        graph.erase_node(node)
        print(f"已删除: {node.name}")
    
    # 重新编译
    traced_model.recompile()
    return traced_model

# 使用
class ModelWithDropout(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear1 = nn.Linear(10, 20)
        self.dropout = nn.Dropout(0.5)
        self.linear2 = nn.Linear(20, 5)
    
    def forward(self, x):
        x = self.linear1(x)
        x = self.dropout(x)  # <- 训练时需要
        x = self.linear2(x)
        return x

model = ModelWithDropout()
traced = fx.symbolic_trace(model)

print("原始代码：")
print(traced.code)

# 删除 dropout
traced = remove_dropout(traced)

print("\n删除后的代码：")
print(traced.code)
```

### 什么时候用？

**推荐**：
- 自动修改模型结构
- 实现自定义优化
- 量化、剪枝工具开发
- 为自定义硬件做图优化

**不推荐**：
- 日常训练（用 torch.compile）
- 简单任务（太复杂）

---

## 4. Lazy Tensor - 延迟执行

### 它是什么？

想象你在做饭：
- **Eager 模式（正常 PyTorch）**：切菜 -> 炒菜 -> 装盘，一步一步做
- **Lazy 模式**：先记录所有步骤，最后一次性做完，可以优化顺序

**一句话总结**：不立即执行，而是先记录下来，最后一起优化执行。

### 对比示例

```python
# Eager 模式（PyTorch 默认）
x = torch.tensor([1.0, 2.0, 3.0])
y = x + 1      # 立即执行
z = y * 2      # 立即执行
w = z - 0.5    # 立即执行
print(w)       # tensor([2.5, 4.5, 6.5])

# 每个操作都要：
# 1. 启动 GPU kernel
# 2. 读取数据
# 3. 计算
# 4. 写回数据
# 总共启动 3 次 kernel

# Lazy 模式（假设）
x = lazy_tensor([1.0, 2.0, 3.0])
y = x + 1      # 不执行，只记录
z = y * 2      # 不执行，只记录
w = z - 0.5    # 不执行，只记录
print(w)       # 现在才执行，而且优化为一个 kernel

# 优化后相当于：
# w = (x + 1) * 2 - 0.5
# 只启动 1 次 kernel！
```

### 实际使用：PyTorch/XLA（TPU）

```python
# 需要安装：pip install torch_xla
import torch_xla
import torch_xla.core.xla_model as xm

# 1. 使用 XLA 设备（TPU）
device = xm.xla_device()

# 2. 创建 lazy tensor
x = torch.randn(1000, 1000, device=device)
y = torch.randn(1000, 1000, device=device)

# 3. 这些操作都是 lazy 的（不立即执行）
z = x @ y      # 矩阵乘法
w = z + 1      # 加法
result = w.mean()  # 求平均

# 4. 显式触发执行
xm.mark_step()  # 现在才真正执行

# 或者转到 CPU 也会触发执行
result_cpu = result.cpu()
print(result_cpu)
```

### 延迟初始化（最常用的 Lazy 功能）

```python
import torch.nn as nn

# 普通 Linear：需要知道输入维度
# linear = nn.Linear(in_features=?, out_features=10)  # ？是多少？

# LazyLinear：不需要知道输入维度
model = nn.Sequential(
    nn.LazyLinear(256),  # 不需要指定 in_features
    nn.ReLU(),
    nn.LazyLinear(128),
    nn.ReLU(),
    nn.LazyLinear(10)
)

# 第一次前向传播时自动推断
x = torch.randn(32, 512)  # 输入是 512 维
output = model(x)  # 自动推断：第一层是 Linear(512, 256)

print(model[0].weight.shape)  # torch.Size([256, 512])
# 自动推断出来了！
```

### 优点

1. **自动算子融合**
```python
# 多个操作融合为一个，减少开销
y = x + 1
z = y * 2
w = z.relu()
# -> 融合为一个 kernel：relu((x + 1) * 2)
```

2. **延迟初始化很方便**
```python
# 不需要提前计算输入维度
model = nn.Sequential(
    nn.LazyConv2d(64, 3),  # 不知道输入通道数
    nn.LazyBatchNorm2d(),  # 不知道特征数
    nn.Flatten(),
    nn.LazyLinear(10)      # 不知道输入维度
)

# 第一次运行自动推断
x = torch.randn(1, 3, 32, 32)
output = model(x)  # 自动配置好所有层
```

### 缺点

1. **调试很困难**
```python
x = lazy_tensor([1, 2, 3])
y = x + 1
print(y)  # <- 触发执行，失去 lazy 的优势

# 而且错误会延迟报告
z = y / 0  # 不会立即报错
w = z + 1  # 也不会报错
print(w)   # <- 现在才报错，但不知道是哪里的问题
```

2. **心智负担大**
```python
# 需要记住什么时候触发执行
x = lazy_tensor(data)
y = compute(x)  # 不执行
z = process(y)  # 不执行

# 什么时候才真正执行？
z.cpu()       # 这里
print(z)      # 或这里
if z.sum() > 0:  # 或这里
```

### 什么时候用？

**推荐**：
- TPU 训练（必须用 PyTorch/XLA）
- 延迟初始化模型（LazyLinear 等）

**不推荐**：
- GPU 训练（用 torch.compile 更好）
- 日常开发（调试困难）

---

## 5. Torch Dynamo - 幕后英雄

### 它是什么？

**torch.compile 的核心引擎**，负责自动捕获你的代码。

想象一个间谍：
- 你写代码运行
- Dynamo 偷偷记录你做了什么
- 然后优化这些操作

**一句话总结**：torch.compile 背后的技术，一般不需要直接使用。

### 你其实一直在用它

```python
# 当你写这行代码时
model = torch.compile(model)

# 内部流程：
# 1. Dynamo 拦截 Python 字节码
# 2. 捕获所有 PyTorch 操作
# 3. 构建计算图
# 4. 交给后端优化
# 5. 返回优化后的函数

# 你只需要知道：它让 torch.compile 工作
```

### 调试时可能用到

```python
import torch
import torch._dynamo as dynamo

# 查看 Dynamo 捕获了什么
def my_function(x, y):
    z = x + y
    return z * 2

# 查看捕获的图
explanation = dynamo.explain(my_function)(torch.randn(10), torch.randn(10))
print(explanation)

# 重置 Dynamo（清空缓存）
dynamo.reset()
```

```bash
Graph Count: 1
Graph Break Count: 0
Op Count: 2
Break Reasons:
Ops per Graph:
  Ops 1:
    <built-in function add>
    <built-in function mul>
Out Guards:
  Guard 1:
    Name: "L['y']"
    Source: local
    Create Function: TENSOR_MATCH
    Guard Types: ['TENSOR_MATCH']
    Code List: ["hasattr(L['y'], '_dynamo_dynamic_indices') == False"]
    Object Weakref: <weakref at 0x7fa13ff91850; dead>
    Guarded Class Weakref: <weakref at 0x7fa2f7900180; to 'torch._C._TensorMeta' at 0x55eac1955790 (Tensor)>
  Guard 2:
    Name: "L['x']"
    Source: local
    Create Function: TENSOR_MATCH
    Guard Types: ['TENSOR_MATCH']
    Code List: ["hasattr(L['x'], '_dynamo_dynamic_indices') == False"]
    Object Weakref: <weakref at 0x7fa13ff915d0; dead>
    Guarded Class Weakref: <weakref at 0x7fa2f7900180; to 'torch._C._TensorMeta' at 0x55eac1955790 (Tensor)>
  Guard 3:
    Name: ''
    Source: global
    Create Function: DEFAULT_DEVICE
    Guard Types: ['DEFAULT_DEVICE']
    Code List: ['utils_device.CURRENT_DEVICE == None']
    Object Weakref: None
    Guarded Class Weakref: None
  Guard 4:
    Name: ''
    Source: global
    Create Function: GRAD_MODE
    Guard Types: None
    Code List: None
    Object Weakref: None
    Guarded Class Weakref: None
  Guard 5:
    Name: ''
    Source: shape_env
    Create Function: SHAPE_ENV
    Guard Types: None
    Code List: None
    Object Weakref: None
    Guarded Class Weakref: None
  Guard 6:
    Name: ''
    Source: global
    Create Function: DETERMINISTIC_ALGORITHMS
    Guard Types: None
    Code List: None
    Object Weakref: None
    Guarded Class Weakref: None
  Guard 7:
    Name: ''
    Source: global
    Create Function: TORCH_FUNCTION_STATE
    Guard Types: None
    Code List: None
    Object Weakref: None
    Guarded Class Weakref: None
Compile Times: TorchDynamo compilation metrics:
Function, Runtimes (s)
_compile.compile_inner, 0.0338
OutputGraph.call_user_compiler, 0.0005
gc, 0.0022
```

### Graph Break（图断裂）问题

```python
import torch
import torch._dynamo as dynamo

def function_with_break(x):
    # 图 1
    a = x + 1
    b = a * 2
    
    print(f'中间结果: {b.sum()}')  # <- Graph Break！
    
    # 图 2（新的图）
    c = b - 1
    return c


# 使用 torch.compile(trace_mode="verbose") 可以帮助诊断哪里发生了 graph break
x = torch.randn(4, 4)
compiled_fn = torch.compile(function_with_break, fullgraph=False)
result = compiled_fn(x)
print('结果: ', result)

# 如果只是调试哪里 graph break，可以使用 torch._dynamo.explain
dynamo_explain = dynamo.explain(function_with_break)(x)
print(dynamo_explain)


# Dynamo 会把它分成两个图
# 影响优化效果

# 解决方法：避免不必要的 print
def function_no_break(x):
    a = x + 1
    b = a * 2
    c = b - 1
    # print 放到最后
    return c
```

```bash
中间结果: 47.17870330810547
结果:  tensor([[-2.2922,  1.1461, -1.0614,  1.8554],
        [ 2.4516,  0.7236,  1.5856,  2.3815],
        [ 6.8707,  1.7623,  2.3160,  2.6533],
        [ 3.3213,  2.3871,  3.7911,  1.2869]])
中间结果: 47.17870330810547
Graph Count: 2
Graph Break Count: 1
Op Count: 3
Break Reasons:
  Break Reason 1:
    Reason: Failed to trace builtin operator
  Explanation: Dynamo does not know how to trace builtin operator `print` with argument types ['<unknown type>'] (has_kwargs False)
  Hint: Avoid calling builtin `print` with argument types ['<unknown type>']. Consider using an equivalent alternative function/method to `print`.
  Hint: If you are attempting to call a logging function (e.g. `print`), you can try adding it to `torch._dynamo.config.reorderable_logging_functions`.
  Hint: Please report an issue to PyTorch.

  Developer debug context: builtin print [<class 'torch._dynamo.variables.misc.StringFormatVariable'>] False

    User Stack:
      <FrameSummary file <string>, line 9 in function_with_break>
Ops per Graph:
  Ops 1:
    <built-in function add>
    <built-in function mul>
  Ops 2:
    <built-in function sub>
Out Guards:
  Guard 1:
    Name: ''
    Source: global
    Create Function: GRAD_MODE
    Guard Types: None
    Code List: None
    Object Weakref: None
    Guarded Class Weakref: None
  Guard 2:
    Name: "L['x']"
    Source: local
    Create Function: TENSOR_MATCH
    Guard Types: ['TENSOR_MATCH']
    Code List: ["hasattr(L['x'], '_dynamo_dynamic_indices') == False"]
    Object Weakref: <weakref at 0x7f373de166b0; to 'Tensor' at 0x7f3a64f879c0>
    Guarded Class Weakref: <weakref at 0x7f399617c270; to 'torch._C._TensorMeta' at 0x562aa205b040 (Tensor)>
  Guard 3:
    Name: ''
    Source: global
    Create Function: DETERMINISTIC_ALGORITHMS
    Guard Types: None
    Code List: None
    Object Weakref: None
    Guarded Class Weakref: None
  Guard 4:
    Name: ''
    Source: global
    Create Function: TORCH_FUNCTION_STATE
    Guard Types: None
    Code List: None
    Object Weakref: None
    Guarded Class Weakref: None
  Guard 5:
    Name: ''
    Source: shape_env
    Create Function: SHAPE_ENV
    Guard Types: None
    Code List: None
    Object Weakref: None
    Guarded Class Weakref: None
  Guard 6:
    Name: "G['__builtins_dict___7']['print']"
    Source: global
    Create Function: BUILTIN_MATCH
    Guard Types: ['ID_MATCH']
    Code List: ["___check_obj_id(G['__builtins_dict___7']['print'], 139888780563760)"]
    Object Weakref: <weakref at 0x7f373dcf2cf0; to 'builtin_function_or_method' at 0x7f3a6512d530>
    Guarded Class Weakref: <weakref at 0x7f3a650bdcb0; to 'type' at 0x562a9d5268a0 (builtin_function_or_method)>
  Guard 7:
    Name: ''
    Source: global
    Create Function: DEFAULT_DEVICE
    Guard Types: ['DEFAULT_DEVICE']
    Code List: ['utils_device.CURRENT_DEVICE == None']
    Object Weakref: None
    Guarded Class Weakref: None
  Guard 8:
    Name: "L['b']"
    Source: local
    Create Function: TENSOR_MATCH
    Guard Types: ['TENSOR_MATCH']
    Code List: ["hasattr(L['b'], '_dynamo_dynamic_indices') == False"]
    Object Weakref: <weakref at 0x7f372697a020; dead>
    Guarded Class Weakref: <weakref at 0x7f399617c270; to 'torch._C._TensorMeta' at 0x562aa205b040 (Tensor)>
  Guard 9:
    Name: ''
    Source: global
    Create Function: GRAD_MODE
    Guard Types: None
    Code List: None
    Object Weakref: None
    Guarded Class Weakref: None
  Guard 10:
    Name: ''
    Source: global
    Create Function: DETERMINISTIC_ALGORITHMS
    Guard Types: None
    Code List: None
    Object Weakref: None
    Guarded Class Weakref: None
  Guard 11:
    Name: ''
    Source: global
    Create Function: TORCH_FUNCTION_STATE
    Guard Types: None
    Code List: None
    Object Weakref: None
    Guarded Class Weakref: None
  Guard 12:
    Name: ''
    Source: shape_env
    Create Function: SHAPE_ENV
    Guard Types: None
    Code List: None
    Object Weakref: None
    Guarded Class Weakref: None
  Guard 13:
    Name: ''
    Source: global
    Create Function: DEFAULT_DEVICE
    Guard Types: ['DEFAULT_DEVICE']
    Code List: ['utils_device.CURRENT_DEVICE == None']
    Object Weakref: None
    Guarded Class Weakref: None
Compile Times: TorchDynamo compilation metrics:
Function, Runtimes (s)
_compile.compile_inner, 0.0291, 0.0101
OutputGraph.call_user_compiler, 0.0003, 0.0003
gc, 0.0006, 0.0004
```


### 什么时候需要了解？

- 使用 torch.compile 遇到问题时
- 想理解编译过程
- 需要调试性能问题

---

## 6. AOTAutograd - 自动微分加速

### 它是什么？

想象你在考试：
- **普通方式**：做完一道题对一道答案（运行时算梯度）
- **AOTAutograd**：提前看答案，知道怎么做最快（提前生成反向图）

**一句话总结**：提前计算好反向传播，训练更快。

### 它在后台工作

```python
# 你只需要
model = torch.compile(model)

# AOTAutograd 在后台：
# 1. 分析前向计算
# 2. 自动生成反向传播代码
# 3. 优化前向和反向一起
# 4. 你训练时自动用优化后的版本

# 结果：训练加速 20-40%
```

### 智能内存优化

```python
# 普通 Autograd：保存所有中间值
def forward(x):
    a = expensive_operation(x)  # 保存 a（占内存）
    b = cheap_operation(a)      # 保存 b（占内存）
    c = b + 1                   # 保存 c（占内存）
    return c.sum()

# AOTAutograd 优化：
# - expensive_operation 的结果要保存（重算太贵）
# - cheap_operation 的结果不保存（重算很快）
# - 内存使用降低
```

### 什么时候用？

**自动使用**：当你用 torch.compile 时
**不需要关心细节**：它在后台自动工作

---

## 7. Dispatcher - 多平台支持

### 它是什么？

想象一个智能快递分拣系统：
- 你寄快递（调用 PyTorch 函数）
- 系统自动判断：CPU？GPU？NPU？
- 发到对应的处理中心（对应的实现）

**一句话总结**：让同一个代码在不同设备上自动调用不同的实现。

### 它自动工作

```python
# 同样的代码
x = torch.randn(100)
y = x + 1

# Dispatcher 自动决定：
# - x 在 CPU -> 调用 CPU 版本的加法
# - x 在 GPU -> 调用 GPU 版本的加法
# - x 在 NPU -> 调用 NPU 版本的加法
```

### 更详细的例子

```python
# CPU 上
x_cpu = torch.randn(10)
y_cpu = x_cpu + 1  # Dispatcher: "调用 add_cpu()"

# GPU 上
x_gpu = torch.randn(10, device='cuda')
y_gpu = x_gpu + 1  # Dispatcher: "调用 add_cuda()"

# 假设你有 NPU
x_npu = torch.randn(10, device='npu')
y_npu = x_npu + 1  # Dispatcher: "调用 add_npu()"

# 代码完全一样，Dispatcher 自动路由
```

### 什么时候需要了解？

**适配新硬件时**：
```python
# 如果你要为国产芯片适配 PyTorch
# 需要向 Dispatcher 注册你的实现

# 伪代码
@register_to_dispatcher(device='my_chip', op='add')
def add_my_chip(x, y):
    # 调用你的芯片的加法 API
    return my_chip.add(x, y)

# 之后用户就可以：
x = torch.randn(10, device='my_chip')
y = x + 1  # 自动调用 add_my_chip
```

---

## 快速选择指南

### 我是新手，只想加速模型

```python
# 就用这个，别想太多
model = torch.compile(model)
```

**原因**：
- 最简单（一行代码）
- 效果最好（加速明显）
- 不需要理解原理

### 我要部署到 C++ 服务器

```python
# 使用 TorchScript
scripted = torch.jit.script(model)
scripted.save("model.pt")
```

**原因**：
- C++ 可以加载
- 不需要 Python 环境
- 成熟稳定

### 我要部署到手机 App

```python
# 使用 TorchScript + Mobile
from torch.utils.mobile_optimizer import optimize_for_mobile

scripted = torch.jit.script(model)
mobile = optimize_for_mobile(scripted)
mobile._save_for_lite_interpreter("model_mobile.ptl")
```

### 我要在 TPU 上训练

```python
# 使用 PyTorch/XLA
import torch_xla
device = torch_xla.core.xla_model.xla_device()
model = model.to(device)
```

### 我要自定义优化模型结构

```python
# 使用 TorchFX
import torch.fx as fx

traced = fx.symbolic_trace(model)
# 修改 traced.graph
traced.recompile()
```

### 我要适配国产芯片

需要学习：
1. Dispatcher（算子注册）
2. TorchFX（图优化）
3. 自定义后端开发

---

## 常见问题 FAQ

### Q1: torch.compile 和 TorchScript 选哪个？

**A: 看用途**

```python
# 训练 -> torch.compile
model = torch.compile(model)
train(model)

# C++ 部署 -> TorchScript
scripted = torch.jit.script(model)
scripted.save("model.pt")
```

### Q2: 为什么编译后第一次运行很慢？

**A: 在编译，之后就快了**

```python
model = torch.compile(model)

# 第一次：编译（慢）
output = model(input)  # 5 秒

# 之后：使用编译好的（快）
output = model(input)  # 0.01 秒
```

**解决方法：预热**
```python
_ = model(dummy_input)  # 预热
# 之后正常使用
```

### Q3: 为什么有时候编译后反而更慢？

**A: 可能的原因**

1. **Graph Breaks 太多**
```python
def bad_code(x):
    a = x + 1
    print(a)  # Break
    b = a * 2
    print(b)  # Break
    return b
```

2. **输入形状一直变化**
```python
for size in [16, 32, 64]:  # 每次都重新编译
    output = model(torch.randn(size, 100))
```

**解决方法**
```python
# 1. 减少 Graph Breaks
def good_code(x):
    a = x + 1
    b = a * 2
    return b  # print 放到外面

# 2. 固定输入形状
dataloader = DataLoader(..., batch_size=32, drop_last=True)
```

### Q4: LazyLinear 和普通 Linear 有什么区别？

**A: 不需要指定输入维度**

```python
# 普通 Linear：必须知道输入维度
linear = nn.Linear(512, 256)  # 必须写 512

# LazyLinear：不需要知道
lazy_linear = nn.LazyLinear(256)  # 不写输入维度

# 第一次使用时自动推断
x = torch.randn(32, 512)
output = lazy_linear(x)  # 自动变成 Linear(512, 256)
```

---

## 学习路线建议

### 第 1 天：入门
```python
# 只学这个
model = torch.compile(model)
```
**够用了！**

### 第 1 周：进阶
- 理解为什么 torch.compile 有效
- 学习如何预热
- 了解 Graph Break

### 第 1 月：深入
- 学习 TorchScript（如果需要部署）
- 学习 TorchFX（如果需要自定义优化）

### 第 3 月：专家
- 理解 Dynamo、AOTAutograd 原理
- 学习 Dispatcher 机制
- 可以适配新硬件

---

## 总结

### 记住这三句话

1. **新手用 torch.compile**
   ```python
   model = torch.compile(model)  # 就这一行
   ```

2. **部署用 TorchScript**
   ```python
   torch.jit.script(model).save("model.pt")
   ```

3. **自定义优化用 TorchFX**
   ```python
   traced = torch.fx.symbolic_trace(model)
   # 修改 traced
   ```

### 最重要的

**不要被这么多技术吓到！**
- 大多数情况下，torch.compile 就够了
- 其他技术都是特定场景才需要
- 一步一步学，不要急

### 下一步

- 如果你想快速上手：看第 1 章（PyTorch 2.0）
- 如果你想理解原理：按顺序看完所有章节
- 如果你要适配芯片：重点看 Dispatcher 和自定义后端

**开始学习吧！**
