# ç¬¬å…­ç« ï¼šAOTAutograd è‡ªåŠ¨å¾®åˆ†

## ğŸ¯ æœ¬ç« ç›®æ ‡

- ç†è§£ AOTAutogradï¼ˆAhead-of-Time Autogradï¼‰çš„åŸç†
- æŒæ¡å‰å‘å›¾å’Œåå‘å›¾çš„ç”Ÿæˆæœºåˆ¶
- äº†è§£ä¸ä¼ ç»Ÿ Autograd çš„åŒºåˆ«
- å­¦ä¹ è”åˆç¼–è¯‘ä¼˜åŒ–çš„æŠ€æœ¯

## 1. ä»€ä¹ˆæ˜¯ AOTAutogradï¼Ÿ

### 1.1 ç®€å•ç†è§£

**ä¼ ç»Ÿ Autogradï¼ˆè¿è¡Œæ—¶ï¼‰ï¼š**

```python
# å‰å‘ä¼ æ’­
x = torch.randn(10, requires_grad=True)
y = x * 2
z = y + 1
loss = z.sum()

# åå‘ä¼ æ’­ï¼ˆè¿è¡Œæ—¶åŠ¨æ€æ„å»ºï¼‰
loss.backward()  # â† åœ¨è¿™é‡Œæ‰æ„å»ºåå‘å›¾
```

**AOTAutogradï¼ˆæå‰ç¼–è¯‘ï¼‰ï¼š**

```python
# torch.compile ä½¿ç”¨ AOTAutograd
compiled_fn = torch.compile(model)

# ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶ï¼š
# 1. æ•è·å‰å‘è®¡ç®—å›¾
# 2. æå‰ç”Ÿæˆåå‘è®¡ç®—å›¾ï¼ˆå³ä½¿è¿˜æ²¡ backwardï¼ï¼‰
# 3. è”åˆä¼˜åŒ–å‰å‘+åå‘
# 4. ç¼–è¯‘ä¸ºé«˜æ•ˆä»£ç 
output = compiled_fn(input_data)

# backward æ—¶ç›´æ¥ä½¿ç”¨ç¼–è¯‘å¥½çš„åå‘å›¾
loss.backward()  # å¾ˆå¿«ï¼
```

**ç±»æ¯”ï¼š**
- **ä¼ ç»Ÿ Autograd**ï¼šè¾¹èµ°è¾¹å»ºè·¯ï¼ˆè¿è¡Œæ—¶æ„å»ºï¼‰
- **AOTAutograd**ï¼šæå‰ä¿®å¥½é«˜é€Ÿå…¬è·¯ï¼ˆç¼–è¯‘æ—¶æ„å»ºï¼‰

### 1.2 ä¸ºä»€ä¹ˆéœ€è¦ AOTAutogradï¼Ÿ

**é—®é¢˜ 1ï¼šä¼ ç»Ÿ Autograd çš„å¼€é”€**

```python
# å‰å‘ä¼ æ’­
for i in range(1000):
    y = x + 1           # è®°å½•æ“ä½œ 1
    z = y * 2           # è®°å½•æ“ä½œ 2
    loss = z.mean()     # è®°å½•æ“ä½œ 3
    
    # æ¯æ¬¡éƒ½è¦æ„å»º backward å›¾ï¼
    loss.backward()
    # å¼€é”€ï¼šæ„å»ºå›¾ + æ‰§è¡Œåå‘ä¼ æ’­
```

**AOTAutograd è§£å†³ï¼š**

```python
# åªç¼–è¯‘ä¸€æ¬¡
compiled_fn = torch.compile(lambda x: ((x + 1) * 2).mean())

for i in range(1000):
    loss = compiled_fn(x)
    loss.backward()  # ä½¿ç”¨é¢„ç¼–è¯‘çš„åå‘å›¾ï¼
```

**é—®é¢˜ 2ï¼šæ— æ³•è”åˆä¼˜åŒ–å‰å‘å’Œåå‘**

```python
# å‰å‘æ“ä½œ
y = x + 1      # éœ€è¦ä¿å­˜ x ç”¨äºåå‘
z = y * 2      # éœ€è¦ä¿å­˜ y ç”¨äºåå‘

# åå‘æ“ä½œ
grad_y = grad_z * 2        # åå‘ z
grad_x = grad_y * 1        # åå‘ y
```

å¦‚æœæå‰çŸ¥é“åå‘å›¾ï¼Œå¯ä»¥ï¼š
- ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼ˆä¸ä¿å­˜ä¸å¿…è¦çš„ä¸­é—´å€¼ï¼‰
- èåˆå‰å‘å’Œåå‘æ“ä½œ
- é‡æ–°è®¡ç®—è€Œä¸æ˜¯ä¿å­˜ï¼ˆmemory-efficientï¼‰

## 2. AOTAutograd çš„å·¥ä½œåŸç†

### 2.1 æ•´ä½“æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç”¨æˆ·ä»£ç ï¼šmodel.forward(input)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dynamoï¼šæ•è·å‰å‘è®¡ç®—å›¾                â”‚
â”‚  å¾—åˆ° FX Graphï¼ˆåªæœ‰å‰å‘æ“ä½œï¼‰         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AOTAutogradï¼šåˆ†æå‰å‘å›¾               â”‚ â† æœ¬ç« é‡ç‚¹
â”‚  1. è¯†åˆ«å“ªäº›å€¼éœ€è¦æ¢¯åº¦                 â”‚
â”‚  2. ç”Ÿæˆåå‘ä¼ æ’­å›¾                     â”‚
â”‚  3. è”åˆä¼˜åŒ–å‰å‘+åå‘                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è¾“å‡ºï¼š                                â”‚
â”‚  - ä¼˜åŒ–åçš„å‰å‘å›¾                      â”‚
â”‚  - åå‘å›¾                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Backend Compiler                      â”‚
â”‚  ç¼–è¯‘ä¸ºé«˜æ•ˆä»£ç                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 å‰å‘å›¾åˆ†æ

```python
# ç¤ºä¾‹å‰å‘ä»£ç 
def forward(x):
    y = x * 2           # æ“ä½œ 1
    z = y + 1           # æ“ä½œ 2
    loss = z.mean()     # æ“ä½œ 3
    return loss

# AOTAutograd åˆ†æï¼š
# - x éœ€è¦ requires_grad
# - éœ€è¦ä¿å­˜ä»€ä¹ˆä¸­é—´å€¼ï¼Ÿ
#   - y: åå‘æ—¶éœ€è¦
#   - z: åå‘æ—¶éœ€è¦
#   - x: ä¸éœ€è¦ä¿å­˜ï¼ˆå¯ä»¥ä»è¾“å…¥è·å–ï¼‰
```

### 2.3 åå‘å›¾ç”Ÿæˆ

```python
# æ ¹æ®å‰å‘å›¾ï¼Œè‡ªåŠ¨ç”Ÿæˆåå‘å›¾

# å‰å‘å›¾ï¼š
# x â†’ *2 â†’ y â†’ +1 â†’ z â†’ mean() â†’ loss

# åå‘å›¾ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰ï¼š
# grad_loss = 1
# grad_z = grad_mean(grad_loss, z)       # mean çš„åå‘
# grad_y = grad_z * 1                    # +1 çš„åå‘
# grad_x = grad_y * 2                    # *2 çš„åå‘
```

**å…³é”®ï¼š** è¿™ä¸€åˆ‡åœ¨**ç¬¬ä¸€æ¬¡å‰å‘ä¼ æ’­ä¹‹å‰**å°±å®Œæˆäº†ï¼

## 3. è¯¦ç»†ç¤ºä¾‹

### 3.1 åŸºæœ¬ç¤ºä¾‹

```python
import torch
import torch._functorch.aot_autograd as aot_autograd

def simple_fn(x):
    y = x * 2
    z = y + 1
    return z.sum()

# ä½¿ç”¨ AOTAutograd
x = torch.randn(10, requires_grad=True)

# æ‰‹åŠ¨ä½¿ç”¨ AOTAutogradï¼ˆé€šå¸¸é€šè¿‡ torch.compile è‡ªåŠ¨è°ƒç”¨ï¼‰
from torch._dynamo import optimize
from functorch.compile import aot_function

# å®šä¹‰ä¸€ä¸ªç®€å•çš„"ç¼–è¯‘å™¨"ï¼ˆå®é™…åªæ˜¯æ‰“å°ï¼‰
def print_compiler(fx_module, args):
    print("å‰å‘å›¾:")
    print(fx_module.code)
    return fx_module

# åŒ…è£…å‡½æ•°
aot_fn = aot_function(simple_fn, fw_compiler=print_compiler, bw_compiler=print_compiler)

# æ‰§è¡Œ
output = aot_fn(x)
print(f"\nè¾“å‡º: {output}")

output.backward()
print(f"\næ¢¯åº¦: {x.grad}")
```

### 3.2 æŸ¥çœ‹ç”Ÿæˆçš„å‰å‘å’Œåå‘å›¾

```python
import torch
from torch._functorch.aot_autograd import aot_module_simplified

class SimpleModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.weight = torch.nn.Parameter(torch.randn(10, 10))
    
    def forward(self, x):
        return torch.matmul(x, self.weight).sum()

model = SimpleModel()
input_data = torch.randn(10)

# ä½¿ç”¨ AOT Autograd
def print_graph(name):
    def compiler(fx_graph, args):
        print(f"\n{'='*50}")
        print(f"{name}")
        print(f"{'='*50}")
        print(fx_graph.code)
        return fx_graph
    return compiler

compiled_model = aot_module_simplified(
    model,
    fw_compiler=print_graph("å‰å‘å›¾"),
    bw_compiler=print_graph("åå‘å›¾"),
)

# æ‰§è¡Œ
output = compiled_model(input_data)
output.backward()
```

## 4. è”åˆä¼˜åŒ–

### 4.1 å‰å‘ä¿å­˜ä¼˜åŒ–

```python
# åŸå§‹å‰å‘ä»£ç 
def forward(x):
    a = x + 1
    b = a * 2
    c = b + 3
    d = c * 4
    return d.sum()

# ä¼ ç»Ÿ Autogradï¼šä¿å­˜æ‰€æœ‰ä¸­é—´å€¼
# ä¿å­˜ï¼šx, a, b, cï¼ˆç”¨äºåå‘ä¼ æ’­ï¼‰

# AOTAutograd ä¼˜åŒ–ï¼š
# åˆ†æåå‘å›¾åå‘ç°ï¼š
# - åå‘æ—¶åªéœ€è¦ b å’Œ c
# - a å¯ä»¥ä¸ä¿å­˜ï¼ˆå› ä¸ºåå‘æ—¶ä¸ç›´æ¥ä½¿ç”¨ï¼‰
# åªä¿å­˜ï¼šb, c
```

### 4.2 ç®—å­èåˆï¼ˆå‰å‘+åå‘ï¼‰

```python
# å‰å‘æ“ä½œ
y = x + 1      # Kernel 1
z = y * 2      # Kernel 2

# åå‘æ“ä½œï¼ˆæœªä¼˜åŒ–ï¼‰
grad_y = grad_z * 2    # Kernel 3
grad_x = grad_y * 1    # Kernel 4

# AOTAutograd èåˆä¼˜åŒ–ï¼š
# å‰å‘ï¼šfused_kernel_fw(x)  â†’ ç”Ÿæˆ y, z
# åå‘ï¼šfused_kernel_bw(grad_z) â†’ ç”Ÿæˆ grad_x
# ä» 4 ä¸ª Kernel å‡å°‘åˆ° 2 ä¸ªï¼
```

### 4.3 é‡è®¡ç®—ï¼ˆRematerializationï¼‰

```python
# æƒ…å†µï¼šå‰å‘æ“ä½œä¾¿å®œï¼Œä½†ä¸­é—´å€¼å ç”¨å¤§é‡å†…å­˜

def forward(x):
    # è¿™ä¸ªæ“ä½œå¾ˆä¾¿å®œï¼ˆè®¡ç®—å¿«ï¼‰
    y = torch.sin(x)  
    
    # ä½† y å¯èƒ½å¾ˆå¤§ï¼ˆå å†…å­˜ï¼‰
    # ä¾‹å¦‚ x.shape = (10000, 10000)
    
    z = y * 2
    return z.sum()

# AOTAutograd ç­–ç•¥ï¼š
# - å‰å‘æ—¶ä¸ä¿å­˜ yï¼ˆèŠ‚çœå†…å­˜ï¼‰
# - åå‘æ—¶é‡æ–°è®¡ç®— y = torch.sin(x)
# 
# æƒè¡¡ï¼šç”¨è®¡ç®—æ¢å†…å­˜
```

**è‡ªåŠ¨é‡è®¡ç®—ç­–ç•¥ï¼š**

```python
# PyTorch ä¼šè‡ªåŠ¨å†³å®šå“ªäº›ä¸­é—´å€¼åº”è¯¥é‡è®¡ç®—
# å†³ç­–å› ç´ ï¼š
# 1. è®¡ç®—æˆæœ¬ vs å†…å­˜å ç”¨
# 2. æ˜¯å¦åœ¨åå‘ä¼ æ’­ä¸­ä½¿ç”¨
# 3. é‡è®¡ç®—çš„ä¾èµ–å…³ç³»
```

## 5. æºç åˆ†æ

### 5.1 æ ¸å¿ƒæ–‡ä»¶ä½ç½®

```
pytorch/torch/_functorch/
â”œâ”€â”€ aot_autograd.py          # AOTAutograd æ ¸å¿ƒå®ç°
â”œâ”€â”€ partitioners.py          # å›¾åˆ†å‰²ï¼ˆå‰å‘/åå‘åˆ†ç¦»ï¼‰
â”œâ”€â”€ compile_utils.py         # ç¼–è¯‘å·¥å…·
â””â”€â”€ compilers.py             # å†…ç½®ç¼–è¯‘å™¨

pytorch/torch/_inductor/
â””â”€â”€ fx_passes/
    â””â”€â”€ joint_graph.py       # è”åˆå›¾ä¼˜åŒ–
```

### 5.2 å…³é”®å‡½æ•°

**aot_autograd.py ä¸­çš„æ ¸å¿ƒå‡½æ•°ï¼š**

```python
def aot_function(
    fn,
    fw_compiler=None,   # å‰å‘ç¼–è¯‘å™¨
    bw_compiler=None,   # åå‘ç¼–è¯‘å™¨
    partition_fn=None,  # å›¾åˆ†å‰²å‡½æ•°
    decompositions=None,
):
    """
    AOTAutograd çš„å…¥å£å‡½æ•°
    
    å·¥ä½œæµç¨‹ï¼š
    1. è°ƒç”¨ fnï¼Œè¿½è¸ªå‰å‘å›¾
    2. åˆ†æå“ªäº›tensoréœ€è¦æ¢¯åº¦
    3. è‡ªåŠ¨ç”Ÿæˆåå‘å›¾
    4. åˆ†åˆ«ç¼–è¯‘å‰å‘å’Œåå‘
    """
    
    def wrapper(*args):
        # è¿½è¸ªå‰å‘å›¾
        with torch.enable_grad():
            fw_graph = make_fx(fn)(*args)
        
        # ç”Ÿæˆåå‘å›¾
        joint_graph = create_joint_forward_backward(fw_graph)
        
        # åˆ†å‰²å‰å‘å’Œåå‘
        fw_module, bw_module = partition_fn(joint_graph)
        
        # ç¼–è¯‘
        compiled_fw = fw_compiler(fw_module)
        compiled_bw = bw_compiler(bw_module)
        
        return compiled_fw, compiled_bw
    
    return wrapper
```

### 5.3 åå‘å›¾ç”Ÿæˆ

```python
# ç®€åŒ–çš„ä¼ªä»£ç 

def create_backward_graph(forward_graph, outputs_need_grad):
    """
    ä»å‰å‘å›¾åˆ›å»ºåå‘å›¾
    """
    backward_graph = Graph()
    
    # 1. éå†å‰å‘å›¾ï¼ˆåå‘é¡ºåºï¼‰
    for node in reversed(forward_graph.nodes):
        if node.op == 'call_function':
            # 2. è·å–è¯¥æ“ä½œçš„åå‘è§„åˆ™
            backward_rule = get_backward_rule(node.target)
            
            # 3. æ·»åŠ åå‘èŠ‚ç‚¹
            grad_inputs = backward_rule(
                grad_output=node.grad,
                forward_inputs=node.args,
                forward_output=node.result
            )
            
            # 4. è¿æ¥åˆ°åå‘å›¾
            for inp, grad_inp in zip(node.args, grad_inputs):
                backward_graph.add_edge(node, inp, grad_inp)
    
    return backward_graph
```

## 6. ä¸ä¼ ç»Ÿ Autograd å¯¹æ¯”

### 6.1 ä¼ ç»Ÿ Autograd

```python
# è¿è¡Œæ—¶æ„å»ºåå‘å›¾

x = torch.randn(10, requires_grad=True)

# å‰å‘ä¼ æ’­
y = x * 2
# â†’ åˆ›å»º MulBackward èŠ‚ç‚¹
# â†’ ä¿å­˜å¿…è¦ä¿¡æ¯ï¼ˆx, å¸¸æ•°2ï¼‰

z = y + 1
# â†’ åˆ›å»º AddBackward èŠ‚ç‚¹
# â†’ é“¾æ¥åˆ° MulBackward

loss = z.sum()
# â†’ åˆ›å»º SumBackward èŠ‚ç‚¹
# â†’ é“¾æ¥åˆ° AddBackward

# åå‘ä¼ æ’­
loss.backward()
# â†’ éå†å·²æ„å»ºçš„åå‘å›¾
# â†’ è®¡ç®—æ¢¯åº¦
```

**ç‰¹ç‚¹ï¼š**
- âœ… çµæ´»ï¼šæ”¯æŒä»»æ„åŠ¨æ€å›¾
- âŒ å¼€é”€ï¼šæ¯æ¬¡éƒ½è¦æ„å»ºåå‘å›¾
- âŒ éš¾ä¼˜åŒ–ï¼šè¿è¡Œæ—¶æ‰çŸ¥é“åå‘å›¾ç»“æ„

### 6.2 AOTAutograd

```python
# ç¼–è¯‘æ—¶æ„å»ºåå‘å›¾

def compute(x):
    y = x * 2
    z = y + 1
    return z.sum()

# ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆç¼–è¯‘ï¼‰
compiled_compute = torch.compile(compute)

x = torch.randn(10, requires_grad=True)
loss = compiled_compute(x)  
# â†‘ åœ¨è¿™é‡Œå°±å·²ç»ç”Ÿæˆäº†å®Œæ•´çš„åå‘å›¾ï¼

# åå‘ä¼ æ’­ï¼ˆä½¿ç”¨é¢„ç¼–è¯‘çš„åå‘å›¾ï¼‰
loss.backward()  # å¾ˆå¿«ï¼
```

**ç‰¹ç‚¹ï¼š**
- âœ… å¿«é€Ÿï¼šä½¿ç”¨é¢„ç¼–è¯‘çš„åå‘å›¾
- âœ… å¯ä¼˜åŒ–ï¼šæå‰çŸ¥é“å®Œæ•´çš„å‰å‘+åå‘ç»“æ„
- âš ï¸ ç›¸å¯¹ä¸çµæ´»ï¼šéœ€è¦å›ºå®šçš„è®¡ç®—æ¨¡å¼

### 6.3 å¯¹æ¯”è¡¨

| ç»´åº¦ | ä¼ ç»Ÿ Autograd | AOTAutograd |
|------|--------------|-------------|
| **å›¾æ„å»ºæ—¶æœº** | è¿è¡Œæ—¶ï¼ˆæ¯æ¬¡å‰å‘ï¼‰ | ç¼–è¯‘æ—¶ï¼ˆä¸€æ¬¡ï¼‰ |
| **åå‘å›¾ç”Ÿæˆ** | å‰å‘æ—¶è®°å½• | ç¼–è¯‘æ—¶æå‰ç”Ÿæˆ |
| **ä¼˜åŒ–ç©ºé—´** | æœ‰é™ | å¤§ï¼ˆè”åˆä¼˜åŒ–ï¼‰ |
| **æ€§èƒ½** | åŸºå‡† | æ›´å¿«ï¼ˆ20-40%ï¼‰ |
| **å†…å­˜ä¼˜åŒ–** | åŸºæœ¬ | é«˜çº§ï¼ˆé‡è®¡ç®—ç­‰ï¼‰ |
| **çµæ´»æ€§** | é«˜ | ä¸­ç­‰ï¼ˆéœ€Guardsï¼‰ |

## 7. å®é™…åº”ç”¨

### 7.1 è®­ç»ƒåŠ é€Ÿ

```python
import torch
import torch.nn as nn

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(1024, 2048),
            nn.ReLU(),
            nn.Linear(2048, 2048),
            nn.ReLU(),
            nn.Linear(2048, 10)
        )
    
    def forward(self, x):
        return self.layers(x)

model = Model().cuda()

# ä½¿ç”¨ torch.compileï¼ˆå†…éƒ¨ä½¿ç”¨ AOTAutogradï¼‰
model = torch.compile(model)

# è®­ç»ƒå¾ªç¯
optimizer = torch.optim.Adam(model.parameters())
for batch in dataloader:
    x, y = batch
    x, y = x.cuda(), y.cuda()
    
    # å‰å‘ï¼ˆä½¿ç”¨ä¼˜åŒ–çš„å‰å‘å›¾ï¼‰
    pred = model(x)
    loss = nn.functional.cross_entropy(pred, y)
    
    # åå‘ï¼ˆä½¿ç”¨é¢„ç¼–è¯‘çš„åå‘å›¾ï¼‰
    loss.backward()
    
    optimizer.step()
    optimizer.zero_grad()
```

**æ€§èƒ½æå‡ï¼š**
- å‰å‘ä¼ æ’­ï¼š~30% åŠ é€Ÿ
- åå‘ä¼ æ’­ï¼š~40% åŠ é€Ÿ
- æ€»è®­ç»ƒæ—¶é—´ï¼š~35% å‡å°‘

### 7.2 å†…å­˜ä¼˜åŒ–ç¤ºä¾‹

```python
# å¤§æ¨¡å‹è®­ç»ƒæ—¶çš„å†…å­˜ä¼˜åŒ–

class LargeModel(nn.Module):
    def forward(self, x):
        # å¤šä¸ªå¤§çš„ä¸­é—´æ¿€æ´»å€¼
        x1 = self.layer1(x)      # å¤§
        x2 = self.layer2(x1)     # å¤§
        x3 = self.layer3(x2)     # å¤§
        x4 = self.layer4(x3)     # å¤§
        return x4

# torch.compile ä¼šè‡ªåŠ¨ï¼š
# 1. åˆ†æå“ªäº›ä¸­é—´å€¼å¿…é¡»ä¿å­˜
# 2. å¯¹äºä¾¿å®œçš„æ“ä½œï¼Œä½¿ç”¨é‡è®¡ç®—
# 3. ä¼˜åŒ–å†…å­˜å¸ƒå±€

model = torch.compile(LargeModel())
```

## 8. é«˜çº§ç‰¹æ€§

### 8.1 è‡ªå®šä¹‰åˆ†åŒºç­–ç•¥

```python
from torch._functorch.aot_autograd import aot_module_simplified

def custom_partition(joint_graph):
    """
    è‡ªå®šä¹‰å¦‚ä½•åˆ†å‰²å‰å‘å’Œåå‘å›¾
    """
    # åˆ†æå›¾
    fw_nodes = []
    bw_nodes = []
    
    for node in joint_graph.nodes:
        if is_forward_node(node):
            fw_nodes.append(node)
        else:
            bw_nodes.append(node)
    
    # åˆ›å»ºåˆ†ç¦»çš„å›¾
    fw_graph = create_graph(fw_nodes)
    bw_graph = create_graph(bw_nodes)
    
    return fw_graph, bw_graph

# ä½¿ç”¨è‡ªå®šä¹‰åˆ†åŒº
model = aot_module_simplified(
    model,
    partition_fn=custom_partition
)
```

### 8.2 æŸ¥çœ‹ä¿å­˜çš„ä¸­é—´å€¼

```python
import torch
from torch.utils._python_dispatch import TorchDispatchMode

class DebugSavedTensors(TorchDispatchMode):
    def __torch_dispatch__(self, func, types, args=(), kwargs=None):
        print(f"æ“ä½œ: {func.__name__}")
        result = func(*args, **(kwargs or {}))
        
        # æ£€æŸ¥æ˜¯å¦ä¿å­˜äº†tensor
        if hasattr(result, '_saved_tensors'):
            print(f"  ä¿å­˜çš„tensors: {len(result._saved_tensors)}")
        
        return result

# ä½¿ç”¨
with DebugSavedTensors():
    x = torch.randn(10, requires_grad=True)
    y = x * 2
    z = y + 1
    loss = z.sum()
    loss.backward()
```

### 8.3 æ§åˆ¶é‡è®¡ç®—ç­–ç•¥

```python
# æ‰‹åŠ¨æ ‡è®°å“ªäº›æ“ä½œåº”è¯¥é‡è®¡ç®—

import torch
from torch.utils.checkpoint import checkpoint

class ModelWithCheckpoint(nn.Module):
    def forward(self, x):
        # è¿™éƒ¨åˆ†ä¼šè¢«é‡è®¡ç®—ï¼ˆä¸ä¿å­˜æ¿€æ´»å€¼ï¼‰
        x = checkpoint(self.expensive_layer1, x)
        x = checkpoint(self.expensive_layer2, x)
        
        # è¿™éƒ¨åˆ†æ­£å¸¸ä¿å­˜
        x = self.cheap_layer(x)
        return x

# torch.compile ä¼šè¯†åˆ« checkpoint å¹¶ä¼˜åŒ–
model = torch.compile(ModelWithCheckpoint())
```

## 9. è°ƒè¯•æŠ€å·§

### 9.1 æŸ¥çœ‹ç¼–è¯‘åçš„ä»£ç 

```python
import torch
from torch._inductor import config

# ä¿å­˜ç”Ÿæˆçš„ä»£ç 
config.debug = True
config.trace.enabled = True

model = torch.compile(model)
output = model(input_data)

# æŸ¥çœ‹ç”Ÿæˆçš„å‰å‘å’Œåå‘ä»£ç 
# ä¼šä¿å­˜åœ¨ /tmp/torchinductor_<user>/ ç›®å½•
```

### 9.2 å¯¹æ¯”ä¼˜åŒ–æ•ˆæœ

```python
import torch
import time

model = Model()
x = torch.randn(32, 1024, requires_grad=True)

# 1. ä¸ä½¿ç”¨ AOTAutograd
model_eager = model
times_eager = []
for _ in range(100):
    start = time.time()
    loss = model_eager(x).sum()
    loss.backward()
    times_eager.append(time.time() - start)

# 2. ä½¿ç”¨ AOTAutograd
model_compiled = torch.compile(model)
times_compiled = []
for _ in range(100):
    start = time.time()
    loss = model_compiled(x).sum()
    loss.backward()
    times_compiled.append(time.time() - start)

print(f"Eager: {sum(times_eager)/len(times_eager)*1000:.2f}ms")
print(f"Compiled: {sum(times_compiled)/len(times_compiled)*1000:.2f}ms")
print(f"åŠ é€Ÿæ¯”: {sum(times_eager)/sum(times_compiled):.2f}x")
```

## 10. å°ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **AOTAutograd = æå‰ç¼–è¯‘çš„è‡ªåŠ¨å¾®åˆ†**
2. **æ ¸å¿ƒä¼˜åŠ¿ï¼š**
   - ç¼–è¯‘æ—¶ç”Ÿæˆåå‘å›¾ï¼ˆè€Œéè¿è¡Œæ—¶ï¼‰
   - è”åˆä¼˜åŒ–å‰å‘å’Œåå‘
   - æ™ºèƒ½çš„å†…å­˜ç®¡ç†å’Œé‡è®¡ç®—
3. **å…³é”®æŠ€æœ¯ï¼š**
   - å‰å‘å›¾åˆ†æ
   - è‡ªåŠ¨åå‘å›¾ç”Ÿæˆ
   - ç®—å­èåˆ
   - é‡è®¡ç®—ç­–ç•¥
4. **æ€§èƒ½æå‡ï¼š** 20-40% çš„è®­ç»ƒåŠ é€Ÿ

### åœ¨ torch.compile ä¸­çš„è§’è‰²

```
torch.compile çš„ä¸‰å±‚æ¶æ„ï¼š
1. Dynamoï¼šæ•è·å‰å‘å›¾
2. AOTAutogradï¼šç”Ÿæˆåå‘å›¾ï¼Œè”åˆä¼˜åŒ– â† æœ¬ç« 
3. Inductorï¼šä»£ç ç”Ÿæˆå’Œæ‰§è¡Œ
```

### ä¸å…¶ä»–æŠ€æœ¯çš„å…³ç³»

- **vs ä¼ ç»Ÿ Autograd**ï¼šæå‰ç¼–è¯‘ vs è¿è¡Œæ—¶æ„å»º
- **ä¾èµ– TorchFX**ï¼šä½¿ç”¨ FX IR è¡¨ç¤ºå‰å‘å’Œåå‘å›¾
- **æ”¯æŒ Dynamo**ï¼šæ¥æ”¶ Dynamo æ•è·çš„å›¾

### ä¸‹ä¸€æ­¥

ç°åœ¨ä½ ç†è§£äº†å¦‚ä½•ä¼˜åŒ–å‰å‘å’Œåå‘ä¼ æ’­ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å­¦ä¹  PyTorch çš„åº•å±‚åˆ†å‘æœºåˆ¶ï¼š

ğŸ“– [ç¬¬ä¸ƒç« ï¼šPyTorch Dispatcher åˆ†å‘æœºåˆ¶](./07_pytorch_dispatch.md) - ç®—å­å¦‚ä½•è·¯ç”±åˆ°ä¸åŒçš„ç¡¬ä»¶åç«¯

---

## ğŸ’¡ ç»ƒä¹ é¢˜

1. ç”¨ `aot_function` åŒ…è£…ä¸€ä¸ªç®€å•å‡½æ•°ï¼Œæ‰“å°å‰å‘å’Œåå‘å›¾
2. å¯¹æ¯”æœ‰æ—  AOTAutograd çš„è®­ç»ƒé€Ÿåº¦
3. åˆ†æä¸€ä¸ªæ¨¡å‹åº”è¯¥ä¿å­˜å“ªäº›ä¸­é—´å€¼ç”¨äºåå‘ä¼ æ’­

**ç»§ç»­å­¦ä¹ ** â†’ [PyTorch Dispatcher åˆ†å‘æœºåˆ¶](./07_pytorch_dispatch.md)

