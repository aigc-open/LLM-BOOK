# ç¬¬ä¸ƒç« ï¼šPyTorch Dispatcher åˆ†å‘æœºåˆ¶

## ğŸ¯ æœ¬ç« ç›®æ ‡

- ç†è§£ PyTorch Dispatcher çš„æ¶æ„è®¾è®¡
- æŒæ¡å¤šåç«¯æ”¯æŒçš„åŸç†
- å­¦ä¹  DispatchKey æœºåˆ¶
- äº†è§£ç®—å­æ³¨å†Œå’Œè‡ªå®šä¹‰ç®—å­å®ç°

## 1. ä»€ä¹ˆæ˜¯ Dispatcherï¼Ÿ

### 1.1 ç®€å•ç†è§£

**é—®é¢˜ï¼šåŒä¸€ä¸ªæ“ä½œï¼Œä¸åŒçš„ç¡¬ä»¶**

```python
import torch

x = torch.randn(10)

# åŒæ ·æ˜¯åŠ æ³•æ“ä½œï¼š
y1 = x + 1  # CPU ä¸Šæ‰§è¡Œ
y2 = x.cuda() + 1  # GPU ä¸Šæ‰§è¡Œ
y3 = x.to('mps') + 1  # Apple M1/M2 GPU ä¸Šæ‰§è¡Œ

# é—®é¢˜ï¼šPyTorch å¦‚ä½•çŸ¥é“è°ƒç”¨å“ªä¸ªå®ç°ï¼Ÿ
```

**Dispatcher = ä¸­å¤®è°ƒåº¦ç³»ç»Ÿ**

```
ç”¨æˆ·è°ƒç”¨: x + 1
    â†“
Dispatcher (è°ƒåº¦ä¸­å¿ƒ)
    â†“
æ ¹æ®ä»¥ä¸‹ä¿¡æ¯å†³å®šè°ƒç”¨å“ªä¸ªå®ç°ï¼š
- Tensor åœ¨å“ªä¸ªè®¾å¤‡ï¼Ÿ(CPU/CUDA/MPS)
- æ˜¯å¦éœ€è¦æ¢¯åº¦ï¼Ÿ(requires_grad)
- æ˜¯å¦åœ¨è¿½è¸ªï¼Ÿ(torch.jit)
- å…¶ä»–ç‰¹æ®Šæ¨¡å¼ï¼Ÿ(vmap, grad)
    â†“
è°ƒç”¨å¯¹åº”çš„ kernel å®ç°
```

### 1.2 ä¸ºä»€ä¹ˆéœ€è¦ Dispatcherï¼Ÿ

**æŒ‘æˆ˜ 1ï¼šå¤šç§è®¾å¤‡æ”¯æŒ**

```python
# PyTorch æ”¯æŒçš„åç«¯ï¼š
- CPU
- CUDA (NVIDIA GPU)
- ROCm (AMD GPU)  
- MPS (Apple Silicon)
- XLA (TPU)
- Vulkan (ç§»åŠ¨ç«¯)
- ... è¿˜åœ¨å¢åŠ 
```

**æŒ‘æˆ˜ 2ï¼šå¤šç§æ‰§è¡Œæ¨¡å¼**

```python
# åŒä¸€ä¸ªæ“ä½œï¼Œä¸åŒæ¨¡å¼ä¸‹è¡Œä¸ºä¸åŒï¼š

# æ­£å¸¸æ¨¡å¼
y = x + 1  # æ‰§è¡ŒåŠ æ³•

# Autograd æ¨¡å¼
y = x + 1  # æ‰§è¡ŒåŠ æ³• + è®°å½•åå‘ä¼ æ’­ä¿¡æ¯

# TorchScript æ¨¡å¼
y = x + 1  # è®°å½•åˆ°è®¡ç®—å›¾

# Vmap æ¨¡å¼
y = vmap(lambda x: x + 1)(x)  # æ‰¹é‡æ˜ å°„

# FakeTensor æ¨¡å¼ï¼ˆç”¨äºå…ƒä¿¡æ¯æ¨å¯¼ï¼‰
y = x + 1  # ä¸æ‰§è¡Œå®é™…è®¡ç®—ï¼Œåªæ¨å¯¼å½¢çŠ¶å’Œç±»å‹
```

**Dispatcher ç»Ÿä¸€ç®¡ç†è¿™ä¸€åˆ‡ï¼**

## 2. Dispatcher çš„æ¶æ„

### 2.1 æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Python API                          â”‚
â”‚         torch.add(x, y)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ATen (PyTorch C++ API)              â”‚
â”‚         at::add(x, y)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Dispatcher (è°ƒåº¦ä¸­å¿ƒ)               â”‚ â† æœ¬ç« é‡ç‚¹
â”‚         - åˆ†æ DispatchKey                  â”‚
â”‚         - é€‰æ‹©æ­£ç¡®çš„ kernel                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
        â†“               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CPU Kernel  â”‚  â”‚  CUDA Kernel â”‚  ...
â”‚  add_cpu()   â”‚  â”‚  add_cuda()  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 DispatchKeyï¼ˆè°ƒåº¦é”®ï¼‰

**DispatchKey = å†³å®šè°ƒåº¦çš„å…³é”®ä¿¡æ¯**

```cpp
// ç®€åŒ–çš„ DispatchKey æšä¸¾
enum class DispatchKey {
    CPU,                 // CPU åç«¯
    CUDA,                // CUDA åç«¯
    MPS,                 // Apple Silicon
    
    AutogradCPU,         // CPU + Autograd
    AutogradCUDA,        // CUDA + Autograd
    
    BackendSelect,       // é€‰æ‹©åç«¯
    ADInplaceOrView,     // Autograd çš„ inplace å¤„ç†
    
    Autocast,            // è‡ªåŠ¨æ··åˆç²¾åº¦
    Functionalize,       // å‡½æ•°åŒ–ï¼ˆç§»é™¤ inplaceï¼‰
    
    Python,              // Python è°ƒåº¦ï¼ˆ__torch_dispatch__ï¼‰
    
    // ... è¿˜æœ‰å¾ˆå¤š
};
```

### 2.3 DispatchKey çš„ä¼˜å…ˆçº§

Dispatcher æŒ‰ç…§ç‰¹å®šçš„ä¼˜å…ˆçº§é¡ºåºæ£€æŸ¥ DispatchKeyï¼š

```
ä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼š

1. Python               (Python è‡ªå®šä¹‰è°ƒåº¦)
2. Functionalize        (å‡½æ•°åŒ–è½¬æ¢)
3. ADInplaceOrView      (Autograd inplace å¤„ç†)
4. AutogradCPU/CUDA     (è‡ªåŠ¨å¾®åˆ†)
5. Autocast             (è‡ªåŠ¨æ··åˆç²¾åº¦)
6. BackendSelect        (é€‰æ‹©åç«¯)
7. CPU/CUDA/...         (å®é™…è®¡ç®— kernel)
```

**ç¤ºä¾‹æµç¨‹ï¼š**

```python
x = torch.randn(10, device='cuda', requires_grad=True)
y = x + 1

# Dispatcher æ£€æŸ¥é¡ºåºï¼š
# 1. Python dispatch? â†’ å¦
# 2. Functionalize? â†’ å¦
# 3. ADInplaceOrView? â†’ å¦
# 4. AutogradCUDA? â†’ æ˜¯ï¼æ‰§è¡Œ Autograd é€»è¾‘
#    â†“
# 5. è®°å½•åå‘ä¼ æ’­ä¿¡æ¯
# 6. ç»§ç»­è°ƒåº¦åˆ°åº•å±‚ kernel
#    â†“
# 7. BackendSelect? â†’ é€‰æ‹© CUDA
# 8. CUDA kernel? â†’ æ˜¯ï¼æ‰§è¡Œ add_cuda_kernel()
```

## 3. ç®—å­æ³¨å†Œ

### 3.1 å¦‚ä½•æ³¨å†Œç®—å­

**C++ ä¾§æ³¨å†Œï¼š**

```cpp
// åœ¨ pytorch/aten/src/ATen/native/BinaryOps.cpp

// 1. å®šä¹‰ CPU å®ç°
Tensor add_cpu(const Tensor& self, const Tensor& other, const Scalar& alpha) {
    // CPU å®ç°ä»£ç 
    return result;
}

// 2. å®šä¹‰ CUDA å®ç°
Tensor add_cuda(const Tensor& self, const Tensor& other, const Scalar& alpha) {
    // CUDA å®ç°ä»£ç 
    return result;
}

// 3. æ³¨å†Œç®—å­
TORCH_LIBRARY_IMPL(aten, CPU, m) {
    m.impl("add.Tensor", &add_cpu);
}

TORCH_LIBRARY_IMPL(aten, CUDA, m) {
    m.impl("add.Tensor", &add_cuda);
}
```

**Python ä¾§æŸ¥çœ‹ï¼š**

```python
import torch

# æŸ¥çœ‹ç®—å­çš„æ³¨å†Œä¿¡æ¯
print(torch._C._dispatch_dump("aten::add"))

# è¾“å‡ºç±»ä¼¼ï¼š
# aten::add.Tensor
#   CPU: registered
#   CUDA: registered
#   AutogradCPU: registered
#   AutogradCUDA: registered
#   ...
```

### 3.2 ç®—å­å®šä¹‰

**åœ¨ native_functions.yaml ä¸­å®šä¹‰ï¼š**

```yaml
# pytorch/aten/src/ATen/native/native_functions.yaml

- func: add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
  structured_delegate: add.out
  variants: function, method
  dispatch:
    CPU: add_cpu
    CUDA: add_cuda
    MPS: add_mps
  tags: [core]
```

è¿™ä¸ª YAML æ–‡ä»¶æ˜¯ PyTorch ç®—å­çš„"å­—å…¸"ï¼š
- å®šä¹‰ç®—å­çš„ç­¾å
- æŒ‡å®šå„ä¸ªåç«¯çš„å®ç°
- è‡ªåŠ¨ç”Ÿæˆåˆ†å‘ä»£ç 

## 4. è‡ªå®šä¹‰ç®—å­å’Œåç«¯

### 4.1 Python å±‚è‡ªå®šä¹‰ç®—å­

```python
import torch
from torch import Tensor

# ä½¿ç”¨ torch.library æ³¨å†Œè‡ªå®šä¹‰ç®—å­
torch.library.define(
    "mylib::custom_add",
    "(Tensor x, Tensor y) -> Tensor"
)

# CPU å®ç°
@torch.library.impl("mylib::custom_add", "CPU")
def custom_add_cpu(x: Tensor, y: Tensor) -> Tensor:
    print("ä½¿ç”¨ CPU å®ç°")
    return x + y

# CUDA å®ç°
@torch.library.impl("mylib::custom_add", "CUDA")
def custom_add_cuda(x: Tensor, y: Tensor) -> Tensor:
    print("ä½¿ç”¨ CUDA å®ç°")
    return x + y

# ä½¿ç”¨è‡ªå®šä¹‰ç®—å­
x_cpu = torch.randn(10)
y_cpu = torch.randn(10)
result_cpu = torch.ops.mylib.custom_add(x_cpu, y_cpu)  # "ä½¿ç”¨ CPU å®ç°"

x_cuda = torch.randn(10, device='cuda')
y_cuda = torch.randn(10, device='cuda')
result_cuda = torch.ops.mylib.custom_add(x_cuda, y_cuda)  # "ä½¿ç”¨ CUDA å®ç°"
```

### 4.2 ä½¿ç”¨ __torch_dispatch__

**æ›´é«˜å±‚çš„ Python æ‹¦æˆªï¼š**

```python
import torch
from torch.utils._python_dispatch import TorchDispatchMode

class LoggingMode(TorchDispatchMode):
    """è®°å½•æ‰€æœ‰ PyTorch æ“ä½œ"""
    
    def __torch_dispatch__(self, func, types, args=(), kwargs=None):
        print(f"è°ƒç”¨: {func.__name__}")
        print(f"  è¾“å…¥å½¢çŠ¶: {[a.shape if isinstance(a, torch.Tensor) else a for a in args]}")
        
        # è°ƒç”¨åŸå§‹å®ç°
        result = func(*args, **(kwargs or {}))
        
        print(f"  è¾“å‡ºå½¢çŠ¶: {result.shape if isinstance(result, torch.Tensor) else result}")
        return result

# ä½¿ç”¨
with LoggingMode():
    x = torch.randn(3, 4)
    y = x + 1
    z = y * 2

# è¾“å‡ºï¼š
# è°ƒç”¨: add
#   è¾“å…¥å½¢çŠ¶: [(3, 4), 1]
#   è¾“å‡ºå½¢çŠ¶: (3, 4)
# è°ƒç”¨: mul
#   è¾“å…¥å½¢çŠ¶: [(3, 4), 2]
#   è¾“å‡ºå½¢çŠ¶: (3, 4)
```

### 4.3 è‡ªå®šä¹‰ Tensor å­ç±»

```python
import torch

class MyTensor(torch.Tensor):
    """è‡ªå®šä¹‰ Tensor ç±»"""
    
    @staticmethod
    def __new__(cls, data):
        return torch.Tensor._make_subclass(cls, data)
    
    def __repr__(self):
        return f"MyTensor({super().__repr__()})"
    
    @classmethod
    def __torch_dispatch__(cls, func, types, args=(), kwargs=None):
        """æ‹¦æˆªæ‰€æœ‰æ“ä½œ"""
        
        def unwrap(x):
            return x.data if isinstance(x, MyTensor) else x
        
        def wrap(x):
            return MyTensor(x) if isinstance(x, torch.Tensor) else x
        
        # è§£åŒ…
        args = tuple(unwrap(a) for a in args)
        kwargs = {k: unwrap(v) for k, v in (kwargs or {}).items()}
        
        # æ‰§è¡ŒåŸå§‹æ“ä½œ
        result = func(*args, **kwargs)
        
        # åŒ…è£…å› MyTensor
        return wrap(result)

# ä½¿ç”¨
x = MyTensor(torch.randn(3, 4))
y = x + 1  # è‡ªåŠ¨æ‹¦æˆªå¹¶åŒ…è£…ç»“æœ
print(type(y))  # <class '__main__.MyTensor'>
```

## 5. é‡è¦çš„ DispatchKey

### 5.1 Autograd Keys

```python
# AutogradCPU, AutogradCUDA ç­‰
# ä½œç”¨ï¼šè‡ªåŠ¨è®°å½•æ¢¯åº¦ä¿¡æ¯

x = torch.randn(10, requires_grad=True)
y = x * 2  # è§¦å‘ AutogradCPU dispatch

# Dispatcher æµç¨‹ï¼š
# 1. æ£€æµ‹åˆ° requires_grad=True
# 2. ä½¿ç”¨ AutogradCPU key
# 3. è®°å½•åå‘ä¼ æ’­ä¿¡æ¯
# 4. ç»§ç»­è°ƒåº¦åˆ° CPU kernel æ‰§è¡Œå®é™…è®¡ç®—
```

### 5.2 BackendSelect Key

```python
# ä½œç”¨ï¼šæ ¹æ®è¾“å…¥ tensor çš„è®¾å¤‡é€‰æ‹©åç«¯

def example(x, y):
    return x + y

# x å’Œ y éƒ½åœ¨ CPU â†’ é€‰æ‹© CPU backend
result1 = example(torch.randn(10), torch.randn(10))

# x å’Œ y éƒ½åœ¨ CUDA â†’ é€‰æ‹© CUDA backend
result2 = example(torch.randn(10, device='cuda'), torch.randn(10, device='cuda'))

# x å’Œ y åœ¨ä¸åŒè®¾å¤‡ â†’ é”™è¯¯ï¼
# result3 = example(torch.randn(10), torch.randn(10, device='cuda'))  # æŠ¥é”™
```

### 5.3 Python Key

```python
# ä½œç”¨ï¼šå…è®¸ Python ä»£ç æ‹¦æˆªè°ƒåº¦

class CustomTensor(torch.Tensor):
    @classmethod
    def __torch_dispatch__(cls, func, types, args=(), kwargs=None):
        print(f"Python dispatch: {func}")
        return func(*args, **(kwargs or {}))

# ä½¿ç”¨ Python Key æ‹¦æˆª
```

### 5.4 FakeTensor Key

```python
# ä½œç”¨ï¼šä¸æ‰§è¡Œå®é™…è®¡ç®—ï¼Œåªæ¨å¯¼å…ƒä¿¡æ¯ï¼ˆå½¢çŠ¶ã€dtype ç­‰ï¼‰

from torch._subclasses.fake_tensor import FakeTensorMode

with FakeTensorMode():
    x = torch.randn(100, 100)  # ä¸åˆ†é…å®é™…å†…å­˜
    y = x @ x                   # ä¸æ‰§è¡Œå®é™…è®¡ç®—
    print(y.shape)              # ä½†çŸ¥é“è¾“å‡ºå½¢çŠ¶ï¼š(100, 100)

# ç”¨äºç¼–è¯‘æ—¶çš„å½¢çŠ¶æ¨å¯¼ï¼Œä¸éœ€è¦å®é™…æ•°æ®
```

## 6. Dispatcher çš„é«˜çº§ç‰¹æ€§

### 6.1 ç®—å­ç»„åˆï¼ˆDecompositionï¼‰

```python
# å¤æ‚ç®—å­å¯ä»¥åˆ†è§£ä¸ºç®€å•ç®—å­

# ä¾‹å¦‚ï¼šgelu å¯ä»¥åˆ†è§£ä¸º
def gelu_decomposition(x):
    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))

# åœ¨ Dispatcher ä¸­æ³¨å†Œåˆ†è§£è§„åˆ™
# æŸäº›åç«¯å¦‚æœæ²¡æœ‰åŸç”Ÿ gelu å®ç°ï¼Œä¼šè‡ªåŠ¨ä½¿ç”¨åˆ†è§£ç‰ˆæœ¬
```

### 6.2 Functionalization

```python
# ä½œç”¨ï¼šå°† inplace æ“ä½œè½¬æ¢ä¸º out-of-place

x = torch.randn(10)

# Inplace æ“ä½œ
x.add_(1)  # ä¿®æ”¹ x

# Functionalization åï¼š
# x_new = x.add(1)  # åˆ›å»ºæ–° tensor
# ç„¶åæ›´æ–°å¼•ç”¨

# è¿™å¯¹ç¼–è¯‘å¾ˆæœ‰ç”¨ï¼ˆé¿å…åˆ«åé—®é¢˜ï¼‰
```

### 6.3 Structured Kernels

```python
# ç»“æ„åŒ– kernelï¼šç»Ÿä¸€å¤„ç† functional å’Œ out å˜ä½“

# ç”¨æˆ·å¯èƒ½è°ƒç”¨ï¼š
y = torch.add(x, 1)          # functional å½¢å¼
torch.add(x, 1, out=y)       # out å½¢å¼

# Dispatcher å†…éƒ¨ç»Ÿä¸€è°ƒåº¦åˆ°åŒä¸€ä¸ª"ç»“æ„åŒ– kernel"
# å‡å°‘ä»£ç é‡å¤
```

## 7. æºç åˆ†æ

### 7.1 æ ¸å¿ƒæ–‡ä»¶ä½ç½®

```
pytorch/
â”œâ”€â”€ aten/src/ATen/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ dispatch/
â”‚   â”‚   â”‚   â”œâ”€â”€ Dispatcher.h         # Dispatcher æ ¸å¿ƒ
â”‚   â”‚   â”‚   â”œâ”€â”€ DispatchKey.h        # DispatchKey å®šä¹‰
â”‚   â”‚   â”‚   â””â”€â”€ OperatorEntry.h      # ç®—å­æ³¨å†Œè¡¨
â”‚   â”œâ”€â”€ native/
â”‚   â”‚   â”œâ”€â”€ native_functions.yaml    # ç®—å­å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ BinaryOps.cpp            # äºŒå…ƒæ“ä½œå®ç°
â”‚   â”‚   â””â”€â”€ ...
â”œâ”€â”€ torch/
â”‚   â”œâ”€â”€ _ops.py                      # Python ç®—å­æ¥å£
â”‚   â””â”€â”€ library.py                   # Python ç®—å­æ³¨å†Œ
â””â”€â”€ c10/core/DispatchKey.h           # DispatchKey åº•å±‚å®šä¹‰
```

### 7.2 Dispatcher çš„æ ¸å¿ƒæ•°æ®ç»“æ„

```cpp
// aten/src/ATen/core/dispatch/Dispatcher.h (ç®€åŒ–ç‰ˆ)

class Dispatcher {
private:
    // ç®—å­æ³¨å†Œè¡¨ï¼šç®—å­å â†’ OperatorEntry
    std::unordered_map<OperatorName, OperatorEntry> operators_;

public:
    // è°ƒåº¦å‡½æ•°
    template<typename Return, typename... Args>
    Return call(const OperatorHandle& op, Args... args) {
        // 1. è®¡ç®— DispatchKey
        DispatchKey key = computeDispatchKey(args...);
        
        // 2. æŸ¥æ‰¾å¯¹åº”çš„ kernel
        KernelFunction* kernel = op.lookup(key);
        
        // 3. è°ƒç”¨ kernel
        return kernel->call(args...);
    }
};
```

### 7.3 DispatchKey è®¡ç®—

```cpp
// ç®€åŒ–çš„ä¼ªä»£ç 

DispatchKey computeDispatchKey(const Tensor& tensor) {
    DispatchKeySet keys;
    
    // 1. æ·»åŠ åç«¯ key
    keys = keys | tensor.device().dispatchKey();  // CPU/CUDA/...
    
    // 2. æ·»åŠ  Autograd keyï¼ˆå¦‚æœéœ€è¦æ¢¯åº¦ï¼‰
    if (tensor.requires_grad()) {
        keys = keys | getAutogradKey(tensor.device());
    }
    
    // 3. æ·»åŠ å…¶ä»– keyï¼ˆfunctionalize, python ç­‰ï¼‰
    keys = keys | getCurrentDispatchKeys();
    
    // 4. è¿”å›æœ€é«˜ä¼˜å…ˆçº§çš„ key
    return keys.highestPriorityKey();
}
```

## 8. é€‚é…è‡ªå®šä¹‰ç¡¬ä»¶

### 8.1 æ·»åŠ æ–°çš„ DispatchKey

```cpp
// 1. åœ¨ c10/core/DispatchKey.h ä¸­æ·»åŠ æ–°çš„ key

enum class DispatchKey {
    // ... ç°æœ‰çš„ keys ...
    
    MyCustomChip,  // è‡ªå®šä¹‰èŠ¯ç‰‡ â† æ–°å¢
    
    // ...
};
```

### 8.2 æ³¨å†Œç®—å­å®ç°

```cpp
// 2. ä¸ºè‡ªå®šä¹‰èŠ¯ç‰‡å®ç°ç®—å­

// mylib/kernels/Add.cpp

Tensor add_custom_chip(const Tensor& self, const Tensor& other, const Scalar& alpha) {
    // è°ƒç”¨è‡ªå®šä¹‰èŠ¯ç‰‡çš„ API
    MyChip::add(self.data(), other.data(), alpha.toDouble());
    return result;
}

// 3. æ³¨å†Œ
TORCH_LIBRARY_IMPL(aten, MyCustomChip, m) {
    m.impl("add.Tensor", &add_custom_chip);
    m.impl("mul.Tensor", &mul_custom_chip);
    // ... æ›´å¤šç®—å­
}
```

### 8.3 è®¾å¤‡ç®¡ç†

```python
# 4. Python ä¾§æš´éœ²è®¾å¤‡

import torch

# æ³¨å†Œè®¾å¤‡ç±»å‹
torch.utils.cpp_extension.load(
    name="my_custom_chip",
    sources=["my_chip.cpp"],
)

# ä½¿ç”¨
x = torch.randn(10, device='my_custom_chip')
y = x + 1  # è‡ªåŠ¨è°ƒåº¦åˆ° add_custom_chip()
```

## 9. å®é™…åº”ç”¨åœºæ™¯

### 9.1 æ€§èƒ½åˆ†æå·¥å…·

```python
class ProfilingMode(torch.utils._python_dispatch.TorchDispatchMode):
    def __init__(self):
        self.op_counts = {}
        self.op_times = {}
    
    def __torch_dispatch__(self, func, types, args=(), kwargs=None):
        import time
        
        # è®°å½•è°ƒç”¨æ¬¡æ•°
        op_name = func.__name__
        self.op_counts[op_name] = self.op_counts.get(op_name, 0) + 1
        
        # è®°å½•æ‰§è¡Œæ—¶é—´
        start = time.time()
        result = func(*args, **(kwargs or {}))
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        elapsed = time.time() - start
        
        self.op_times[op_name] = self.op_times.get(op_name, 0) + elapsed
        
        return result
    
    def report(self):
        print("\næ“ä½œç»Ÿè®¡:")
        for op, count in sorted(self.op_counts.items(), key=lambda x: -x[1]):
            time_ms = self.op_times[op] * 1000
            print(f"  {op:30} è°ƒç”¨: {count:5} æ¬¡, æ€»æ—¶é—´: {time_ms:.2f}ms")

# ä½¿ç”¨
profiler = ProfilingMode()
with profiler:
    model(input_data)

profiler.report()
```

### 9.2 è‡ªåŠ¨æ··åˆç²¾åº¦

```python
# PyTorch çš„ autocast å°±æ˜¯é€šè¿‡ Dispatcher å®ç°çš„

with torch.cuda.amp.autocast():
    # æŸäº›æ“ä½œè‡ªåŠ¨è½¬ä¸º float16
    output = model(input)

# å†…éƒ¨æµç¨‹ï¼š
# 1. è¿›å…¥ autocast ä¸Šä¸‹æ–‡
# 2. æ·»åŠ  Autocast DispatchKey
# 3. Dispatcher æ£€æµ‹åˆ° Autocast key
# 4. æ ¹æ®è§„åˆ™è‡ªåŠ¨è½¬æ¢ç²¾åº¦
```

## 10. å°ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **Dispatcher = PyTorch çš„ä¸­å¤®è°ƒåº¦ç³»ç»Ÿ**
2. **DispatchKey**ï¼šå†³å®šè°ƒç”¨å“ªä¸ª kernel çš„å…³é”®
3. **å¤šåç«¯æ”¯æŒ**ï¼šç»Ÿä¸€çš„æ¥å£ï¼Œä¸åŒçš„å®ç°
4. **ä¼˜å…ˆçº§æœºåˆ¶**ï¼šæŒ‰é¡ºåºæ£€æŸ¥å¤šä¸ª DispatchKey
5. **å¯æ‰©å±•æ€§**ï¼šæ˜“äºæ·»åŠ æ–°çš„åç«¯å’Œç®—å­

### å…³é”®æ¦‚å¿µ

- **DispatchKey**ï¼šè°ƒåº¦é”®ï¼ˆCPU/CUDA/Autograd/...ï¼‰
- **Kernel**ï¼šç®—å­çš„å…·ä½“å®ç°
- **ç®—å­æ³¨å†Œ**ï¼šå°† kernel ä¸ DispatchKey å…³è”
- **__torch_dispatch__**ï¼šPython å±‚çš„æ‹¦æˆªæœºåˆ¶
- **BackendSelect**ï¼šè‡ªåŠ¨é€‰æ‹©åç«¯

### Dispatcher çš„ä½œç”¨

```
ç»Ÿä¸€æ¥å£ + å¤šæ ·åŒ–åç«¯ + æ‰©å±•æ€§
    â†“
æ”¯æŒå¤šç§è®¾å¤‡ï¼ˆCPU/GPU/TPU/...ï¼‰
æ”¯æŒå¤šç§æ¨¡å¼ï¼ˆAutograd/JIT/Vmap/...ï¼‰
æ”¯æŒè‡ªå®šä¹‰æ‰©å±•
```

### åœ¨ torch.compile ä¸­çš„è§’è‰²

```
torch.compile æœ€ç»ˆç”Ÿæˆçš„ä»£ç ä¹Ÿæ˜¯é€šè¿‡ Dispatcher è°ƒåº¦ï¼š
Dynamo â†’ AOTAutograd â†’ Inductor ç”Ÿæˆä»£ç 
                           â†“
                    è°ƒç”¨ PyTorch ç®—å­
                           â†“
                       Dispatcher
                           â†“
                  å®é™…çš„ kernel æ‰§è¡Œ
```

### ä¸‹ä¸€æ­¥

ç°åœ¨ä½ ç†è§£äº† PyTorch çš„åº•å±‚åˆ†å‘æœºåˆ¶ï¼Œæœ€åæˆ‘ä»¬æ¥çœ‹å¦‚ä½•å°†è¿™äº›çŸ¥è¯†åº”ç”¨åˆ°å®é™…åœºæ™¯ï¼š

ğŸ“– [ç¬¬å…«ç« ï¼šå›½äº§èŠ¯ç‰‡é€‚é…å®æˆ˜æŒ‡å—](./08_custom_chip_adaptation.md) - å¦‚ä½•ä¸ºè‡ªå®šä¹‰ç¡¬ä»¶é€‚é… torch.compile

---

## ğŸ’¡ ç»ƒä¹ é¢˜

1. ä½¿ç”¨ `torch.library` æ³¨å†Œä¸€ä¸ªè‡ªå®šä¹‰ç®—å­
2. ç¼–å†™ä¸€ä¸ª `TorchDispatchMode` æ¥è®°å½•æ‰€æœ‰ç®—å­è°ƒç”¨
3. æŸ¥çœ‹ä¸€ä¸ªç®—å­ï¼ˆå¦‚ `add`ï¼‰çš„æ‰€æœ‰æ³¨å†Œä¿¡æ¯

**ç»§ç»­å­¦ä¹ ** â†’ [å›½äº§èŠ¯ç‰‡é€‚é…å®æˆ˜æŒ‡å—](./08_custom_chip_adaptation.md)

