# Liger-Kernel é¡¹ç›®æ·±åº¦åˆ†æ

## ç›®å½•

1. [é¡¹ç›®ä½œç”¨ä¸è§£å†³çš„é—®é¢˜](#1-é¡¹ç›®ä½œç”¨ä¸è§£å†³çš„é—®é¢˜)
2. [ç®—å­æ³¨å†Œæœºåˆ¶](#2-ç®—å­æ³¨å†Œæœºåˆ¶)
3. [Monkey Patch å®ç°æœºåˆ¶](#3-monkey-patch-å®ç°æœºåˆ¶)
4. [é¡¹ç›®ä½¿ç”¨æµç¨‹](#4-é¡¹ç›®ä½¿ç”¨æµç¨‹)
5. [NPU å…¼å®¹æ€§åˆ†æ](#5-npu-å…¼å®¹æ€§åˆ†æ)
6. [å•å…ƒæµ‹è¯•ä½¿ç”¨æ–¹æ³•](#6-å•å…ƒæµ‹è¯•ä½¿ç”¨æ–¹æ³•)

---

## 1. é¡¹ç›®ä½œç”¨ä¸è§£å†³çš„é—®é¢˜

### 1.1 é¡¹ç›®ç®€ä»‹

**Liger-Kernel** æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒè®¾è®¡çš„é«˜æ€§èƒ½ Triton ç®—å­åº“ã€‚å®ƒç”± LinkedIn å¼€å‘å¹¶å¼€æºï¼Œæ—¨åœ¨æé«˜è®­ç»ƒæ•ˆç‡å’Œå‡å°‘å†…å­˜å ç”¨ã€‚

### 1.2 æ ¸å¿ƒä»·å€¼

#### æ€§èƒ½æå‡
- **è®­ç»ƒååé‡æå‡ 20%**ï¼šé€šè¿‡ä¼˜åŒ–çš„ Triton å†…æ ¸å®ç°æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦
- **å†…å­˜å ç”¨å‡å°‘ 60%**ï¼šå…è®¸ä½¿ç”¨æ›´é•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€æ›´å¤§çš„æ‰¹æ¬¡å¤§å°
- **åè®­ç»ƒä¼˜åŒ–é«˜è¾¾ 80% å†…å­˜èŠ‚çœ**ï¼šé’ˆå¯¹å¯¹é½å’Œè’¸é¦ä»»åŠ¡ï¼ˆDPOã€ORPOã€CPOç­‰ï¼‰

#### è§£å†³çš„æ ¸å¿ƒé—®é¢˜

1. **å†…å­˜ç“¶é¢ˆ**
   - åŸç”Ÿ HuggingFace æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡ï¼ˆ>4Kï¼‰æ—¶å®¹æ˜“ OOM
   - Liger-Kernel å¯ä»¥æ‰©å±•åˆ° 16K ä¸Šä¸‹æ–‡é•¿åº¦

2. **è®­ç»ƒæ•ˆç‡ä½ä¸‹**
   - åŸç”Ÿå®ç°æ²¡æœ‰å……åˆ†åˆ©ç”¨ GPU å¹¶è¡Œè®¡ç®—èƒ½åŠ›
   - é€šè¿‡å†…æ ¸èåˆï¼ˆKernel Fusionï¼‰å‡å°‘å†…å­˜è®¿é—®å¼€é”€

3. **æ˜“ç”¨æ€§å·®**
   - ä¼ ç»Ÿä¼˜åŒ–éœ€è¦æ‰‹åŠ¨ä¿®æ”¹å¤§é‡ä»£ç 
   - Liger-Kernel æä¾›ä¸€è¡Œä»£ç é›†æˆæ–¹æ¡ˆ

### 1.3 ä½¿ç”¨æ–¹æ³•

Liger-Kernel æä¾›ä¸‰ç§ä½¿ç”¨æ–¹å¼ï¼Œä»ç®€å•åˆ°çµæ´»ï¼š

#### æ–¹æ³• 1ï¼šè‡ªåŠ¨é›†æˆï¼ˆæœ€ç®€å•ï¼‰

ä½¿ç”¨ `AutoLigerKernelForCausalLM` è‡ªåŠ¨åº”ç”¨ä¼˜åŒ–ï¼š

```python
from liger_kernel.transformers import AutoLigerKernelForCausalLM

# è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹å¹¶åº”ç”¨ä¼˜åŒ–
model = AutoLigerKernelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
```

#### æ–¹æ³• 2ï¼šæ¨¡å‹ç‰¹å®š Patch APIï¼ˆæ¨èï¼‰

é’ˆå¯¹ç‰¹å®šæ¨¡å‹ä½¿ç”¨ patching APIï¼š

```python
import transformers
from liger_kernel.transformers import apply_liger_kernel_to_llama

# åœ¨æ¨¡å‹åˆå§‹åŒ–å‰åº”ç”¨ patch
apply_liger_kernel_to_llama()

# æˆ–è€…é€‰æ‹©æ€§åº”ç”¨ç‰¹å®šä¼˜åŒ–
apply_liger_kernel_to_llama(
    rope=True,              # RoPE ä½ç½®ç¼–ç 
    swiglu=True,           # SwiGLU æ¿€æ´»å‡½æ•°
    cross_entropy=True,    # äº¤å‰ç†µæŸå¤±
    fused_linear_cross_entropy=False,  # èåˆçº¿æ€§å±‚+äº¤å‰ç†µ
    rms_norm=True          # RMS å½’ä¸€åŒ–
)

# ç„¶åæ­£å¸¸åŠ è½½æ¨¡å‹
model = transformers.AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
```

#### æ–¹æ³• 3ï¼šåº•å±‚ API ç»„åˆï¼ˆæœ€çµæ´»ï¼‰

ç›´æ¥ä½¿ç”¨å„ä¸ªç®—å­æ¨¡å—æ„å»ºè‡ªå®šä¹‰æ¨¡å‹ï¼š

```python
from liger_kernel.transformers import LigerFusedLinearCrossEntropyLoss
import torch.nn as nn
import torch

# åˆ›å»ºçº¿æ€§å±‚
model = nn.Linear(128, 256).cuda()

# ä½¿ç”¨èåˆçš„çº¿æ€§+äº¤å‰ç†µæŸå¤±å‡½æ•°
loss_fn = LigerFusedLinearCrossEntropyLoss()

input = torch.randn(4, 128, requires_grad=True, device="cuda")
target = torch.randint(256, (4,), device="cuda")

# è®¡ç®—æŸå¤±ï¼ˆè‡ªåŠ¨è¿›è¡Œåˆ†å—è®¡ç®—ä»¥å‡å°‘å†…å­˜ï¼‰
loss = loss_fn(model.weight, input, target)
loss.backward()
```

### 1.4 æ ¸å¿ƒç‰¹æ€§

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| **æ˜“ç”¨æ€§** | ä¸€è¡Œä»£ç é›†æˆï¼Œæ— éœ€ä¿®æ”¹ç°æœ‰è®­ç»ƒä»£ç  |
| **é«˜æ•ˆæ€§** | é€šè¿‡å†…æ ¸èåˆã€åŸåœ°æ›¿æ¢ã€åˆ†å—æŠ€æœ¯æå‡æ€§èƒ½ |
| **ç²¾ç¡®æ€§** | è®¡ç®—å®Œå…¨ç²¾ç¡®ï¼Œä¸ä½¿ç”¨è¿‘ä¼¼ï¼ŒåŒ…å«ä¸¥æ ¼çš„å•å…ƒæµ‹è¯• |
| **è½»é‡çº§** | ä»…ä¾èµ– Torch å’Œ Tritonï¼Œæ— é¢å¤–åº“ä¾èµ– |
| **å¤š GPU æ”¯æŒ** | å…¼å®¹ FSDPã€DeepSpeedã€DDP ç­‰åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ |
| **æ¡†æ¶é›†æˆ** | å·²é›†æˆåˆ° Axolotlã€LLaMA-Factoryã€SFTTrainerã€HF Trainer ç­‰ |

### 1.5 æ”¯æŒçš„æ¨¡å‹

- **LLaMA ç³»åˆ—**ï¼šLLaMA 2/3ã€LLaMA 3.2-Visionã€LLaMA 4
- **Mistral ç³»åˆ—**ï¼šMistralã€Mixtral
- **Gemma ç³»åˆ—**ï¼šGemma 1/2/3
- **Qwen ç³»åˆ—**ï¼šQwen2ã€Qwen2-VLã€Qwen3ã€QwQ
- **å…¶ä»–**ï¼šPhi3ã€Graniteã€OLMo2ã€GLM-4ã€InternVL3 ç­‰

---

## 2. ç®—å­æ³¨å†Œæœºåˆ¶

### 2.1 **ä¸æ˜¯** Torch æ³¨å†Œçš„ç®—å­

**é‡è¦ç»“è®º**ï¼šLiger-Kernel çš„ç®—å­**ä¸æ˜¯é€šè¿‡ PyTorch å®˜æ–¹çš„ç®—å­æ³¨å†Œæœºåˆ¶ï¼ˆå¦‚ `torch.library`ï¼‰æ³¨å†Œçš„**ã€‚

### 2.2 å®ç°æ–¹å¼

Liger-Kernel ä½¿ç”¨äº†ä»¥ä¸‹æŠ€æœ¯ï¼š

#### 2.2.1 Triton JIT ç¼–è¯‘

æ‰€æœ‰æ ¸å¿ƒç®—å­éƒ½æ˜¯ç”¨ Triton ç¼–å†™çš„ï¼Œé€šè¿‡ `@triton.jit` è£…é¥°å™¨è¿›è¡Œ JIT ç¼–è¯‘ï¼š

```python
# src/liger_kernel/ops/rms_norm.py
import triton
import triton.language as tl

@triton.jit
def _rms_norm_forward_kernel(
    Y_ptr,       # è¾“å‡ºæŒ‡é’ˆ
    X_ptr,       # è¾“å…¥æŒ‡é’ˆ
    W_ptr,       # æƒé‡æŒ‡é’ˆ
    RSTD_ptr,    # RMS æ ‡å‡†å·®ç¼“å­˜
    n_cols,      # åˆ—æ•°
    eps,         # epsilon
    BLOCK_SIZE: tl.constexpr,  # å—å¤§å°
):
    """
    RMS Normalization: y_i = (x_i / RMS) * w_i
    RMS = sqrt(sum(x_i^2) / N)
    """
    row_idx = tl.program_id(0).to(tl.int64)
    col_offsets = tl.arange(0, BLOCK_SIZE)
    mask = col_offsets < n_cols
    
    # åŠ è½½æ•°æ®
    X_row = tl.load(X_ptr + row_idx * n_cols + col_offsets, mask=mask, other=0)
    W_row = tl.load(W_ptr + col_offsets, mask=mask, other=0)
    
    # è®¡ç®— RMS
    mean_square = tl.sum(X_row * X_row, axis=0) / n_cols
    rstd = 1.0 / tl.sqrt(mean_square + eps)
    
    # å½’ä¸€åŒ–å¹¶åº”ç”¨æƒé‡
    Y_row = X_row * rstd * W_row
    
    # å­˜å‚¨ç»“æœ
    tl.store(Y_ptr + row_idx * n_cols + col_offsets, Y_row, mask=mask)
```

#### 2.2.2 PyTorch Autograd Function

æ¯ä¸ªç®—å­éƒ½åŒ…è£…åœ¨ `torch.autograd.Function` ä¸­ï¼Œå®ç°è‡ªåŠ¨å¾®åˆ†ï¼š

```python
# src/liger_kernel/ops/rms_norm.py
class LigerRMSNormFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, X, W, eps, offset, casting_mode):
        # åˆ†é…è¾“å‡ºå¼ é‡
        Y = torch.empty_like(X)
        RSTD = torch.empty(X.shape[0], dtype=torch.float32, device=X.device)
        
        # è®¡ç®—ç½‘æ ¼å¤§å°å’Œå—å¤§å°
        n_rows, n_cols = X.shape
        BLOCK_SIZE = triton.next_power_of_2(n_cols)
        
        # å¯åŠ¨ Triton å†…æ ¸
        _rms_norm_forward_kernel[(n_rows,)](
            Y, X, W, RSTD,
            n_cols, eps, BLOCK_SIZE
        )
        
        # ä¿å­˜ç”¨äºåå‘ä¼ æ’­çš„å˜é‡
        ctx.save_for_backward(X, W, RSTD)
        ctx.eps = eps
        
        return Y
    
    @staticmethod
    def backward(ctx, dY):
        X, W, RSTD = ctx.saved_tensors
        
        # åˆ†é…æ¢¯åº¦å¼ é‡
        dX = torch.empty_like(X)
        dW = torch.empty_like(W)
        
        # å¯åŠ¨åå‘ä¼ æ’­å†…æ ¸
        _rms_norm_backward_kernel[(X.shape[0],)](
            dY, X, W, RSTD, dX, dW,
            X.shape[1], ctx.eps, BLOCK_SIZE
        )
        
        return dX, dW, None, None, None
```

#### 2.2.3 PyTorch Module å°è£…

æä¾›æ ‡å‡†çš„ `nn.Module` æ¥å£ï¼š

```python
# src/liger_kernel/transformers/rms_norm.py
class LigerRMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6, offset=0.0, 
                 casting_mode="llama", init_fn="ones"):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps
        self.offset = offset
        self.casting_mode = casting_mode
    
    def forward(self, hidden_states):
        return LigerRMSNormFunction.apply(
            hidden_states,
            self.weight,
            self.variance_epsilon,
            self.offset,
            self.casting_mode
        )
```

### 2.3 ä¸ PyTorch çš„é›†æˆæ–¹å¼

Liger-Kernel é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¸ PyTorch ç”Ÿæ€ç³»ç»Ÿé›†æˆï¼š

1. **å‡½æ•°çº§æ›¿æ¢**ï¼šæ›¿æ¢æ ‡å‡†å‡½æ•°ï¼ˆå¦‚ `F.cross_entropy`ï¼‰
2. **æ¨¡å—çº§æ›¿æ¢**ï¼šæ›¿æ¢ `nn.Module` ç±»ï¼ˆå¦‚ `LlamaRMSNorm`ï¼‰
3. **æ–¹æ³•çº§æ›¿æ¢**ï¼šæ›¿æ¢æ¨¡å‹çš„ `forward` æ–¹æ³•

è¿™äº›éƒ½æ˜¯é€šè¿‡ **Monkey Patching** å®ç°çš„ï¼Œè€Œä¸æ˜¯ PyTorch çš„å®˜æ–¹æ³¨å†Œæœºåˆ¶ã€‚

---

## 3. Monkey Patch å®ç°æœºåˆ¶

### 3.1 Monkey Patch çš„æ ¸å¿ƒåŸç†

Monkey Patching æ˜¯ä¸€ç§åœ¨è¿è¡Œæ—¶åŠ¨æ€ä¿®æ”¹ç±»æˆ–æ¨¡å—çš„æŠ€æœ¯ã€‚Liger-Kernel ä½¿ç”¨è¿™ç§æŠ€æœ¯æ¥æ›¿æ¢ HuggingFace Transformers ä¸­çš„åŸå§‹å®ç°ã€‚

### 3.2 å®ç°å±‚æ¬¡

#### 3.2.1 æ¨¡å—çº§ Patch

ç›´æ¥æ›¿æ¢ transformers æ¨¡å—ä¸­çš„ç±»ï¼š

```python
# src/liger_kernel/transformers/monkey_patch.py

def apply_liger_kernel_to_llama(
    rope: bool = True,
    cross_entropy: bool = False,
    fused_linear_cross_entropy: bool = True,
    rms_norm: bool = True,
    swiglu: bool = True,
    model: PreTrainedModel = None,
) -> None:
    """
    å°† Liger å†…æ ¸åº”ç”¨åˆ° LLaMA æ¨¡å‹
    """
    from transformers.models.llama import modeling_llama
    
    # 1. æ›¿æ¢ RoPE å‡½æ•°
    if rope:
        modeling_llama.apply_rotary_pos_emb = liger_rotary_pos_emb
    
    # 2. æ›¿æ¢ RMSNorm ç±»
    if rms_norm:
        modeling_llama.LlamaRMSNorm = LigerRMSNorm
    
    # 3. æ›¿æ¢ SwiGLU MLP ç±»
    if swiglu:
        modeling_llama.LlamaMLP = LigerSwiGLUMLP
    
    # 4. æ›¿æ¢äº¤å‰ç†µæŸå¤±å‡½æ•°
    if cross_entropy:
        from transformers.loss.loss_utils import nn
        nn.functional.cross_entropy = liger_cross_entropy
    
    # 5. æ›¿æ¢æ¨¡å‹çš„ forward æ–¹æ³•ï¼ˆèåˆçº¿æ€§å±‚+äº¤å‰ç†µï¼‰
    if fused_linear_cross_entropy:
        if model is not None:
            # ä¸ºå·²å­˜åœ¨çš„æ¨¡å‹å®ä¾‹æ›¿æ¢ forward æ–¹æ³•
            model.forward = MethodType(llama_lce_forward, model)
        else:
            # ä¸ºç±»æ›¿æ¢ forward æ–¹æ³•
            modeling_llama.LlamaForCausalLM.forward = llama_lce_forward
```

#### 3.2.2 å®ä¾‹çº§ Patch

å¯¹äºå·²ç»åˆå§‹åŒ–çš„æ¨¡å‹å®ä¾‹ï¼Œéœ€è¦é¢å¤–çš„å¤„ç†ï¼š

```python
def apply_liger_kernel_to_llama(model: PreTrainedModel = None, ...):
    # ... æ¨¡å—çº§ patchï¼ˆå¦‚ä¸Šï¼‰
    
    if model is not None:
        # è·å–åŸºç¡€æ¨¡å‹
        base_model = getattr(model, model.base_model_prefix, model)
        
        # æ›¿æ¢å®ä¾‹ä¸­çš„æ¨¡å—
        if rms_norm:
            _patch_rms_norm_module(base_model.norm)
        
        # éå†æ‰€æœ‰è§£ç å±‚
        for decoder_layer in base_model.layers:
            if swiglu:
                _patch_swiglu_module(decoder_layer.mlp, LigerSwiGLUMLP)
            if rms_norm:
                _patch_rms_norm_module(decoder_layer.input_layernorm)
                _patch_rms_norm_module(decoder_layer.post_attention_layernorm)


def _patch_rms_norm_module(module):
    """åŸåœ°æ›¿æ¢ RMSNorm æ¨¡å—çš„å‚æ•°å’Œæ–¹æ³•"""
    if hasattr(module, 'weight'):
        # ä¿ç•™åŸå§‹æƒé‡
        original_weight = module.weight
        eps = module.variance_epsilon
        
        # åˆ›å»ºæ–°çš„ LigerRMSNorm
        new_module = LigerRMSNorm(
            hidden_size=original_weight.shape[0],
            eps=eps
        )
        
        # å¤åˆ¶æƒé‡
        new_module.weight.data.copy_(original_weight.data)
        
        # æ›¿æ¢ forward æ–¹æ³•
        module.forward = new_module.forward
        module.__class__ = type(new_module)


def _patch_swiglu_module(module, swiglu_class):
    """æ›¿æ¢ SwiGLU MLP æ¨¡å—"""
    original_state = module.state_dict()
    
    # åˆ›å»ºæ–°æ¨¡å—
    new_module = swiglu_class(
        config=module.config,
        hidden_size=module.hidden_size,
        intermediate_size=module.intermediate_size
    )
    
    # åŠ è½½åŸå§‹æƒé‡
    new_module.load_state_dict(original_state, strict=False)
    
    # æ›¿æ¢æ¨¡å—
    module.__class__ = type(new_module)
    module.forward = new_module.forward
```

### 3.3 Patch æ—¶æœº

æœ‰ä¸¤ä¸ªå…³é”®æ—¶æœºï¼š

#### æ—¶æœº 1ï¼šæ¨¡å‹åˆå§‹åŒ–å‰ï¼ˆæ¨èï¼‰

```python
# å…ˆ patch
apply_liger_kernel_to_llama()

# å†åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
```

**ä¼˜ç‚¹**ï¼š
- æ¨¡å‹åœ¨åˆ›å»ºæ—¶å°±ä½¿ç”¨ä¼˜åŒ–çš„å®ç°
- æ— éœ€å¤„ç†å·²æœ‰å®ä¾‹çš„æƒé‡è¿ç§»
- æ€§èƒ½æ›´å¥½

#### æ—¶æœº 2ï¼šæ¨¡å‹åˆå§‹åŒ–å

```python
# å…ˆåŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")

# å† patchï¼ˆéœ€è¦ä¼ å…¥ model å‚æ•°ï¼‰
apply_liger_kernel_to_llama(model=model)
```

**ä¼˜ç‚¹**ï¼š
- å¯ä»¥å¯¹å·²æœ‰æ¨¡å‹è¿›è¡Œä¼˜åŒ–
- çµæ´»æ€§æ›´é«˜

**ç¼ºç‚¹**ï¼š
- éœ€è¦å¤„ç†å®ä¾‹å˜é‡çš„æ›¿æ¢
- å¯èƒ½ä¼šæœ‰é¢å¤–çš„å†…å­˜å¼€é”€

### 3.4 è‡ªåŠ¨æ£€æµ‹æœºåˆ¶

Liger-Kernel æä¾›äº†è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹çš„åŠŸèƒ½ï¼š

```python
# src/liger_kernel/transformers/monkey_patch.py

MODEL_TYPE_TO_APPLY_LIGER_FN = {
    "llama": apply_liger_kernel_to_llama,
    "llama4": apply_liger_kernel_to_llama4,
    "mistral": apply_liger_kernel_to_mistral,
    "mixtral": apply_liger_kernel_to_mixtral,
    "gemma": apply_liger_kernel_to_gemma,
    "gemma2": apply_liger_kernel_to_gemma2,
    "qwen2": apply_liger_kernel_to_qwen2,
    "phi3": apply_liger_kernel_to_phi3,
    # ... æ›´å¤šæ¨¡å‹
}


def _apply_liger_kernel(model_type: str, **kwargs) -> None:
    """æ ¹æ®æ¨¡å‹ç±»å‹è‡ªåŠ¨åº”ç”¨ Liger å†…æ ¸"""
    if model_type not in MODEL_TYPE_TO_APPLY_LIGER_FN:
        logger.info(f"No Liger kernels for model type: {model_type}")
        return
    
    apply_fn = MODEL_TYPE_TO_APPLY_LIGER_FN[model_type]
    apply_fn(**kwargs)


def _apply_liger_kernel_to_instance(model: PreTrainedModel, **kwargs) -> None:
    """å¯¹æ¨¡å‹å®ä¾‹åº”ç”¨ Liger å†…æ ¸"""
    # ä»æ¨¡å‹é…ç½®ä¸­è·å–æ¨¡å‹ç±»å‹
    model_type = getattr(model.config, "model_type", None)
    
    if not model_type:
        logger.info("Cannot determine model type")
        return
    
    apply_fn = MODEL_TYPE_TO_APPLY_LIGER_FN.get(model_type)
    if apply_fn:
        apply_fn(model=model, **kwargs)
```

### 3.5 å¦‚ä½•æ·»åŠ æ–°çš„ Monkey Patch

å‡è®¾ä½ è¦ä¸ºæ–°æ¨¡å‹ `NewModel` æ·»åŠ  Liger ä¼˜åŒ–ï¼š

```python
# ç¬¬ä¸€æ­¥ï¼šåˆ›å»º patch å‡½æ•°
def apply_liger_kernel_to_newmodel(
    rope: bool = True,
    rms_norm: bool = True,
    swiglu: bool = True,
    cross_entropy: bool = False,
    fused_linear_cross_entropy: bool = True,
    model: PreTrainedModel = None,
) -> None:
    """ä¸º NewModel åº”ç”¨ Liger å†…æ ¸"""
    from transformers.models.newmodel import modeling_newmodel
    
    # æ›¿æ¢ç®—å­
    if rope:
        modeling_newmodel.apply_rotary_pos_emb = liger_rotary_pos_emb
    
    if rms_norm:
        modeling_newmodel.NewModelRMSNorm = LigerRMSNorm
    
    if swiglu:
        modeling_newmodel.NewModelMLP = LigerSwiGLUMLP
    
    if cross_entropy:
        from transformers.loss.loss_utils import nn
        nn.functional.cross_entropy = liger_cross_entropy
    
    if fused_linear_cross_entropy:
        # éœ€è¦å…ˆå®ç° newmodel_lce_forward å‡½æ•°
        if model is not None:
            model.forward = MethodType(newmodel_lce_forward, model)
        else:
            modeling_newmodel.NewModelForCausalLM.forward = newmodel_lce_forward
    
    # å¦‚æœæ˜¯å®ä¾‹çº§ patch
    if model is not None:
        base_model = getattr(model, model.base_model_prefix, model)
        
        if rms_norm:
            _patch_rms_norm_module(base_model.norm)
        
        for layer in base_model.layers:
            if swiglu:
                _patch_swiglu_module(layer.mlp, LigerSwiGLUMLP)
            if rms_norm:
                _patch_rms_norm_module(layer.input_layernorm)


# ç¬¬äºŒæ­¥ï¼šæ³¨å†Œåˆ°æ˜ å°„è¡¨
MODEL_TYPE_TO_APPLY_LIGER_FN["newmodel"] = apply_liger_kernel_to_newmodel


# ç¬¬ä¸‰æ­¥ï¼šå¯¼å‡º API
# åœ¨ src/liger_kernel/transformers/__init__.py ä¸­æ·»åŠ 
__all__.append("apply_liger_kernel_to_newmodel")
```

---

## 4. é¡¹ç›®ä½¿ç”¨æµç¨‹

### 4.1 å®Œæ•´çš„è®­ç»ƒæµç¨‹ç¤ºä¾‹

ä»¥ä¸‹æ˜¯ä½¿ç”¨ Liger-Kernel è®­ç»ƒ LLaMA æ¨¡å‹çš„å®Œæ•´ç¤ºä¾‹ï¼š

```python
# train_with_liger.py

import torch
import transformers
from datasets import load_dataset
from transformers import AutoTokenizer, TrainingArguments
from trl import SFTTrainer, DataCollatorForCompletionOnlyLM

# æ–¹æ³• 1ï¼šä½¿ç”¨ AutoLigerKernelForCausalLMï¼ˆæ¨èï¼‰
from liger_kernel.transformers import AutoLigerKernelForCausalLM

def main():
    # 1. åŠ è½½ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        "meta-llama/Llama-2-7b-hf",
        padding_side="left",
        truncation_side="left",
    )
    tokenizer.pad_token = tokenizer.eos_token
    
    # 2. åŠ è½½æ•°æ®é›†
    dataset = load_dataset("tatsu-lab/alpaca")["train"]
    train_dataset = dataset.train_test_split(test_size=0.1)["train"]
    eval_dataset = dataset.train_test_split(test_size=0.1)["test"]
    
    # 3. é…ç½®æ•°æ® collator
    response_template = tokenizer.encode("### Response:\n", add_special_tokens=False)
    collator = DataCollatorForCompletionOnlyLM(
        tokenizer=tokenizer,
        response_template=response_template,
    )
    
    # 4. åŠ è½½æ¨¡å‹ï¼ˆè‡ªåŠ¨åº”ç”¨ Liger ä¼˜åŒ–ï¼‰
    model = AutoLigerKernelForCausalLM.from_pretrained(
        "meta-llama/Llama-2-7b-hf",
        torch_dtype=torch.bfloat16,
        use_cache=False,  # è®­ç»ƒæ—¶å¿…é¡»ç¦ç”¨ç¼“å­˜
        # å¯é€‰ï¼šè¦†ç›–é»˜è®¤çš„ä¼˜åŒ–è®¾ç½®
        # rope=True,
        # rms_norm=True,
        # swiglu=True,
        # fused_linear_cross_entropy=True,
    )
    
    # 5. é…ç½®è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir="./llama2-7b-alpaca",
        num_train_epochs=3,
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        gradient_accumulation_steps=4,
        learning_rate=2e-5,
        fp16=False,
        bf16=True,
        logging_steps=10,
        evaluation_strategy="steps",
        eval_steps=100,
        save_steps=500,
        save_total_limit=2,
        # FSDP é…ç½®ï¼ˆå¤š GPUï¼‰
        fsdp="full_shard auto_wrap",
        fsdp_config={
            "fsdp_transformer_layer_cls_to_wrap": ["LlamaDecoderLayer"],
        },
    )
    
    # 6. åˆ›å»º trainer
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=collator,
        tokenizer=tokenizer,
        max_seq_length=2048,
    )
    
    # 7. å¼€å§‹è®­ç»ƒ
    trainer.train()
    
    # 8. ä¿å­˜æ¨¡å‹
    trainer.save_model("./llama2-7b-alpaca-final")


if __name__ == "__main__":
    main()
```

### 4.2 ä½¿ç”¨æ–¹æ³•å¯¹æ¯”

#### 4.2.1 ä¸ä½¿ç”¨ Liger-Kernel

```python
# æ ‡å‡† HuggingFace è®­ç»ƒ
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    torch_dtype=torch.bfloat16,
)

# é—®é¢˜ï¼š
# - 4K context å®¹æ˜“ OOM
# - è®­ç»ƒé€Ÿåº¦è¾ƒæ…¢
# - å†…å­˜å ç”¨é«˜
```

#### 4.2.2 ä½¿ç”¨ Liger-Kernelï¼ˆæ–¹æ³• 1ï¼šè‡ªåŠ¨ï¼‰

```python
from liger_kernel.transformers import AutoLigerKernelForCausalLM

model = AutoLigerKernelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    torch_dtype=torch.bfloat16,
)

# ä¼˜ç‚¹ï¼š
# - ä¸€è¡Œä»£ç å®Œæˆä¼˜åŒ–
# - è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹
# - ä½¿ç”¨é»˜è®¤æœ€ä¼˜é…ç½®
```

#### 4.2.3 ä½¿ç”¨ Liger-Kernelï¼ˆæ–¹æ³• 2ï¼šæ‰‹åŠ¨ patchï¼‰

```python
from liger_kernel.transformers import apply_liger_kernel_to_llama

# å…ˆ patch
apply_liger_kernel_to_llama(
    rope=True,
    rms_norm=True,
    swiglu=True,
    fused_linear_cross_entropy=True,
)

# å†åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    torch_dtype=torch.bfloat16,
)

# ä¼˜ç‚¹ï¼š
# - å®Œå…¨æ§åˆ¶è¦ä¼˜åŒ–çš„ç»„ä»¶
# - å¯ä»¥è¿›è¡Œ A/B æµ‹è¯•
# - é€‚åˆè°ƒè¯•
```

#### 4.2.4 ä½¿ç”¨ Liger-Kernelï¼ˆæ–¹æ³• 3ï¼šåº•å±‚ APIï¼‰

```python
from liger_kernel.transformers import (
    LigerRMSNorm,
    LigerSwiGLUMLP,
    LigerFusedLinearCrossEntropyLoss,
    liger_rotary_pos_emb,
)

# è‡ªå®šä¹‰æ¨¡å‹æ¶æ„
class CustomLlamaDecoderLayer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.input_layernorm = LigerRMSNorm(config.hidden_size)  # ä½¿ç”¨ Liger RMSNorm
        self.self_attn = LlamaAttention(config)
        self.post_attention_layernorm = LigerRMSNorm(config.hidden_size)
        self.mlp = LigerSwiGLUMLP(config)  # ä½¿ç”¨ Liger SwiGLU
    
    def forward(self, hidden_states, position_ids, ...):
        # ä½¿ç”¨ Liger RoPE
        cos, sin = liger_rotary_pos_emb(...)
        # ... å…¶ä»–é€»è¾‘

# ä¼˜ç‚¹ï¼š
# - æœ€å¤§çµæ´»æ€§
# - å¯ä»¥æ··åˆä½¿ç”¨åŸç”Ÿå’Œ Liger å®ç°
# - é€‚åˆç ”ç©¶å’Œå®éªŒ
```

### 4.3 åˆ†å¸ƒå¼è®­ç»ƒæµç¨‹

#### 4.3.1 ä½¿ç”¨ FSDPï¼ˆPyTorch åŸç”Ÿï¼‰

```python
# train_fsdp.py

from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from liger_kernel.transformers import AutoLigerKernelForCausalLM

# åŠ è½½æ¨¡å‹ï¼ˆå·²è‡ªåŠ¨ä¼˜åŒ–ï¼‰
model = AutoLigerKernelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    torch_dtype=torch.bfloat16,
)

# ä½¿ç”¨ HuggingFace Trainer è‡ªåŠ¨å¤„ç† FSDP
training_args = TrainingArguments(
    ...,
    fsdp="full_shard auto_wrap",
    fsdp_config={
        "fsdp_transformer_layer_cls_to_wrap": ["LlamaDecoderLayer"],
    },
)

trainer = SFTTrainer(model=model, args=training_args, ...)
trainer.train()
```

#### 4.3.2 ä½¿ç”¨ DeepSpeed

```python
# deepspeed_config.json
{
  "train_batch_size": "auto",
  "train_micro_batch_size_per_gpu": "auto",
  "gradient_accumulation_steps": "auto",
  "fp16": {
    "enabled": false
  },
  "bf16": {
    "enabled": true
  },
  "zero_optimization": {
    "stage": 3,
    "offload_optimizer": {
      "device": "cpu"
    },
    "offload_param": {
      "device": "cpu"
    }
  }
}
```

```python
# train_deepspeed.py

from liger_kernel.transformers import AutoLigerKernelForCausalLM

model = AutoLigerKernelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    torch_dtype=torch.bfloat16,
)

training_args = TrainingArguments(
    ...,
    deepspeed="deepspeed_config.json",
)

trainer = SFTTrainer(model=model, args=training_args, ...)
trainer.train()
```

### 4.4 åè®­ç»ƒï¼ˆAlignmentï¼‰æµç¨‹

Liger-Kernel å¯¹é½ç®—æ³•ï¼ˆDPOã€ORPOç­‰ï¼‰æä¾›äº†é«˜è¾¾ 80% çš„å†…å­˜èŠ‚çœï¼š

```python
# train_orpo.py

from liger_kernel.chunked_loss import LigerFusedLinearORPOLoss
import torch.nn as nn

class ORPOModel(nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model
        self.lm_head = model.lm_head
        
        # ä½¿ç”¨ Liger çš„èåˆ ORPO æŸå¤±
        self.orpo_loss = LigerFusedLinearORPOLoss()
    
    def forward(self, input_ids, labels, ...):
        # è·å–éšè—çŠ¶æ€
        hidden_states = self.model(input_ids, ...).last_hidden_state
        
        # è®¡ç®— ORPO æŸå¤±ï¼ˆèåˆçº¿æ€§å±‚+ORPOï¼‰
        loss = self.orpo_loss(
            self.lm_head.weight,  # çº¿æ€§å±‚æƒé‡
            hidden_states,         # éšè—çŠ¶æ€
            labels,                # æ ‡ç­¾
        )
        
        return loss


# ä½¿ç”¨ç¤ºä¾‹
from liger_kernel.transformers import AutoLigerKernelForCausalLM

base_model = AutoLigerKernelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    torch_dtype=torch.bfloat16,
)

orpo_model = ORPOModel(base_model)

# è®­ç»ƒ
trainer = Trainer(model=orpo_model, ...)
trainer.train()
```

---

## 5. NPU å…¼å®¹æ€§åˆ†æ

### 5.1 å½“å‰ç¡¬ä»¶æ”¯æŒ

Liger-Kernel ç›®å‰æ”¯æŒä»¥ä¸‹ç¡¬ä»¶å¹³å°ï¼š

| ç¡¬ä»¶å¹³å° | æ”¯æŒçŠ¶æ€ | ä¾èµ– | CI çŠ¶æ€ |
|---------|---------|------|---------|
| **NVIDIA GPU** | âœ… å®Œå…¨æ”¯æŒ | CUDA, Triton | âœ… æœ‰ CI |
| **AMD GPU** | âœ… å®Œå…¨æ”¯æŒ | ROCm, Triton | âœ… æœ‰ CI |
| **Intel GPU** | âœ… å®éªŒæ€§æ”¯æŒ | XPU, Triton | âœ… æœ‰ CI |
| **å…¶ä»– NPU** | âŒ ä¸æ”¯æŒ | - | âŒ æ—  CI |

### 5.2 æŠ€æœ¯ä¾èµ–åˆ†æ

#### 5.2.1 Triton ä¾èµ–

Liger-Kernel çš„æ‰€æœ‰ç®—å­éƒ½åŸºäº **Triton** ç¼–å†™ï¼š

```python
# æ‰€æœ‰ç®—å­éƒ½ä½¿ç”¨ @triton.jit è£…é¥°å™¨
import triton
import triton.language as tl

@triton.jit
def my_kernel(...):
    # Triton DSL ä»£ç 
    pass
```

**Triton çš„ç¡¬ä»¶æ”¯æŒ**ï¼š
- âœ… NVIDIA GPUï¼šåŸç”Ÿæ”¯æŒï¼ˆCUDA åç«¯ï¼‰
- âœ… AMD GPUï¼šå®˜æ–¹æ”¯æŒï¼ˆROCm åç«¯ï¼‰
- âœ… Intel GPUï¼šå®˜æ–¹æ”¯æŒï¼ˆXPU åç«¯ï¼‰
- âŒ å…¶ä»– NPUï¼šéœ€è¦ Triton åç«¯æ”¯æŒ

#### 5.2.2 PyTorch ä¾èµ–

æ‰€æœ‰ç®—å­éƒ½é€šè¿‡ `torch.autograd.Function` é›†æˆï¼š

```python
class MyKernelFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        # è°ƒç”¨ Triton å†…æ ¸
        output = torch.empty_like(input)
        my_kernel[grid](output, input, ...)
        return output
```

**PyTorch çš„è®¾å¤‡æ”¯æŒ**ï¼š
- éœ€è¦ NPU æœ‰ PyTorch çš„å®˜æ–¹æ”¯æŒ
- éœ€è¦å®ç° `torch.Tensor` åœ¨ NPU ä¸Šçš„æ“ä½œ

### 5.3 NPU è¿ç§»çš„æŒ‘æˆ˜

#### æŒ‘æˆ˜ 1ï¼šTriton åç«¯ç¼ºå¤±

**é—®é¢˜**ï¼šå¤§å¤šæ•° NPUï¼ˆå¦‚åä¸ºæ˜‡è…¾ã€å¯’æ­¦çºªç­‰ï¼‰æ²¡æœ‰ Triton åç«¯ã€‚

**å½±å“**ï¼š
- æ— æ³•ç›´æ¥è¿è¡Œ `@triton.jit` ä¿®é¥°çš„å†…æ ¸
- éœ€è¦é‡å†™æ‰€æœ‰ç®—å­

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. **ç­‰å¾…å®˜æ–¹æ”¯æŒ**ï¼šç­‰å¾… Triton ç¤¾åŒºä¸º NPU å¼€å‘åç«¯
2. **æ‰‹åŠ¨ç§»æ¤**ï¼šå°† Triton ä»£ç ç¿»è¯‘ä¸º NPU åŸç”Ÿä»£ç ï¼ˆCANNã€BANGç­‰ï¼‰
3. **ä½¿ç”¨é€šç”¨ Python å®ç°**ï¼šæ€§èƒ½ä¼šå¤§å¹…ä¸‹é™

#### æŒ‘æˆ˜ 2ï¼šå†…æ ¸è¯­ä¹‰å·®å¼‚

ä¸åŒ NPU çš„ç¼–ç¨‹æ¨¡å‹å·®å¼‚ï¼š

| ç‰¹æ€§ | CUDA/Triton | æ˜‡è…¾ CANN | å¯’æ­¦çºª BANG |
|------|------------|-----------|------------|
| çº¿ç¨‹æ¨¡å‹ | Warp/Block | Cube/Block | NRAM/SRAM |
| å†…å­˜å±‚æ¬¡ | Global/Shared/Local | HBM/L1/UB | GDRAM/NRAM/WRAM |
| åŒæ­¥åŸè¯­ | `__syncthreads()` | `pipe_barrier()` | `__sync_cluster()` |

#### æŒ‘æˆ˜ 3ï¼šMonkey Patch çš„ NPU å…¼å®¹æ€§

**å¥½æ¶ˆæ¯**ï¼šMonkey Patch æœºåˆ¶æœ¬èº«æ˜¯ **NPU æ— å…³çš„**ã€‚

**åˆ†æ**ï¼š
```python
# Monkey Patch åªæ˜¯æ›¿æ¢ Python å¯¹è±¡
modeling_llama.LlamaRMSNorm = LigerRMSNorm

# è¿™ä¸ªæ“ä½œä¸æ¶‰åŠç¡¬ä»¶
# åªè¦æ›¿æ¢åçš„æ¨¡å—èƒ½åœ¨ NPU ä¸Šè¿è¡Œå³å¯
```

**å‰ææ¡ä»¶**ï¼š
- æ›¿æ¢åçš„ç®—å­å¿…é¡»æ”¯æŒ NPU
- PyTorch åœ¨ NPU ä¸Šæ­£å¸¸å·¥ä½œ

### 5.4 NPU è¿ç§»å¯è¡Œæ€§è¯„ä¼°

#### 5.4.1 ç›´æ¥è¿ç§»ï¼ˆä¸å¯è¡Œï¼‰

```python
# âŒ è¿™ä¸ä¼šå·¥ä½œ
from liger_kernel.transformers import apply_liger_kernel_to_llama

apply_liger_kernel_to_llama()  # åº•å±‚æ˜¯ Triton ä»£ç ï¼ŒNPU æ— æ³•è¿è¡Œ
model = AutoModelForCausalLM.from_pretrained("...", device="npu")
```

**åŸå› **ï¼šTriton å†…æ ¸æ— æ³•åœ¨ NPU ä¸Šç¼–è¯‘å’Œæ‰§è¡Œã€‚

#### 5.4.2 éƒ¨åˆ†è¿ç§»ï¼ˆç†è®ºå¯è¡Œï¼Œå·¥ä½œé‡å¤§ï¼‰

**æ­¥éª¤**ï¼š

1. **é‡å†™ç®—å­**ï¼šä¸º NPU é‡æ–°å®ç°æ‰€æœ‰ Triton å†…æ ¸

```python
# ç¤ºä¾‹ï¼šä¸ºæ˜‡è…¾ NPU é‡å†™ RMSNorm

# åŸå§‹ Triton å®ç°
@triton.jit
def _rms_norm_kernel(...):
    # Triton DSL ä»£ç 
    pass

# æ˜‡è…¾ NPU å®ç°ï¼ˆä¼ªä»£ç ï¼‰
import torch_npu  # æ˜‡è…¾çš„ PyTorch æ‰©å±•

@torch.library.custom_op("liger::rms_norm_npu", mutates_args=())
def rms_norm_npu(x: torch.Tensor, weight: torch.Tensor, eps: float) -> torch.Tensor:
    # è°ƒç”¨æ˜‡è…¾çš„ AscendC ç®—å­
    return torch_npu.npu_rms_norm(x, weight, eps)
```

2. **é‡å†™ Autograd Function**

```python
class LigerRMSNormFunctionNPU(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, weight, eps):
        # è°ƒç”¨ NPU ç®—å­
        output = rms_norm_npu(x, weight, eps)
        ctx.save_for_backward(x, weight)
        ctx.eps = eps
        return output
    
    @staticmethod
    def backward(ctx, grad_output):
        x, weight = ctx.saved_tensors
        # è°ƒç”¨ NPU åå‘ç®—å­
        grad_x = rms_norm_npu_backward(grad_output, x, weight, ctx.eps)
        grad_weight = ...
        return grad_x, grad_weight, None
```

3. **ä¿æŒ Monkey Patch ä¸å˜**

```python
# Monkey Patch æœºåˆ¶æ— éœ€ä¿®æ”¹
def apply_liger_kernel_to_llama_npu(...):
    from transformers.models.llama import modeling_llama
    
    # æ›¿æ¢ä¸º NPU ç‰ˆæœ¬çš„ç®—å­
    modeling_llama.LlamaRMSNorm = LigerRMSNormNPU
    # ... å…¶ä»–æ›¿æ¢
```

**å·¥ä½œé‡ä¼°ç®—**ï¼š
- éœ€è¦é‡å†™çº¦ **20+ ä¸ªæ ¸å¿ƒç®—å­**
- æ¯ä¸ªç®—å­éœ€è¦å®ç°å‰å‘å’Œåå‘ä¼ æ’­
- éœ€è¦å¤§é‡çš„æ€§èƒ½ä¼˜åŒ–å’Œæµ‹è¯•
- **é¢„è®¡å·¥ä½œé‡ï¼š3-6 ä¸ªæœˆï¼ˆå¯¹äºç†Ÿæ‚‰ NPU ç¼–ç¨‹çš„å›¢é˜Ÿï¼‰**

#### 5.4.3 æ··åˆæ–¹æ¡ˆï¼ˆçŸ­æœŸå¯è¡Œï¼‰

**ç­–ç•¥**ï¼šåªè¿ç§»ç“¶é¢ˆç®—å­ï¼Œå…¶ä»–ä½¿ç”¨ PyTorch åŸç”Ÿå®ç°ã€‚

```python
def apply_liger_kernel_to_llama_npu(
    rope: bool = False,        # NPU æš‚ä¸ä¼˜åŒ–
    rms_norm: bool = True,     # âœ… ç§»æ¤åˆ° NPU
    swiglu: bool = False,      # NPU æš‚ä¸ä¼˜åŒ–
    fused_linear_cross_entropy: bool = True,  # âœ… ç§»æ¤åˆ° NPU
):
    from transformers.models.llama import modeling_llama
    
    # åªæ›¿æ¢å·²ç§»æ¤çš„ç®—å­
    if rms_norm:
        modeling_llama.LlamaRMSNorm = LigerRMSNormNPU
    
    if fused_linear_cross_entropy:
        modeling_llama.LlamaForCausalLM.forward = llama_lce_forward_npu
```

**ä¼˜ç‚¹**ï¼š
- å¯ä»¥é€æ­¥è¿ç§»
- å¿«é€ŸéªŒè¯æ”¶ç›Š
- é™ä½é£é™©

**ç¼ºç‚¹**ï¼š
- æ”¶ç›Šæœ‰é™ï¼ˆå¯èƒ½åªæœ‰ 10-20% æå‡ï¼‰
- æ— æ³•äº«å—å®Œæ•´ä¼˜åŒ–

### 5.5 ç»“è®ºï¼šMonkey Patch å¯ä»¥è¿ç§»å—ï¼Ÿ

**ç­”æ¡ˆï¼šç†è®ºä¸Šå¯ä»¥ï¼Œä½†å®è·µä¸­å›°éš¾é‡é‡ã€‚**

#### âœ… Monkey Patch æœºåˆ¶æœ¬èº«æ˜¯å¯è¿ç§»çš„

- Monkey Patch æ˜¯çº¯ Python å±‚é¢çš„æ“ä½œ
- ä¸ä¾èµ–ç‰¹å®šç¡¬ä»¶
- åªè¦æ›¿æ¢çš„æ¨¡å—æ”¯æŒ NPUï¼Œpatch å°±èƒ½å·¥ä½œ

#### âŒ åº•å±‚ç®—å­å®ç°ä¸å¯è¿ç§»

- Liger-Kernel çš„æ‰€æœ‰ç®—å­éƒ½åŸºäº Triton
- Triton ç›®å‰ä¸æ”¯æŒå¤§å¤šæ•° NPU
- éœ€è¦ä¸ºæ¯ä¸ª NPU é‡æ–°å®ç°æ‰€æœ‰ç®—å­

#### ğŸ“Š è¿ç§»å¯è¡Œæ€§æ€»ç»“

| NPU ç±»å‹ | ç›´æ¥è¿ç§» | éƒ¨åˆ†è¿ç§» | å®Œå…¨é‡å†™ | é¢„è®¡å·¥ä½œé‡ |
|---------|---------|---------|---------|-----------|
| **åä¸ºæ˜‡è…¾** | âŒ | âš ï¸ å¯è¡Œ | âœ… å¯è¡Œ | 3-6 ä¸ªæœˆ |
| **å¯’æ­¦çºª** | âŒ | âš ï¸ å¯è¡Œ | âœ… å¯è¡Œ | 3-6 ä¸ªæœˆ |
| **ç‡§åŸ** | âŒ | âš ï¸ å¯è¡Œ | âœ… å¯è¡Œ | 3-6 ä¸ªæœˆ |
| **å£ä»** | âŒ | âš ï¸ å¯è¡Œ | âœ… å¯è¡Œ | 3-6 ä¸ªæœˆ |

#### å»ºè®®çš„è¿ç§»è·¯å¾„

1. **çŸ­æœŸï¼ˆ1 ä¸ªæœˆï¼‰**ï¼š
   - éªŒè¯ PyTorch åœ¨ç›®æ ‡ NPU ä¸Šçš„åŸºæœ¬åŠŸèƒ½
   - è¯†åˆ«æ€§èƒ½ç“¶é¢ˆç®—å­
   - ç§»æ¤ 1-2 ä¸ªå…³é”®ç®—å­ä½œä¸º POC

2. **ä¸­æœŸï¼ˆ3 ä¸ªæœˆï¼‰**ï¼š
   - ç§»æ¤æ ¸å¿ƒç®—å­ï¼ˆRMSNormã€CrossEntropyã€SwiGLUï¼‰
   - å»ºç«‹æµ‹è¯•æ¡†æ¶ç¡®ä¿æ­£ç¡®æ€§
   - è¿›è¡Œæ€§èƒ½è°ƒä¼˜

3. **é•¿æœŸï¼ˆ6 ä¸ªæœˆï¼‰**ï¼š
   - å®Œæˆæ‰€æœ‰ç®—å­ç§»æ¤
   - å®ç°ä¸ NVIDIA GPU ç›¸å½“çš„æ€§èƒ½
   - è´¡çŒ®å›å¼€æºç¤¾åŒº

---

## 6. å•å…ƒæµ‹è¯•ä½¿ç”¨æ–¹æ³•

### 6.1 æµ‹è¯•æ¡†æ¶

Liger-Kernel ä½¿ç”¨ **pytest** ä½œä¸ºæµ‹è¯•æ¡†æ¶ã€‚

#### 6.1.1 æµ‹è¯•ç›®å½•ç»“æ„

```
test/
â”œâ”€â”€ conftest.py                    # pytest é…ç½®
â”œâ”€â”€ utils.py                       # æµ‹è¯•å·¥å…·å‡½æ•°
â”œâ”€â”€ transformers/                  # Transformers ç›¸å…³æµ‹è¯•
â”‚   â”œâ”€â”€ test_rms_norm.py
â”‚   â”œâ”€â”€ test_cross_entropy.py
â”‚   â”œâ”€â”€ test_swiglu.py
â”‚   â”œâ”€â”€ test_rope.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ chunked_loss/                  # åˆ†å—æŸå¤±æµ‹è¯•
â”‚   â”œâ”€â”€ test_dpo_loss.py
â”‚   â”œâ”€â”€ test_orpo_loss.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ convergence/                   # æ”¶æ•›æ€§æµ‹è¯•
â”‚   â”œâ”€â”€ fp32/
â”‚   â”‚   â””â”€â”€ test_mini_models.py
â”‚   â””â”€â”€ bf16/
â”‚       â””â”€â”€ test_mini_models.py
â””â”€â”€ triton/                        # Triton ç›¸å…³æµ‹è¯•
    â””â”€â”€ test_triton_monkey_patch.py
```

### 6.2 åŸºæœ¬æµ‹è¯•ç¤ºä¾‹

#### 6.2.1 ç®—å­æ­£ç¡®æ€§æµ‹è¯•

```python
# test/transformers/test_rms_norm.py

import pytest
import torch
from test.utils import assert_verbose_allclose, set_seed

from liger_kernel.transformers.rms_norm import LigerRMSNorm


# å®šä¹‰å‚è€ƒå®ç°
class ReferenceRMSNorm(torch.nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        super().__init__()
        self.weight = torch.nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps
    
    def forward(self, hidden_states):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)


# å‚æ•°åŒ–æµ‹è¯•ï¼šæµ‹è¯•ä¸åŒçš„å½¢çŠ¶å’Œæ•°æ®ç±»å‹
@pytest.mark.parametrize(
    "batch_size, seq_len, hidden_size",
    [
        (2, 128, 512),
        (4, 256, 1024),
        (1, 512, 2048),
    ],
)
@pytest.mark.parametrize(
    "dtype, atol, rtol",
    [
        (torch.float32, 1e-5, 1e-6),
        (torch.bfloat16, 2e-2, 2e-2),
    ],
)
def test_rms_norm_correctness(batch_size, seq_len, hidden_size, dtype, atol, rtol):
    # è®¾ç½®éšæœºç§å­
    set_seed(42)
    
    # åˆ›å»ºè¾“å…¥
    x = torch.randn(batch_size, seq_len, hidden_size, dtype=dtype, device="cuda")
    x.requires_grad = True
    
    # å‚è€ƒå®ç°
    ref_norm = ReferenceRMSNorm(hidden_size).cuda().to(dtype)
    ref_output = ref_norm(x)
    ref_output.sum().backward()
    ref_grad = x.grad.clone()
    
    # Liger å®ç°
    x.grad = None
    liger_norm = LigerRMSNorm(hidden_size).cuda().to(dtype)
    liger_norm.weight.data.copy_(ref_norm.weight.data)
    liger_output = liger_norm(x)
    liger_output.sum().backward()
    liger_grad = x.grad.clone()
    
    # éªŒè¯å‰å‘ä¼ æ’­
    assert_verbose_allclose(
        liger_output,
        ref_output,
        atol=atol,
        rtol=rtol,
        msg="RMSNorm forward output mismatch"
    )
    
    # éªŒè¯åå‘ä¼ æ’­
    assert_verbose_allclose(
        liger_grad,
        ref_grad,
        atol=atol,
        rtol=rtol,
        msg="RMSNorm backward gradient mismatch"
    )
```

#### 6.2.2 æ€§èƒ½æµ‹è¯•

```python
# benchmark/scripts/benchmark_rms_norm.py

import torch
import triton

from liger_kernel.transformers.rms_norm import LigerRMSNorm


@triton.testing.perf_report(
    triton.testing.Benchmark(
        x_names=["seq_len"],  # æ¨ªè½´å‚æ•°
        x_vals=[2**i for i in range(10, 14)],  # 1024, 2048, 4096, 8192
        line_arg="provider",  # å›¾ä¾‹
        line_vals=["torch", "liger"],
        line_names=["PyTorch", "Liger"],
        styles=[("blue", "-"), ("red", "-")],
        ylabel="ms",  # çºµè½´æ ‡ç­¾
        plot_name="rms-norm-performance",
        args={"hidden_size": 4096},  # å›ºå®šå‚æ•°
    )
)
def benchmark_rms_norm(seq_len, hidden_size, provider):
    x = torch.randn(1, seq_len, hidden_size, device="cuda", dtype=torch.bfloat16)
    
    if provider == "torch":
        norm = ReferenceRMSNorm(hidden_size).cuda()
    else:
        norm = LigerRMSNorm(hidden_size).cuda()
    
    # é¢„çƒ­
    for _ in range(10):
        _ = norm(x)
    
    # æµ‹é‡
    quantiles = [0.5, 0.2, 0.8]
    ms, min_ms, max_ms = triton.testing.do_bench(lambda: norm(x), quantiles=quantiles)
    
    return ms, max_ms, min_ms


if __name__ == "__main__":
    benchmark_rms_norm.run(print_data=True, show_plots=True)
```

#### 6.2.3 æ”¶æ•›æ€§æµ‹è¯•

```python
# test/convergence/fp32/test_mini_models.py

import pytest
import torch
from transformers import LlamaConfig, LlamaForCausalLM
from liger_kernel.transformers import apply_liger_kernel_to_llama


def run_training(use_liger=False, num_steps=100):
    """è¿è¡Œå°è§„æ¨¡è®­ç»ƒ"""
    set_seed(42)
    
    # é…ç½®å°æ¨¡å‹
    config = LlamaConfig(
        vocab_size=1000,
        hidden_size=256,
        intermediate_size=512,
        num_hidden_layers=4,
        num_attention_heads=8,
    )
    
    # åº”ç”¨ Liger ä¼˜åŒ–
    if use_liger:
        apply_liger_kernel_to_llama()
    
    # åˆ›å»ºæ¨¡å‹
    model = LlamaForCausalLM(config).cuda()
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    
    # è®­ç»ƒå¾ªç¯
    losses = []
    for step in range(num_steps):
        input_ids = torch.randint(0, 1000, (2, 64), device="cuda")
        labels = input_ids.clone()
        
        outputs = model(input_ids=input_ids, labels=labels)
        loss = outputs.loss
        
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        
        losses.append(loss.item())
    
    return losses


def test_convergence():
    """æµ‹è¯• Liger ä¼˜åŒ–æ˜¯å¦å½±å“æ”¶æ•›æ€§"""
    # ä¸ä½¿ç”¨ Liger
    torch_losses = run_training(use_liger=False, num_steps=100)
    
    # ä½¿ç”¨ Liger
    liger_losses = run_training(use_liger=True, num_steps=100)
    
    # éªŒè¯æœ€ç»ˆæŸå¤±ç›¸è¿‘
    assert abs(torch_losses[-1] - liger_losses[-1]) < 0.1, \
        f"Convergence mismatch: torch={torch_losses[-1]:.4f}, liger={liger_losses[-1]:.4f}"
    
    # éªŒè¯æŸå¤±æ›²çº¿ç›¸å…³æ€§
    import numpy as np
    correlation = np.corrcoef(torch_losses, liger_losses)[0, 1]
    assert correlation > 0.95, f"Loss curves diverge: correlation={correlation:.4f}"
```

### 6.3 è¿è¡Œæµ‹è¯•

#### 6.3.1 è¿è¡Œæ‰€æœ‰æµ‹è¯•

```bash
# å®‰è£…æµ‹è¯•ä¾èµ–
pip install -e ".[dev]"

# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest test/

# æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
pytest test/ -v

# æ˜¾ç¤ºæ‰“å°ä¿¡æ¯
pytest test/ -s
```

#### 6.3.2 è¿è¡Œç‰¹å®šæµ‹è¯•

```bash
# è¿è¡Œå•ä¸ªæ–‡ä»¶
pytest test/transformers/test_rms_norm.py

# è¿è¡Œå•ä¸ªæµ‹è¯•å‡½æ•°
pytest test/transformers/test_rms_norm.py::test_rms_norm_correctness

# è¿è¡Œç‰¹å®šå‚æ•°çš„æµ‹è¯•
pytest test/transformers/test_rms_norm.py::test_rms_norm_correctness[2-128-512-float32-1e-5-1e-6]

# è¿è¡ŒåŒ…å«ç‰¹å®šå…³é”®è¯çš„æµ‹è¯•
pytest test/ -k "rms_norm"
```

#### 6.3.3 å¹¶è¡Œæµ‹è¯•

```bash
# ä½¿ç”¨å¤šä¸ªè¿›ç¨‹å¹¶è¡Œæµ‹è¯•ï¼ˆéœ€è¦ pytest-xdistï¼‰
pytest test/ -n auto

# æŒ‡å®šè¿›ç¨‹æ•°
pytest test/ -n 4
```

#### 6.3.4 ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š

```bash
# è¿è¡Œæµ‹è¯•å¹¶ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Šï¼ˆéœ€è¦ pytest-covï¼‰
pytest test/ --cov=src/liger_kernel --cov-report=html

# æŸ¥çœ‹æŠ¥å‘Š
# æ‰“å¼€ htmlcov/index.html
```

#### 6.3.5 è·³è¿‡æ…¢é€Ÿæµ‹è¯•

```bash
# åªè¿è¡Œå¿«é€Ÿæµ‹è¯•ï¼ˆè·³è¿‡æ”¶æ•›æ€§æµ‹è¯•ï¼‰
pytest test/ -m "not slow"

# è¿è¡Œç‰¹å®šæ ‡è®°çš„æµ‹è¯•
pytest test/ -m "correctness"
```

### 6.4 æµ‹è¯•å·¥å…·å‡½æ•°

#### 6.4.1 `assert_verbose_allclose`

```python
# test/utils.py

def assert_verbose_allclose(
    actual: torch.Tensor,
    expected: torch.Tensor,
    atol: float = 1e-5,
    rtol: float = 1e-5,
    msg: str = "",
):
    """
    æ–­è¨€ä¸¤ä¸ªå¼ é‡è¿‘ä¼¼ç›¸ç­‰ï¼Œå¹¶æä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
    """
    if not torch.allclose(actual, expected, atol=atol, rtol=rtol):
        diff = (actual - expected).abs()
        max_diff = diff.max().item()
        mean_diff = diff.mean().item()
        
        error_msg = (
            f"{msg}\n"
            f"Max difference: {max_diff}\n"
            f"Mean difference: {mean_diff}\n"
            f"Tolerance: atol={atol}, rtol={rtol}\n"
            f"Shapes: actual={actual.shape}, expected={expected.shape}\n"
            f"Dtypes: actual={actual.dtype}, expected={expected.dtype}"
        )
        raise AssertionError(error_msg)
```

#### 6.4.2 `set_seed`

```python
# test/utils.py

def set_seed(seed=42):
    """å›ºå®šæ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§"""
    import random
    import numpy as np
    
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    
    # ç¡®ä¿ç¡®å®šæ€§è¡Œä¸º
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
```

#### 6.4.3 `supports_bfloat16`

```python
# test/utils.py

def supports_bfloat16():
    """æ£€æŸ¥å½“å‰ GPU æ˜¯å¦æ”¯æŒ bfloat16"""
    if not torch.cuda.is_available():
        return False
    
    # Ampere (SM 80) åŠä»¥ä¸Šæ”¯æŒ bfloat16
    major, minor = torch.cuda.get_device_capability()
    return major >= 8
```

### 6.5 CI/CD æµ‹è¯•

Liger-Kernel ä½¿ç”¨ GitHub Actions è¿›è¡ŒæŒç»­é›†æˆï¼š

```yaml
# .github/workflows/nvi-ci.yml

name: NVIDIA CI

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    container:
      image: nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Install dependencies
        run: |
          pip install -e ".[dev]"
      
      - name: Run tests
        run: |
          pytest test/ -v --cov=src/liger_kernel
      
      - name: Upload coverage
        uses: codecov/codecov-action@v3
```

### 6.6 ç¼–å†™è‡ªå®šä¹‰æµ‹è¯•

#### ç¤ºä¾‹ï¼šä¸ºæ–°ç®—å­æ·»åŠ æµ‹è¯•

å‡è®¾ä½ å®ç°äº†ä¸€ä¸ªæ–°çš„ `LigerLayerScale` ç®—å­ï¼š

```python
# test/transformers/test_layer_scale.py

import pytest
import torch
from test.utils import assert_verbose_allclose, set_seed

from liger_kernel.transformers.layer_scale import LigerLayerScale


class ReferenceLayerScale(torch.nn.Module):
    """å‚è€ƒå®ç°"""
    def __init__(self, dim, init_value=1e-5):
        super().__init__()
        self.scale = torch.nn.Parameter(torch.ones(dim) * init_value)
    
    def forward(self, x):
        return x * self.scale


@pytest.mark.parametrize("batch_size, seq_len, dim", [(2, 128, 512), (4, 256, 1024)])
@pytest.mark.parametrize("dtype", [torch.float32, torch.bfloat16])
@pytest.mark.parametrize("init_value", [1e-5, 1e-2])
def test_layer_scale_correctness(batch_size, seq_len, dim, dtype, init_value):
    set_seed(42)
    
    # åˆ›å»ºè¾“å…¥
    x = torch.randn(batch_size, seq_len, dim, dtype=dtype, device="cuda", requires_grad=True)
    
    # å‚è€ƒå®ç°
    ref_layer_scale = ReferenceLayerScale(dim, init_value).cuda().to(dtype)
    ref_output = ref_layer_scale(x)
    ref_output.sum().backward()
    ref_grad = x.grad.clone()
    ref_scale_grad = ref_layer_scale.scale.grad.clone()
    
    # Liger å®ç°
    x.grad = None
    liger_layer_scale = LigerLayerScale(dim, init_value).cuda().to(dtype)
    liger_layer_scale.scale.data.copy_(ref_layer_scale.scale.data)
    liger_output = liger_layer_scale(x)
    liger_output.sum().backward()
    liger_grad = x.grad.clone()
    liger_scale_grad = liger_layer_scale.scale.grad.clone()
    
    # éªŒè¯
    assert_verbose_allclose(liger_output, ref_output, atol=1e-5, rtol=1e-5)
    assert_verbose_allclose(liger_grad, ref_grad, atol=1e-5, rtol=1e-5)
    assert_verbose_allclose(liger_scale_grad, ref_scale_grad, atol=1e-5, rtol=1e-5)


@pytest.mark.benchmark
def test_layer_scale_performance():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    import time
    
    dim = 4096
    x = torch.randn(8, 2048, dim, device="cuda")
    
    # é¢„çƒ­
    ref = ReferenceLayerScale(dim).cuda()
    liger = LigerLayerScale(dim).cuda()
    
    for _ in range(10):
        _ = ref(x)
        _ = liger(x)
    
    # æµ‹é‡å‚è€ƒå®ç°
    torch.cuda.synchronize()
    start = time.time()
    for _ in range(100):
        _ = ref(x)
    torch.cuda.synchronize()
    ref_time = time.time() - start
    
    # æµ‹é‡ Liger å®ç°
    torch.cuda.synchronize()
    start = time.time()
    for _ in range(100):
        _ = liger(x)
    torch.cuda.synchronize()
    liger_time = time.time() - start
    
    speedup = ref_time / liger_time
    print(f"Speedup: {speedup:.2f}x")
    
    # éªŒè¯è‡³å°‘ä¸æ¯”å‚è€ƒæ…¢
    assert speedup >= 0.9, f"Performance regression: speedup={speedup:.2f}x"
```

---

## æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **é¡¹ç›®å®šä½**ï¼šLiger-Kernel æ˜¯ä¸º LLM è®­ç»ƒä¼˜åŒ–çš„é«˜æ€§èƒ½ Triton ç®—å­åº“ï¼Œé€šè¿‡å†…æ ¸èåˆå’Œå†…å­˜ä¼˜åŒ–æä¾›æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

2. **ç®—å­æ³¨å†Œ**ï¼š**ä¸æ˜¯** PyTorch å®˜æ–¹æ³¨å†Œæœºåˆ¶ï¼Œè€Œæ˜¯é€šè¿‡ Triton JIT ç¼–è¯‘ + PyTorch Autograd Function å®ç°ã€‚

3. **Monkey Patch**ï¼šæ ¸å¿ƒå®ç°æœºåˆ¶ï¼Œé€šè¿‡åŠ¨æ€æ›¿æ¢ transformers æ¨¡å—ä¸­çš„ç±»å’Œå‡½æ•°ï¼Œå®ç°æ— ä¾µå…¥å¼ä¼˜åŒ–ã€‚æ”¯æŒæ¨¡å—çº§å’Œå®ä¾‹çº§ patchã€‚

4. **ä½¿ç”¨æµç¨‹**ï¼šæä¾›ä¸‰ç§ä½¿ç”¨æ–¹å¼ï¼ˆè‡ªåŠ¨ã€æ‰‹åŠ¨ã€åº•å±‚ï¼‰ï¼Œæ”¯æŒ FSDPã€DeepSpeed ç­‰åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ã€‚

5. **NPU å…¼å®¹æ€§**ï¼š
   - âœ… Monkey Patch æœºåˆ¶æœ¬èº«æ˜¯å¯è¿ç§»çš„
   - âŒ åº•å±‚ Triton ç®—å­éœ€è¦å®Œå…¨é‡å†™
   - âš ï¸ é¢„è®¡éœ€è¦ 3-6 ä¸ªæœˆçš„ç§»æ¤å·¥ä½œ

6. **æµ‹è¯•ä½“ç³»**ï¼šåŸºäº pytest çš„å®Œå–„æµ‹è¯•æ¡†æ¶ï¼ŒåŒ…æ‹¬æ­£ç¡®æ€§æµ‹è¯•ã€æ€§èƒ½æµ‹è¯•å’Œæ”¶æ•›æ€§æµ‹è¯•ã€‚

### æ¨èå®è·µ

- **å…¥é—¨ç”¨æˆ·**ï¼šä½¿ç”¨ `AutoLigerKernelForCausalLM` å¿«é€Ÿä¸Šæ‰‹
- **é«˜çº§ç”¨æˆ·**ï¼šä½¿ç”¨æ¨¡å‹ç‰¹å®šçš„ patch API è¿›è¡Œç²¾ç»†æ§åˆ¶
- **ç ”ç©¶äººå‘˜**ï¼šä½¿ç”¨åº•å±‚ API æ„å»ºè‡ªå®šä¹‰æ¨¡å‹
- **NPU å¼€å‘è€…**ï¼šå…³æ³¨ Triton ç¤¾åŒºçš„ NPU åç«¯å¼€å‘ï¼Œæˆ–è€ƒè™‘è‡ªè¡Œç§»æ¤æ ¸å¿ƒç®—å­

### å‚è€ƒèµ„æº

- **å®˜æ–¹æ–‡æ¡£**ï¼šhttps://linkedin.github.io/Liger-Kernel/
- **GitHub ä»“åº“**ï¼šhttps://github.com/linkedin/Liger-Kernel
- **æŠ€æœ¯æŠ¥å‘Š**ï¼šhttps://arxiv.org/pdf/2410.10989
- **Discord ç¤¾åŒº**ï¼šhttps://discord.gg/gpumode

---


