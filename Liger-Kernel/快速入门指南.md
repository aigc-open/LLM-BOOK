# Liger-Kernel å¿«é€Ÿå…¥é—¨æŒ‡å—

## ä¸€å¥è¯æ€»ç»“

Liger-Kernel æ˜¯ä¸€ä¸ªåŸºäº Triton çš„ LLM è®­ç»ƒä¼˜åŒ–åº“ï¼Œ**ä¸€è¡Œä»£ç **å³å¯æå‡ 20% è®­ç»ƒé€Ÿåº¦ï¼Œå‡å°‘ 60% å†…å­˜å ç”¨ã€‚

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…

```bash
# ç¨³å®šç‰ˆ
pip install liger-kernel

# æˆ–è€…ä»æºç å®‰è£…
git clone https://github.com/linkedin/Liger-Kernel.git
cd Liger-Kernel
pip install -e .
```

### 2. ä¸‰ç§ä½¿ç”¨æ–¹å¼

#### æ–¹å¼ 1ï¼šè‡ªåŠ¨ä¼˜åŒ–ï¼ˆæœ€ç®€å•ï¼‰â­

```python
from liger_kernel.transformers import AutoLigerKernelForCausalLM

# ä¸€è¡Œä»£ç ï¼Œè‡ªåŠ¨ä¼˜åŒ–
model = AutoLigerKernelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
```

#### æ–¹å¼ 2ï¼šæ‰‹åŠ¨ Patchï¼ˆæ¨èï¼‰

```python
from liger_kernel.transformers import apply_liger_kernel_to_llama
import transformers

# åœ¨åŠ è½½æ¨¡å‹å‰åº”ç”¨ä¼˜åŒ–
apply_liger_kernel_to_llama()

# æ­£å¸¸åŠ è½½æ¨¡å‹
model = transformers.AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
```

#### æ–¹å¼ 3ï¼šåº•å±‚ APIï¼ˆæœ€çµæ´»ï¼‰

```python
from liger_kernel.transformers import (
    LigerRMSNorm,
    LigerSwiGLUMLP,
    LigerFusedLinearCrossEntropyLoss,
)

# ç›´æ¥ä½¿ç”¨å„ä¸ªç®—å­æ„å»ºæ¨¡å‹
```

### 3. å®Œæ•´è®­ç»ƒç¤ºä¾‹

```python
from liger_kernel.transformers import AutoLigerKernelForCausalLM
from transformers import AutoTokenizer, TrainingArguments
from trl import SFTTrainer
from datasets import load_dataset

# 1. åŠ è½½æ•°æ®
dataset = load_dataset("tatsu-lab/alpaca")["train"]
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

# 2. åŠ è½½æ¨¡å‹ï¼ˆè‡ªåŠ¨åº”ç”¨ Liger ä¼˜åŒ–ï¼‰
model = AutoLigerKernelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    torch_dtype="auto",
    use_cache=False,  # è®­ç»ƒæ—¶å¿…é¡»è®¾ç½®
)

# 3. è®­ç»ƒ
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    tokenizer=tokenizer,
    args=TrainingArguments(
        output_dir="./output",
        num_train_epochs=3,
        per_device_train_batch_size=4,
        bf16=True,
    ),
)

trainer.train()
```

## æ ¸å¿ƒé—®ç­”

### Q1: é¡¹ç›®è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ

**A**: è§£å†³ LLM è®­ç»ƒçš„ä¸‰å¤§ç—›ç‚¹ï¼š
- ğŸš€ **è®­ç»ƒæ…¢**ï¼šæå‡ 20% è®­ç»ƒååé‡
- ğŸ’¾ **å†…å­˜ä¸å¤Ÿ**ï¼šå‡å°‘ 60% å†…å­˜å ç”¨ï¼Œæ”¯æŒæ›´é•¿ä¸Šä¸‹æ–‡ï¼ˆ4K â†’ 16Kï¼‰
- ğŸ”§ **é›†æˆå¤æ‚**ï¼šä¸€è¡Œä»£ç å³å¯ä½¿ç”¨

### Q2: æ˜¯ Torch æ³¨å†Œçš„ç®—å­å—ï¼Ÿ

**A**: **ä¸æ˜¯**ã€‚Liger-Kernel ä½¿ç”¨ä»¥ä¸‹æŠ€æœ¯æ ˆï¼š

1. **Triton JIT** ç¼–å†™å†…æ ¸ï¼ˆ`@triton.jit`ï¼‰
2. **PyTorch Autograd Function** å®ç°è‡ªåŠ¨å¾®åˆ†
3. **Monkey Patching** æ›¿æ¢ HuggingFace å®ç°

```python
# æ ¸å¿ƒæŠ€æœ¯æ ˆ
@triton.jit                           # 1. Triton å†…æ ¸
def my_kernel(...): pass

class MyFunction(torch.autograd.Function):  # 2. Autograd
    def forward(ctx, x):
        my_kernel[grid](...)

class MyModule(nn.Module):            # 3. Module
    def forward(self, x):
        return MyFunction.apply(x)

# 4. Monkey Patch
transformers.models.llama.LlamaRMSNorm = MyModule
```

### Q3: å¦‚ä½•æ·»åŠ  Monkey Patchï¼Ÿ

**A**: ä¸‰æ­¥å®ç°ï¼š

```python
# æ­¥éª¤ 1ï¼šåˆ›å»º patch å‡½æ•°
def apply_liger_kernel_to_mymodel(
    rms_norm: bool = True,
    swiglu: bool = True,
    model: PreTrainedModel = None,
):
    from transformers.models.mymodel import modeling_mymodel
    
    # æ›¿æ¢ç±»
    if rms_norm:
        modeling_mymodel.MyModelRMSNorm = LigerRMSNorm
    if swiglu:
        modeling_mymodel.MyModelMLP = LigerSwiGLUMLP
    
    # å¦‚æœæ˜¯å·²å®ä¾‹åŒ–çš„æ¨¡å‹ï¼Œè¿˜éœ€æ›¿æ¢å®ä¾‹
    if model is not None:
        for layer in model.layers:
            if rms_norm:
                _patch_rms_norm_module(layer.norm)
            if swiglu:
                _patch_swiglu_module(layer.mlp, LigerSwiGLUMLP)

# æ­¥éª¤ 2ï¼šæ³¨å†Œåˆ°æ˜ å°„è¡¨
MODEL_TYPE_TO_APPLY_LIGER_FN["mymodel"] = apply_liger_kernel_to_mymodel

# æ­¥éª¤ 3ï¼šå¯¼å‡º API
# åœ¨ __init__.py ä¸­æ·»åŠ 
__all__.append("apply_liger_kernel_to_mymodel")
```

**å…³é”®ç‚¹**ï¼š
- âœ… æ¨¡å‹åˆå§‹åŒ–**å‰** patchï¼šæ€§èƒ½æ›´å¥½ï¼ˆæ¨èï¼‰
- âœ… æ¨¡å‹åˆå§‹åŒ–**å** patchï¼šéœ€è¦ä¼ å…¥ `model` å‚æ•°ï¼Œå¤„ç†å®ä¾‹å˜é‡

### Q4: æ•´ä¸ªé¡¹ç›®ä½¿ç”¨æµç¨‹ï¼Ÿ

**A**: 

```mermaid
graph LR
    A[å®‰è£… Liger-Kernel] --> B{é€‰æ‹©ä½¿ç”¨æ–¹å¼}
    B --> C[æ–¹å¼1: AutoModel]
    B --> D[æ–¹å¼2: Patch API]
    B --> E[æ–¹å¼3: åº•å±‚API]
    
    C --> F[åŠ è½½æ¨¡å‹]
    D --> G[å…ˆ patch] --> F
    E --> H[æ„å»ºæ¨¡å‹] --> F
    
    F --> I[é…ç½® Trainer]
    I --> J[å¼€å§‹è®­ç»ƒ]
    J --> K{åˆ†å¸ƒå¼?}
    K -->|æ˜¯| L[FSDP/DeepSpeed]
    K -->|å¦| M[å•å¡è®­ç»ƒ]
    
    L --> N[è®­ç»ƒå®Œæˆ]
    M --> N
    N --> O[ä¿å­˜æ¨¡å‹]
```

**è¯¦ç»†æµç¨‹**ï¼š

```python
# 1. å®‰è£…
pip install liger-kernel

# 2. å¯¼å…¥å¹¶ patchï¼ˆåœ¨æ¨¡å‹åŠ è½½å‰ï¼‰
from liger_kernel.transformers import apply_liger_kernel_to_llama
apply_liger_kernel_to_llama()

# 3. åŠ è½½æ¨¡å‹ï¼ˆå·²è‡ªåŠ¨ä¼˜åŒ–ï¼‰
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")

# 4. æ­£å¸¸è®­ç»ƒ
trainer = Trainer(model=model, ...)
trainer.train()
```

### Q5: å¯ä»¥è¿ç§»åˆ°å…¶ä»– NPU å—ï¼Ÿ

**A**: **ç†è®ºå¯è¡Œï¼Œä½†å·¥ä½œé‡å¤§**ã€‚

#### âœ… å¯è¿ç§»çš„éƒ¨åˆ†

- **Monkey Patch æœºåˆ¶**ï¼šçº¯ Python æ“ä½œï¼ŒNPU æ— å…³
- **é¡¹ç›®æ¶æ„**ï¼šæ¨¡å—åŒ–è®¾è®¡å¯ä»¥å¤ç”¨

#### âŒ éœ€è¦é‡å†™çš„éƒ¨åˆ†

- **æ‰€æœ‰ Triton å†…æ ¸**ï¼šçº¦ 20+ ä¸ªç®—å­
- **å‰å‘å’Œåå‘ä¼ æ’­**ï¼šæ¯ä¸ªç®—å­éƒ½éœ€è¦
- **æ€§èƒ½ä¼˜åŒ–**ï¼šé’ˆå¯¹ NPU çš„ç‰¹æ€§è°ƒä¼˜

#### ğŸ“Š è¿ç§»å·¥ä½œé‡ä¼°ç®—

| NPU å¹³å° | é¢„è®¡æ—¶é—´ | éš¾åº¦ | å¯è¡Œæ€§ |
|---------|---------|------|--------|
| åä¸ºæ˜‡è…¾ | 3-6 ä¸ªæœˆ | â­â­â­â­ | å¯è¡Œ |
| å¯’æ­¦çºª | 3-6 ä¸ªæœˆ | â­â­â­â­ | å¯è¡Œ |
| ç‡§åŸ | 3-6 ä¸ªæœˆ | â­â­â­â­ | å¯è¡Œ |

#### ğŸ›¤ï¸ æ¨èè¿ç§»è·¯å¾„

1. **é˜¶æ®µ 1**ï¼ˆ1 ä¸ªæœˆï¼‰ï¼šéªŒè¯ PyTorch åœ¨ NPU ä¸Šçš„å¯ç”¨æ€§
2. **é˜¶æ®µ 2**ï¼ˆ2 ä¸ªæœˆï¼‰ï¼šç§»æ¤æ ¸å¿ƒç®—å­ï¼ˆRMSNormã€CrossEntropyï¼‰
3. **é˜¶æ®µ 3**ï¼ˆ3 ä¸ªæœˆï¼‰ï¼šç§»æ¤å…¨éƒ¨ç®—å­å¹¶ä¼˜åŒ–æ€§èƒ½

#### æ ¸å¿ƒæŒ‘æˆ˜

```python
# âŒ Triton ä»£ç æ— æ³•ç›´æ¥åœ¨ NPU ä¸Šè¿è¡Œ
@triton.jit
def my_kernel(...):
    # Triton DSL ä»£ç 
    pass

# âœ… éœ€è¦ä¸º NPU é‡å†™
def my_kernel_npu(...):
    # ä½¿ç”¨ NPU çš„ç¼–ç¨‹æ¥å£ï¼ˆå¦‚ CANNã€BANGï¼‰
    pass
```

### Q6: å¦‚ä½•ä½¿ç”¨ unittestï¼Ÿ

**A**: ä½¿ç”¨ pytest æ¡†æ¶ã€‚

#### åŸºæœ¬å‘½ä»¤

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest test/

# è¿è¡Œç‰¹å®šæ–‡ä»¶
pytest test/transformers/test_rms_norm.py

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest test/transformers/test_rms_norm.py::test_rms_norm_correctness

# å¹¶è¡Œæµ‹è¯•ï¼ˆåŠ é€Ÿï¼‰
pytest test/ -n auto

# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest test/ --cov=src/liger_kernel --cov-report=html
```

#### æµ‹è¯•ç±»å‹

1. **æ­£ç¡®æ€§æµ‹è¯•**ï¼šéªŒè¯ç®—å­è¾“å‡ºä¸å‚è€ƒå®ç°ä¸€è‡´

```python
def test_correctness():
    # å‚è€ƒå®ç°
    ref_output = reference_implementation(input)
    
    # Liger å®ç°
    liger_output = liger_implementation(input)
    
    # éªŒè¯
    assert torch.allclose(liger_output, ref_output, atol=1e-5)
```

2. **æ€§èƒ½æµ‹è¯•**ï¼šæµ‹é‡åŠ é€Ÿæ¯”

```python
@triton.testing.perf_report(...)
def benchmark():
    # æµ‹é‡ PyTorch vs Liger
    pass
```

3. **æ”¶æ•›æ€§æµ‹è¯•**ï¼šéªŒè¯è®­ç»ƒæ”¶æ•›ä¸å—å½±å“

```python
def test_convergence():
    torch_losses = train_without_liger()
    liger_losses = train_with_liger()
    
    # éªŒè¯æŸå¤±æ›²çº¿ç›¸ä¼¼
    assert correlation(torch_losses, liger_losses) > 0.95
```

## æ”¯æŒçš„æ¨¡å‹

| æ¨¡å‹ç³»åˆ— | Patch API | æ”¯æŒçš„ç®—å­ |
|---------|----------|-----------|
| LLaMA 2/3/4 | `apply_liger_kernel_to_llama` | RoPE, RMSNorm, SwiGLU, CE, FusedLinearCE |
| Mistral | `apply_liger_kernel_to_mistral` | RoPE, RMSNorm, SwiGLU, CE, FusedLinearCE |
| Gemma 1/2/3 | `apply_liger_kernel_to_gemma` | RoPE, RMSNorm, GeGLU, CE, FusedLinearCE |
| Qwen 2/3 | `apply_liger_kernel_to_qwen2` | RoPE, RMSNorm, SwiGLU, CE, FusedLinearCE |
| Phi3 | `apply_liger_kernel_to_phi3` | RoPE, RMSNorm, SwiGLU, CE, FusedLinearCE |

å®Œæ•´åˆ—è¡¨è§ï¼šhttps://github.com/linkedin/Liger-Kernel#patching

## æ ¸å¿ƒç®—å­

### æ¨¡å‹ç®—å­

| ç®—å­ | API | ç”¨é€” |
|------|-----|------|
| RMSNorm | `LigerRMSNorm` | å½’ä¸€åŒ–å±‚ |
| LayerNorm | `LigerLayerNorm` | å½’ä¸€åŒ–å±‚ |
| RoPE | `liger_rotary_pos_emb` | ä½ç½®ç¼–ç  |
| SwiGLU | `LigerSwiGLUMLP` | æ¿€æ´»å‡½æ•° |
| GeGLU | `LigerGEGLUMLP` | æ¿€æ´»å‡½æ•° |
| CrossEntropy | `LigerCrossEntropyLoss` | æŸå¤±å‡½æ•° |
| FusedLinearCE | `LigerFusedLinearCrossEntropyLoss` | èåˆæŸå¤±ï¼ˆèŠ‚çœ 80% å†…å­˜ï¼‰ |

### å¯¹é½ç®—å­ï¼ˆåè®­ç»ƒï¼‰

| ç®—å­ | API | ç”¨é€” |
|------|-----|------|
| DPO | `LigerFusedLinearDPOLoss` | Direct Preference Optimization |
| ORPO | `LigerFusedLinearORPOLoss` | Odds Ratio Preference Optimization |
| CPO | `LigerFusedLinearCPOLoss` | Contrastive Preference Optimization |
| SimPO | `LigerFusedLinearSimPOLoss` | Simple Preference Optimization |
| KTO | `LigerFusedLinearKTOLoss` | Kahneman-Tversky Optimization |

## æ€§èƒ½æ•°æ®

### è®­ç»ƒé€Ÿåº¦

| æ¨¡å‹ | Batch Size | Seq Length | HF (tokens/s) | Liger (tokens/s) | æå‡ |
|------|-----------|-----------|---------------|------------------|------|
| LLaMA 3-8B | 8 | 4K | 2000 | 2400 | +20% |
| LLaMA 3-8B | 8 | 8K | 1200 | 1600 | +33% |
| LLaMA 3-8B | 8 | 16K | OOM | 800 | âœ… å¯ç”¨ |

### å†…å­˜å ç”¨

| æ¨¡å‹ | Context Length | HF Memory | Liger Memory | èŠ‚çœ |
|------|---------------|-----------|--------------|------|
| LLaMA 3-8B | 4K | 40 GB | 16 GB | -60% |
| LLaMA 3-8B | 8K | OOM | 28 GB | âœ… å¯ç”¨ |

### åè®­ç»ƒå†…å­˜

| ä»»åŠ¡ | HF Memory | Liger Memory | èŠ‚çœ |
|-----|-----------|--------------|------|
| DPO | 80 GB | 16 GB | -80% |
| ORPO | 80 GB | 16 GB | -80% |

## å¸¸è§é—®é¢˜

### Q: ä¼šå½±å“æ¨¡å‹ç²¾åº¦å—ï¼Ÿ

**A**: ä¸ä¼šã€‚æ‰€æœ‰ç®—å­éƒ½ç»è¿‡ä¸¥æ ¼æµ‹è¯•ï¼Œè®¡ç®—å®Œå…¨ç²¾ç¡®ï¼Œä¸ä½¿ç”¨ä»»ä½•è¿‘ä¼¼ã€‚

### Q: å…¼å®¹ Flash Attention å—ï¼Ÿ

**A**: å®Œå…¨å…¼å®¹ã€‚Liger-Kernel ä¸ Flash Attention äº’è¡¥ï¼Œå¯ä»¥åŒæ—¶ä½¿ç”¨ï¼š

```python
model = AutoLigerKernelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    attn_implementation="flash_attention_2",  # ä½¿ç”¨ Flash Attention
)
```

### Q: æ”¯æŒé‡åŒ–å—ï¼Ÿ

**A**: æ”¯æŒã€‚å¯ä»¥ä¸ bitsandbytesã€GPTQ ç­‰é‡åŒ–æ–¹æ³•é…åˆä½¿ç”¨ï¼š

```python
model = AutoLigerKernelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    load_in_4bit=True,  # 4-bit é‡åŒ–
)
```

### Q: æ¨ç†é˜¶æ®µå¯ä»¥ç”¨å—ï¼Ÿ

**A**: å¯ä»¥ï¼Œä½†æ”¶ç›Šæœ‰é™ã€‚Liger-Kernel ä¸»è¦ä¼˜åŒ–è®­ç»ƒé˜¶æ®µï¼Œæ¨ç†é˜¶æ®µå»ºè®®ä½¿ç”¨ vLLMã€TGI ç­‰æ¨ç†æ¡†æ¶ã€‚

### Q: å¦‚ä½•è°ƒè¯•ï¼Ÿ

**A**: 

```python
# 1. å•ç‹¬ç¦ç”¨æŸä¸ªä¼˜åŒ–
apply_liger_kernel_to_llama(
    rope=False,  # ç¦ç”¨ RoPE ä¼˜åŒ–
    rms_norm=True,
    swiglu=True,
)

# 2. å¯¹æ¯”æµ‹è¯•
# å…ˆè®­ç»ƒä¸å¸¦ Liger çš„æ¨¡å‹
model_vanilla = AutoModelForCausalLM.from_pretrained(...)

# å†è®­ç»ƒå¸¦ Liger çš„æ¨¡å‹
apply_liger_kernel_to_llama()
model_liger = AutoModelForCausalLM.from_pretrained(...)

# æ¯”è¾ƒæŸå¤±æ›²çº¿
```

## æœ€ä½³å®è·µ

### âœ… æ¨è

1. **ä½¿ç”¨ AutoModel**ï¼šæœ€ç®€å•ï¼Œè‡ªåŠ¨ä¼˜åŒ–
2. **è®­ç»ƒæ—¶ç¦ç”¨ KV cache**ï¼š`use_cache=False`
3. **ä½¿ç”¨ bfloat16**ï¼šåœ¨ Ampere+ GPU ä¸Š
4. **å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹**ï¼šèŠ‚çœæ›´å¤šå†…å­˜
5. **ä½¿ç”¨ FusedLinearCrossEntropy**ï¼šé»˜è®¤å¯ç”¨ï¼ŒèŠ‚çœ 80% å†…å­˜

```python
model = AutoLigerKernelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    torch_dtype=torch.bfloat16,  # âœ… ä½¿ç”¨ bfloat16
    use_cache=False,              # âœ… ç¦ç”¨ cache
)

training_args = TrainingArguments(
    ...,
    gradient_checkpointing=True,  # âœ… æ¢¯åº¦æ£€æŸ¥ç‚¹
    bf16=True,                    # âœ… bfloat16 è®­ç»ƒ
)
```

### âŒ é¿å…

1. **æ¨ç†æ—¶ç¦ç”¨ cache**ï¼šæ¨ç†éœ€è¦ KV cache
2. **æ··åˆä½¿ç”¨ FusedLinearCE å’Œæ™®é€š CE**ï¼šé€‰ä¸€ä¸ª
3. **åœ¨ Volta GPU ä¸Šç”¨ bfloat16**ï¼šä¸æ”¯æŒ

## æ›´å¤šèµ„æº

- ğŸ“– **å®Œæ•´æ–‡æ¡£**ï¼š[Liger-Kernelé¡¹ç›®æ·±åº¦åˆ†æ.md](./Liger-Kernelé¡¹ç›®æ·±åº¦åˆ†æ.md)
- ğŸŒ **å®˜æ–¹æ–‡æ¡£**ï¼šhttps://linkedin.github.io/Liger-Kernel/
- ğŸ’» **GitHub**ï¼šhttps://github.com/linkedin/Liger-Kernel
- ğŸ“„ **æŠ€æœ¯è®ºæ–‡**ï¼šhttps://arxiv.org/pdf/2410.10989
- ğŸ’¬ **Discord**ï¼šhttps://discord.gg/gpumode
- ğŸ“ **åšå®¢**ï¼šhttps://www.linkedin.com/blog/engineering/open-source/liger-kernel-open-source-ecosystem-for-efficient-llm-training


