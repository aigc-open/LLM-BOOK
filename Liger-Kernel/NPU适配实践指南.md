# NPU é€‚é…å®è·µæŒ‡å— - ä¿æŒå®¢æˆ·ä½¿ç”¨æ–¹å¼ä¸å˜

## åœºæ™¯è¯´æ˜

**é—®é¢˜**ï¼šNPU å·²ç»æ”¯æŒ Tritonï¼Œä½†æŸäº›ç®—å­åœ¨ NPU ä¸Šå¯èƒ½éœ€è¦ä¸åŒçš„å®ç°ï¼ˆæ€§èƒ½ä¼˜åŒ–ã€å†…å­˜å¸ƒå±€å·®å¼‚ç­‰ï¼‰ï¼ŒåŒæ—¶å¸Œæœ›ä¿æŒå®¢æˆ·ä½¿ç”¨æ–¹å¼å®Œå…¨ä¸€è‡´ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨æ¡ä»¶ Monkey Patchï¼Œæ ¹æ®è®¾å¤‡ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å®ç°ã€‚

---

## æ ¸å¿ƒæ€è·¯

```python
# å®¢æˆ·ä»£ç ä¿æŒä¸å˜
from liger_kernel.transformers import apply_liger_kernel_to_llama

apply_liger_kernel_to_llama()  # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡å¹¶åº”ç”¨æœ€ä¼˜å®ç°
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
```

**å†…éƒ¨å®ç°**ï¼š
1. æ£€æµ‹å½“å‰è®¾å¤‡ç±»å‹ï¼ˆCUDA / NPU / AMDï¼‰
2. ä¸ºä¸åŒè®¾å¤‡æä¾›ç‰¹åŒ–å®ç°
3. é€šè¿‡ Monkey Patch ç»Ÿä¸€æ¥å£

---

## å®è·µæ­¥éª¤

### æ­¥éª¤ 1ï¼šåˆ›å»ºè®¾å¤‡æ£€æµ‹å·¥å…·

```python
# src/liger_kernel/utils.py æˆ–åˆ›å»ºæ–°æ–‡ä»¶

import torch

def infer_device():
    """
    æ¨æ–­å½“å‰è®¾å¤‡ç±»å‹
    """
    if torch.cuda.is_available():
        # æ£€æŸ¥æ˜¯å¦æ˜¯ NPUï¼ˆå‡è®¾ NPU æœ‰ç‰¹å®šå±æ€§ï¼‰
        try:
            device_name = torch.cuda.get_device_name(0)
            if "NPU" in device_name or "Ascend" in device_name:
                return "npu"
            elif "AMD" in device_name:
                return "amd"
            else:
                return "cuda"
        except:
            return "cuda"
    return "cpu"


def is_npu_available():
    """æ£€æŸ¥æ˜¯å¦æœ‰ NPU å¯ç”¨"""
    return infer_device() == "npu"
```

### æ­¥éª¤ 2ï¼šä¸º NPU åˆ›å»ºç‰¹åŒ–ç®—å­

å‡è®¾æ‚¨éœ€è¦ä¸º NPU ä¼˜åŒ– RMSNorm ç®—å­ï¼š

```python
# src/liger_kernel/ops/rms_norm_npu.py

import torch
import triton
import triton.language as tl

# NPU ç‰¹åŒ–çš„ Triton å†…æ ¸
@triton.jit
def _rms_norm_forward_kernel_npu(
    Y_ptr,
    X_ptr,
    W_ptr,
    RSTD_ptr,
    n_cols,
    eps,
    BLOCK_SIZE: tl.constexpr,
):
    """
    é’ˆå¯¹ NPU ä¼˜åŒ–çš„ RMSNorm å‰å‘å†…æ ¸
    
    ä¼˜åŒ–ç‚¹ï¼š
    1. è°ƒæ•´ BLOCK_SIZE é€‚é… NPU å†…å­˜å±‚æ¬¡
    2. ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
    3. ä½¿ç”¨ NPU ç‰¹å®šæŒ‡ä»¤
    """
    row_idx = tl.program_id(0).to(tl.int64)
    col_offsets = tl.arange(0, BLOCK_SIZE)
    mask = col_offsets < n_cols
    
    # åŠ è½½æ•°æ® - NPU ä¼˜åŒ–ï¼šä½¿ç”¨æ›´å¤§çš„å‘é‡åŒ–å®½åº¦
    X_ptr += row_idx * n_cols
    Y_ptr += row_idx * n_cols
    
    X_row = tl.load(X_ptr + col_offsets, mask=mask, other=0.0)
    W_row = tl.load(W_ptr + col_offsets, mask=mask, other=0.0)
    
    # è®¡ç®— - NPU ä¼˜åŒ–ï¼šå¯èƒ½ä½¿ç”¨ä¸åŒçš„ç²¾åº¦ç­–ç•¥
    X_row = X_row.to(tl.float32)  # NPU å¯èƒ½åœ¨ FP32 è®¡ç®—æ›´å¿«
    mean_square = tl.sum(X_row * X_row, axis=0) / n_cols
    rstd = 1.0 / tl.sqrt(mean_square + eps)
    
    # å­˜å‚¨ rstd
    tl.store(RSTD_ptr + row_idx, rstd)
    
    # å½’ä¸€åŒ–
    Y_row = X_row * rstd * W_row
    
    # å­˜å‚¨ç»“æœ - NPU ä¼˜åŒ–ï¼šä½¿ç”¨è¿ç»­å†…å­˜å†™å…¥
    tl.store(Y_ptr + col_offsets, Y_row, mask=mask)


@triton.jit
def _rms_norm_backward_kernel_npu(
    dY_ptr,
    X_ptr,
    W_ptr,
    RSTD_ptr,
    dX_ptr,
    dW_ptr,
    n_cols,
    BLOCK_SIZE: tl.constexpr,
):
    """NPU ä¼˜åŒ–çš„åå‘ä¼ æ’­å†…æ ¸"""
    # NPU ç‰¹åŒ–çš„åå‘ä¼ æ’­å®ç°
    # ç±»ä¼¼çš„ä¼˜åŒ–ç­–ç•¥
    pass


class LigerRMSNormFunctionNPU(torch.autograd.Function):
    """NPU ä¼˜åŒ–çš„ RMSNorm Autograd Function"""
    
    @staticmethod
    def forward(ctx, X, W, eps, offset, casting_mode):
        # ä½¿ç”¨ NPU ä¼˜åŒ–çš„å†…æ ¸
        n_rows, n_cols = X.shape
        Y = torch.empty_like(X)
        RSTD = torch.empty(n_rows, dtype=torch.float32, device=X.device)
        
        # NPU ç‰¹å®šçš„ BLOCK_SIZE é€‰æ‹©
        BLOCK_SIZE = calculate_npu_block_size(n_cols)
        
        # å¯åŠ¨ NPU ä¼˜åŒ–çš„å†…æ ¸
        grid = (n_rows,)
        _rms_norm_forward_kernel_npu[grid](
            Y, X, W, RSTD,
            n_cols, eps, BLOCK_SIZE
        )
        
        ctx.save_for_backward(X, W, RSTD)
        ctx.n_cols = n_cols
        ctx.BLOCK_SIZE = BLOCK_SIZE
        return Y
    
    @staticmethod
    def backward(ctx, dY):
        X, W, RSTD = ctx.saved_tensors
        dX = torch.empty_like(X)
        dW = torch.empty_like(W)
        
        # NPU ä¼˜åŒ–çš„åå‘ä¼ æ’­
        _rms_norm_backward_kernel_npu[(ctx.n_rows,)](
            dY, X, W, RSTD, dX, dW,
            ctx.n_cols, ctx.BLOCK_SIZE
        )
        
        return dX, dW, None, None, None


def calculate_npu_block_size(n_cols):
    """
    æ ¹æ® NPU ç‰¹æ€§è®¡ç®—æœ€ä¼˜ BLOCK_SIZE
    
    NPU å¯èƒ½æœ‰ä¸åŒçš„ï¼š
    - å‘é‡å¯„å­˜å™¨å®½åº¦
    - L1 cache å¤§å°
    - å†…å­˜å¸¦å®½ç‰¹æ€§
    """
    # ç¤ºä¾‹ï¼šNPU åœ¨ 4096 å—å¤§å°æ—¶æ€§èƒ½æœ€ä½³
    if n_cols <= 2048:
        return 2048
    elif n_cols <= 4096:
        return 4096
    else:
        return 8192
```

### æ­¥éª¤ 3ï¼šåˆ›å»ºè®¾å¤‡æ„ŸçŸ¥çš„ Module

```python
# src/liger_kernel/transformers/rms_norm.py

import torch.nn as nn
from liger_kernel.utils import is_npu_available
from liger_kernel.ops.rms_norm import LigerRMSNormFunction  # CUDA ç‰ˆæœ¬
from liger_kernel.ops.rms_norm_npu import LigerRMSNormFunctionNPU  # NPU ç‰ˆæœ¬


class LigerRMSNorm(nn.Module):
    """
    è®¾å¤‡æ„ŸçŸ¥çš„ RMSNorm æ¨¡å—
    è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å®ç°ï¼Œå¯¹ç”¨æˆ·é€æ˜
    """
    
    def __init__(self, hidden_size, eps=1e-6, offset=0.0, 
                 casting_mode="llama", init_fn="ones"):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps
        self.offset = offset
        self.casting_mode = casting_mode
        
        # æ ¹æ®è®¾å¤‡é€‰æ‹©å®ç°
        self._select_implementation()
    
    def _select_implementation(self):
        """æ ¹æ®å½“å‰è®¾å¤‡é€‰æ‹©æœ€ä¼˜å®ç°"""
        if is_npu_available():
            self._forward_impl = LigerRMSNormFunctionNPU.apply
            self._device_type = "npu"
        else:
            self._forward_impl = LigerRMSNormFunction.apply
            self._device_type = "cuda"
    
    def forward(self, hidden_states):
        """
        å‰å‘ä¼ æ’­ - è‡ªåŠ¨ä½¿ç”¨æœ€ä¼˜å®ç°
        ç”¨æˆ·æ— éœ€å…³å¿ƒåº•å±‚è®¾å¤‡
        """
        return self._forward_impl(
            hidden_states,
            self.weight,
            self.variance_epsilon,
            self.offset,
            self.casting_mode
        )
    
    def extra_repr(self):
        """æ˜¾ç¤ºå½“å‰ä½¿ç”¨çš„è®¾å¤‡å®ç°"""
        return f"device={self._device_type}, hidden_size={self.weight.shape[0]}, eps={self.variance_epsilon}"
```

### æ­¥éª¤ 4ï¼šæ›´æ–° Monkey Patch å‡½æ•°ï¼ˆä¿æŒæ¥å£ä¸å˜ï¼‰

```python
# src/liger_kernel/transformers/monkey_patch.py

from liger_kernel.transformers.rms_norm import LigerRMSNorm
from liger_kernel.utils import infer_device


def apply_liger_kernel_to_llama(
    rope: bool = True,
    cross_entropy: bool = False,
    fused_linear_cross_entropy: bool = True,
    rms_norm: bool = True,
    swiglu: bool = True,
    model: PreTrainedModel = None,
) -> None:
    """
    ä¸º LLaMA åº”ç”¨ Liger å†…æ ¸
    
    è‡ªåŠ¨æ£€æµ‹è®¾å¤‡å¹¶åº”ç”¨æœ€ä¼˜å®ç°ï¼š
    - NVIDIA GPU: ä½¿ç”¨ CUDA ä¼˜åŒ–
    - NPU: ä½¿ç”¨ NPU ç‰¹åŒ–ç‰ˆæœ¬
    - AMD GPU: ä½¿ç”¨ ROCm ä¼˜åŒ–
    
    å®¢æˆ·ä½¿ç”¨æ–¹å¼å®Œå…¨ä¸€è‡´ï¼
    """
    from transformers.models.llama import modeling_llama
    
    # æ£€æµ‹è®¾å¤‡ï¼ˆå¯é€‰ï¼šæ‰“å°æ—¥å¿—ï¼‰
    device_type = infer_device()
    logger.info(f"Detected device: {device_type}, applying optimized kernels")
    
    # æ›¿æ¢ RMSNorm - è‡ªåŠ¨ä½¿ç”¨è®¾å¤‡æ„ŸçŸ¥ç‰ˆæœ¬
    if rms_norm:
        # LigerRMSNorm å†…éƒ¨ä¼šè‡ªåŠ¨é€‰æ‹© NPU/CUDA å®ç°
        modeling_llama.LlamaRMSNorm = LigerRMSNorm
    
    # å…¶ä»–ç®—å­çš„æ›¿æ¢
    if rope:
        modeling_llama.apply_rotary_pos_emb = liger_rotary_pos_emb
    
    if swiglu:
        # SwiGLU ä¹Ÿå¯ä»¥åšç±»ä¼¼çš„è®¾å¤‡æ„ŸçŸ¥å®ç°
        modeling_llama.LlamaMLP = LigerSwiGLUMLP
    
    if cross_entropy:
        from transformers.loss.loss_utils import nn
        nn.functional.cross_entropy = liger_cross_entropy
    
    if fused_linear_cross_entropy:
        if model is not None:
            model.forward = MethodType(llama_lce_forward, model)
        else:
            modeling_llama.LlamaForCausalLM.forward = llama_lce_forward
    
    # å®ä¾‹çº§ patchï¼ˆå¦‚æœéœ€è¦ï¼‰
    if model is not None:
        base_model = getattr(model, model.base_model_prefix, model)
        
        if rms_norm:
            _patch_rms_norm_module(base_model.norm)
        
        for decoder_layer in base_model.layers:
            if swiglu:
                _patch_swiglu_module(decoder_layer.mlp, LigerSwiGLUMLP)
            if rms_norm:
                _patch_rms_norm_module(decoder_layer.input_layernorm)
                _patch_rms_norm_module(decoder_layer.post_attention_layernorm)
```

---

## å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

### å®¢æˆ·ç«¯ä»£ç ï¼ˆå®Œå…¨ä¸å˜ï¼‰

```python
# train.py - å®¢æˆ·ä»£ç æ— éœ€ä»»ä½•ä¿®æ”¹

from liger_kernel.transformers import apply_liger_kernel_to_llama
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# 1. åº”ç”¨ Liger ä¼˜åŒ–ï¼ˆè‡ªåŠ¨æ£€æµ‹è®¾å¤‡ï¼‰
apply_liger_kernel_to_llama()

# 2. åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    torch_dtype=torch.bfloat16,
    device_map="auto",  # è‡ªåŠ¨åˆ†é…åˆ° NPU æˆ– GPU
)

# 3. æ­£å¸¸è®­ç»ƒ
trainer = Trainer(model=model, ...)
trainer.train()
```

**è¿è¡Œç»“æœ**ï¼š
- ğŸ–¥ï¸ åœ¨ NVIDIA GPU ä¸Šï¼šè‡ªåŠ¨ä½¿ç”¨ CUDA ä¼˜åŒ–å†…æ ¸
- ğŸ”§ åœ¨ NPU ä¸Šï¼šè‡ªåŠ¨ä½¿ç”¨ NPU ç‰¹åŒ–å†…æ ¸
- ğŸ’» å®¢æˆ·ä»£ç å®Œå…¨ç›¸åŒï¼Œæ— éœ€ä»»ä½•ä¿®æ”¹ï¼

---

## é«˜çº§ç”¨æ³•ï¼šéƒ¨åˆ†ç®—å­ä½¿ç”¨ NPU ä¼˜åŒ–

### åœºæ™¯ï¼šåªä¼˜åŒ–éƒ¨åˆ†ç®—å­

å‡è®¾æ‚¨åªæƒ³ä¸º NPU ä¼˜åŒ– RMSNorm å’Œ CrossEntropyï¼Œå…¶ä»–ç®—å­ä½¿ç”¨é€šç”¨å®ç°ï¼š

```python
# src/liger_kernel/transformers/monkey_patch.py

def apply_liger_kernel_to_llama(
    rope: bool = True,
    cross_entropy: bool = False,
    fused_linear_cross_entropy: bool = True,
    rms_norm: bool = True,
    swiglu: bool = True,
    model: PreTrainedModel = None,
    force_device: str = None,  # æ–°å¢ï¼šå¼ºåˆ¶æŒ‡å®šè®¾å¤‡ç±»å‹
) -> None:
    """æ”¯æŒå¼ºåˆ¶æŒ‡å®šè®¾å¤‡ç±»å‹ç”¨äºæµ‹è¯•"""
    from transformers.models.llama import modeling_llama
    
    device_type = force_device or infer_device()
    
    # æ ¹æ®è®¾å¤‡å’Œç®—å­ç±»å‹é€‰æ‹©å®ç°
    if rms_norm:
        if device_type == "npu":
            # NPU ä½¿ç”¨ç‰¹åŒ–ç‰ˆæœ¬
            modeling_llama.LlamaRMSNorm = LigerRMSNormNPU
        else:
            # å…¶ä»–è®¾å¤‡ä½¿ç”¨é€šç”¨ç‰ˆæœ¬
            modeling_llama.LlamaRMSNorm = LigerRMSNorm
    
    if cross_entropy:
        if device_type == "npu":
            # NPU ä½¿ç”¨ç‰¹åŒ–çš„ CrossEntropy
            from liger_kernel.transformers.functional_npu import liger_cross_entropy_npu
            from transformers.loss.loss_utils import nn
            nn.functional.cross_entropy = liger_cross_entropy_npu
        else:
            # é€šç”¨ç‰ˆæœ¬
            from transformers.loss.loss_utils import nn
            nn.functional.cross_entropy = liger_cross_entropy
    
    # SwiGLU å¯èƒ½åœ¨ NPU ä¸Šä¸éœ€è¦ç‰¹åŒ–ï¼Œä½¿ç”¨é€šç”¨ç‰ˆæœ¬
    if swiglu:
        modeling_llama.LlamaMLP = LigerSwiGLUMLP  # æ‰€æœ‰è®¾å¤‡é€šç”¨
    
    # ... å…¶ä»–ç®—å­
```

---

## è°ƒè¯•å’Œæ€§èƒ½åˆ†æ

### 1. éªŒè¯æ­£ç¡®æ€§

```python
# test/test_npu_correctness.py

import pytest
import torch
from liger_kernel.transformers.rms_norm import LigerRMSNorm
from liger_kernel.utils import is_npu_available


@pytest.mark.skipif(not is_npu_available(), reason="NPU not available")
def test_rms_norm_npu_correctness():
    """éªŒè¯ NPU å®ç°çš„æ­£ç¡®æ€§"""
    hidden_size = 4096
    batch_size = 2
    seq_len = 128
    
    # åˆ›å»ºè¾“å…¥
    x = torch.randn(batch_size, seq_len, hidden_size, device="npu", dtype=torch.bfloat16)
    x.requires_grad = True
    
    # å‚è€ƒå®ç°ï¼ˆCPUï¼‰
    x_cpu = x.detach().cpu().float()
    x_cpu.requires_grad = True
    ref_norm = torch.nn.RMSNorm(hidden_size).cpu()
    ref_output = ref_norm(x_cpu)
    ref_output.sum().backward()
    ref_grad = x_cpu.grad
    
    # NPU å®ç°
    x.grad = None
    npu_norm = LigerRMSNorm(hidden_size).to("npu")
    npu_norm.weight.data.copy_(ref_norm.weight.data)
    npu_output = npu_norm(x)
    npu_output.sum().backward()
    npu_grad = x.grad
    
    # éªŒè¯ï¼ˆå…è®¸ä¸€å®šè¯¯å·®ï¼‰
    assert torch.allclose(
        npu_output.cpu().float(), 
        ref_output, 
        atol=1e-2, rtol=1e-2
    ), "NPU forward output mismatch"
    
    assert torch.allclose(
        npu_grad.cpu().float(),
        ref_grad,
        atol=1e-2, rtol=1e-2
    ), "NPU backward gradient mismatch"


def test_cross_device_compatibility():
    """éªŒè¯åœ¨ä¸åŒè®¾å¤‡ä¸Šçš„å…¼å®¹æ€§"""
    from liger_kernel.transformers import apply_liger_kernel_to_llama
    
    # åº”ç”¨ patch
    apply_liger_kernel_to_llama()
    
    # åŠ è½½æ¨¡å‹åˆ°ä¸åŒè®¾å¤‡åº”è¯¥éƒ½èƒ½å·¥ä½œ
    devices = []
    if torch.cuda.is_available():
        devices.append("cuda")
    if is_npu_available():
        devices.append("npu")
    
    for device in devices:
        model = AutoModelForCausalLM.from_pretrained(
            "meta-llama/Llama-2-7b-hf",
            device_map=device,
        )
        
        # ç®€å•å‰å‘ä¼ æ’­
        input_ids = torch.randint(0, 1000, (2, 10), device=device)
        output = model(input_ids)
        
        assert output.logits is not None
        print(f"âœ“ Model works on {device}")
```

### 2. æ€§èƒ½å¯¹æ¯”

```python
# benchmark/benchmark_npu_vs_cuda.py

import torch
import time
from liger_kernel.transformers.rms_norm import LigerRMSNorm
from liger_kernel.utils import is_npu_available


def benchmark_device(device, hidden_size=4096, seq_len=2048, num_runs=100):
    """åœ¨æŒ‡å®šè®¾å¤‡ä¸Šè¿›è¡Œæ€§èƒ½æµ‹è¯•"""
    x = torch.randn(8, seq_len, hidden_size, device=device, dtype=torch.bfloat16)
    norm = LigerRMSNorm(hidden_size).to(device)
    
    # é¢„çƒ­
    for _ in range(10):
        _ = norm(x)
    
    # åŒæ­¥
    if device == "cuda":
        torch.cuda.synchronize()
    elif device == "npu":
        torch.npu.synchronize()  # å‡è®¾ NPU æœ‰ç±»ä¼¼ API
    
    # æµ‹é‡
    start = time.time()
    for _ in range(num_runs):
        _ = norm(x)
    
    if device == "cuda":
        torch.cuda.synchronize()
    elif device == "npu":
        torch.npu.synchronize()
    
    elapsed = time.time() - start
    throughput = (num_runs * 8 * seq_len) / elapsed  # tokens/s
    
    return throughput


if __name__ == "__main__":
    results = {}
    
    if torch.cuda.is_available():
        results["CUDA"] = benchmark_device("cuda")
    
    if is_npu_available():
        results["NPU"] = benchmark_device("npu")
    
    # æ‰“å°ç»“æœ
    print("\n=== Performance Comparison ===")
    for device, throughput in results.items():
        print(f"{device}: {throughput:.2f} tokens/s")
    
    if len(results) == 2:
        speedup = results["NPU"] / results["CUDA"]
        print(f"\nNPU vs CUDA speedup: {speedup:.2f}x")
```

---

## é…ç½®å’Œç¯å¢ƒå˜é‡

### æ”¯æŒç¯å¢ƒå˜é‡æ§åˆ¶

```python
# src/liger_kernel/utils.py

import os

def get_preferred_device():
    """
    æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡å¼ºåˆ¶æŒ‡å®šè®¾å¤‡
    
    ç”¨æ³•ï¼š
        export LIGER_FORCE_DEVICE=npu
        export LIGER_FORCE_DEVICE=cuda
    """
    forced = os.environ.get("LIGER_FORCE_DEVICE", "").lower()
    if forced in ["npu", "cuda", "amd", "cpu"]:
        return forced
    return infer_device()


def is_npu_optimized_enabled():
    """
    æ”¯æŒç¦ç”¨ NPU ä¼˜åŒ–ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    
    ç”¨æ³•ï¼š
        export LIGER_DISABLE_NPU_OPT=1
    """
    return os.environ.get("LIGER_DISABLE_NPU_OPT", "0") != "1"
```

### å®¢æˆ·ä½¿ç”¨

```bash
# å¼ºåˆ¶ä½¿ç”¨ NPU ä¼˜åŒ–
export LIGER_FORCE_DEVICE=npu
python train.py

# ç¦ç”¨ NPU ä¼˜åŒ–ï¼ˆä½¿ç”¨é€šç”¨ç‰ˆæœ¬ï¼‰
export LIGER_DISABLE_NPU_OPT=1
python train.py

# è°ƒè¯•æ¨¡å¼ï¼šæ‰“å°è®¾å¤‡é€‰æ‹©ä¿¡æ¯
export LIGER_DEBUG=1
python train.py
```

---

## æœ€ä½³å®è·µæ€»ç»“

### âœ… æ¨èåšæ³•

1. **è®¾å¤‡æ£€æµ‹è‡ªåŠ¨åŒ–**
   - åœ¨æ¨¡å—åˆå§‹åŒ–æ—¶è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
   - ç”¨æˆ·æ— éœ€å…³å¿ƒåº•å±‚å®ç°

2. **ä¿æŒæ¥å£ä¸€è‡´**
   - æ‰€æœ‰è®¾å¤‡ä½¿ç”¨ç›¸åŒçš„ API
   - å®¢æˆ·ä»£ç é›¶ä¿®æ”¹

3. **æ€§èƒ½ä¼˜å…ˆçº§**
   ```python
   # ä¼˜å…ˆçº§ï¼šNPU ç‰¹åŒ– > CUDA ä¼˜åŒ– > é€šç”¨å®ç°
   if is_npu_available():
       use_npu_kernel()
   elif is_cuda_available():
       use_cuda_kernel()
   else:
       use_generic_kernel()
   ```

4. **å……åˆ†æµ‹è¯•**
   - æ­£ç¡®æ€§æµ‹è¯•ï¼šç¡®ä¿æ•°å€¼ä¸€è‡´
   - æ€§èƒ½æµ‹è¯•ï¼šéªŒè¯åŠ é€Ÿæ¯”
   - å…¼å®¹æ€§æµ‹è¯•ï¼šè·¨è®¾å¤‡æµ‹è¯•

5. **æä¾›é™çº§æ–¹æ¡ˆ**
   - å¦‚æœ NPU ç‰¹åŒ–å¤±è´¥ï¼Œå›é€€åˆ°é€šç”¨å®ç°
   - æ‰“å°è­¦å‘Šä¿¡æ¯

### âŒ é¿å…çš„åšæ³•

1. **ç¡¬ç¼–ç è®¾å¤‡ç±»å‹**
   ```python
   # âŒ ä¸å¥½
   if device == "npu":
       ...
   
   # âœ… å¥½
   if is_npu_available():
       ...
   ```

2. **ä¿®æ”¹å®¢æˆ· API**
   ```python
   # âŒ ä¸å¥½ - è¦æ±‚å®¢æˆ·ä¿®æ”¹ä»£ç 
   apply_liger_kernel_to_llama_npu()
   
   # âœ… å¥½ - è‡ªåŠ¨æ£€æµ‹
   apply_liger_kernel_to_llama()
   ```

3. **ç¼ºå°‘å›é€€æœºåˆ¶**
   ```python
   # âŒ ä¸å¥½ - NPU å¤±è´¥å°±å´©æºƒ
   return npu_kernel(x)
   
   # âœ… å¥½ - æœ‰å›é€€
   try:
       return npu_kernel(x)
   except Exception as e:
       logger.warning(f"NPU kernel failed: {e}, using fallback")
       return generic_kernel(x)
   ```

---

## ç›®å½•ç»“æ„å»ºè®®

```
src/liger_kernel/
â”œâ”€â”€ ops/
â”‚   â”œâ”€â”€ rms_norm.py              # CUDA å®ç°ï¼ˆé»˜è®¤ï¼‰
â”‚   â”œâ”€â”€ rms_norm_npu.py          # NPU ç‰¹åŒ–å®ç°
â”‚   â”œâ”€â”€ cross_entropy.py         # CUDA å®ç°
â”‚   â”œâ”€â”€ cross_entropy_npu.py     # NPU ç‰¹åŒ–å®ç°
â”‚   â””â”€â”€ ...
â”œâ”€â”€ transformers/
â”‚   â”œâ”€â”€ rms_norm.py              # è®¾å¤‡æ„ŸçŸ¥çš„ Module
â”‚   â”œâ”€â”€ cross_entropy.py         # è®¾å¤‡æ„ŸçŸ¥çš„ Module
â”‚   â””â”€â”€ monkey_patch.py          # ç»Ÿä¸€çš„ patch æ¥å£
â”œâ”€â”€ utils.py                     # è®¾å¤‡æ£€æµ‹å·¥å…·
â””â”€â”€ config.py                    # é…ç½®ç®¡ç†
```

---

## å¸¸è§é—®é¢˜

### Q: NPU çš„ Triton å’Œ CUDA çš„ Triton æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**A**: ä¸»è¦åŒºåˆ«å¯èƒ½åœ¨äºï¼š

1. **å†…å­˜å±‚æ¬¡**ï¼šNPU å¯èƒ½æœ‰ä¸åŒçš„ç¼“å­˜ç»“æ„
2. **å—å¤§å°**ï¼šæœ€ä¼˜çš„ BLOCK_SIZE å¯èƒ½ä¸åŒ
3. **æŒ‡ä»¤é›†**ï¼šæŸäº› Triton æ“ä½œåœ¨ NPU ä¸Šå¯èƒ½æœ‰ç‰¹æ®Šä¼˜åŒ–
4. **æ•°æ®ç±»å‹**ï¼šNPU å¯èƒ½å¯¹æŸäº›ç²¾åº¦æœ‰ç‰¹æ®Šæ”¯æŒ

**è§£å†³æ–¹æ¡ˆ**ï¼šé€šè¿‡ NPU ç‰¹åŒ–å†…æ ¸è°ƒæ•´è¿™äº›å‚æ•°ã€‚

### Q: å¦‚ä½•ç¡®ä¿ NPU ç‰ˆæœ¬å’Œ CUDA ç‰ˆæœ¬æ•°å€¼ä¸€è‡´ï¼Ÿ

**A**: 

1. **ä¸¥æ ¼æµ‹è¯•**ï¼šç¼–å†™æ­£ç¡®æ€§æµ‹è¯•å¯¹æ¯”è¾“å‡º
2. **å…è®¸è¯¯å·®**ï¼šè€ƒè™‘ä¸åŒç¡¬ä»¶çš„æµ®ç‚¹è¯¯å·®
3. **ä½¿ç”¨ç›¸åŒç®—æ³•**ï¼šç¡®ä¿æ•°å­¦é€»è¾‘ä¸€è‡´

```python
# æ­£ç¡®æ€§æµ‹è¯•
def test_cross_device():
    cuda_output = model_cuda(input)
    npu_output = model_npu(input)
    
    # å…è®¸ç¡¬ä»¶å·®å¼‚å¯¼è‡´çš„å°è¯¯å·®
    assert torch.allclose(cuda_output, npu_output, atol=1e-2, rtol=1e-2)
```

### Q: æ€§èƒ½ä¸å¦‚é¢„æœŸæ€ä¹ˆåŠï¼Ÿ

**A**: 

1. **Profiling**ï¼šä½¿ç”¨ NPU çš„æ€§èƒ½åˆ†æå·¥å…·
2. **è°ƒæ•´å‚æ•°**ï¼šBLOCK_SIZEã€num_warps ç­‰
3. **å†…å­˜ä¼˜åŒ–**ï¼šè°ƒæ•´å†…å­˜è®¿é—®æ¨¡å¼
4. **å¯¹æ¯”åŸºå‡†**ï¼šä¸ NPU åŸç”Ÿå®ç°å¯¹æ¯”

```python
# æ€§èƒ½åˆ†æ
with torch.profiler.profile() as prof:
    output = model(input)

print(prof.key_averages().table())
```

---

## å®Œæ•´ç¤ºä¾‹ï¼šç«¯åˆ°ç«¯æµç¨‹

```python
# 1. è®¾å¤‡æ£€æµ‹ï¼ˆè‡ªåŠ¨ï¼‰
from liger_kernel.utils import infer_device
device = infer_device()  # è¿”å› "npu" æˆ– "cuda"

# 2. åº”ç”¨ä¼˜åŒ–ï¼ˆå®¢æˆ·ä»£ç ä¸å˜ï¼‰
from liger_kernel.transformers import apply_liger_kernel_to_llama
apply_liger_kernel_to_llama()  # è‡ªåŠ¨é€‰æ‹© NPU/CUDA å®ç°

# 3. åŠ è½½æ¨¡å‹ï¼ˆå®¢æˆ·ä»£ç ä¸å˜ï¼‰
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    device_map="auto",
)

# 4. è®­ç»ƒï¼ˆå®¢æˆ·ä»£ç ä¸å˜ï¼‰
trainer = Trainer(model=model, ...)
trainer.train()

# ç»“æœï¼š
# - NPU ä¸Šè‡ªåŠ¨ä½¿ç”¨ NPU ä¼˜åŒ–å†…æ ¸
# - CUDA ä¸Šè‡ªåŠ¨ä½¿ç”¨ CUDA ä¼˜åŒ–å†…æ ¸
# - å®¢æˆ·ä»£ç å®Œå…¨ä¸€è‡´ï¼
```

---

## æ€»ç»“

### æ ¸å¿ƒåŸåˆ™

1. **é€æ˜æ€§**ï¼šè®¾å¤‡é€‰æ‹©å¯¹ç”¨æˆ·é€æ˜
2. **ä¸€è‡´æ€§**ï¼šAPI æ¥å£ä¿æŒå®Œå…¨ä¸€è‡´
3. **æ€§èƒ½**ï¼šè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å®ç°
4. **å…¼å®¹æ€§**ï¼šæ”¯æŒå¤šç§è®¾å¤‡
5. **å¯æµ‹è¯•**ï¼šå……åˆ†çš„æµ‹è¯•ä¿è¯æ­£ç¡®æ€§

### å…³é”®ä¼˜åŠ¿

âœ… **å®¢æˆ·ä»£ç é›¶ä¿®æ”¹**  
âœ… **è‡ªåŠ¨è®¾å¤‡æ£€æµ‹**  
âœ… **æ€§èƒ½è‡ªåŠ¨ä¼˜åŒ–**  
âœ… **æ˜“äºç»´æŠ¤å’Œæ‰©å±•**  
âœ… **å®Œå…¨å‘åå…¼å®¹**

### å®ç°è¦ç‚¹

1. è®¾å¤‡æ£€æµ‹è‡ªåŠ¨åŒ–ï¼ˆ`infer_device()`ï¼‰
2. è®¾å¤‡æ„ŸçŸ¥çš„ Moduleï¼ˆå†…éƒ¨é€‰æ‹©å®ç°ï¼‰
3. ç»Ÿä¸€çš„ Monkey Patch æ¥å£
4. å……åˆ†çš„æµ‹è¯•å’Œæ€§èƒ½åˆ†æ

---

**é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ‚¨å¯ä»¥ä¸º NPU æä¾›ç‰¹åŒ–ä¼˜åŒ–ï¼ŒåŒæ—¶ä¿æŒå®¢æˆ·ä½¿ç”¨æ–¹å¼å®Œå…¨ä¸å˜ï¼** ğŸ‰

