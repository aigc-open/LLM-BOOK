# FlashInfer 二次开发指南

本指南将帮助你了解如何在 FlashInfer 的基础上进行二次开发，包括添加新功能、优化现有内核、集成到你的项目等。

## 开发环境设置

### 1. 基础要求

**硬件要求**：
- NVIDIA GPU (SM75+，如 RTX 2080、A100、H100 等)
- 至少 16GB 内存
- 推荐 32GB+ 内存用于大规模编译

**软件要求**：
- Linux 系统（推荐 Ubuntu 20.04+）
- CUDA Toolkit 12.0+ (推荐 12.3+)
- Python 3.10+
- GCC 9+ 或 Clang 10+
- Git

### 2. 克隆仓库

```bash
# 克隆仓库（包含子模块）
git clone https://github.com/flashinfer-ai/flashinfer.git --recursive
cd flashinfer

# 如果忘记 --recursive，可以后续执行
git submodule update --init --recursive
```

### 3. 安装依赖

```bash
# 创建虚拟环境（推荐）
python -m venv venv
source venv/bin/activate

# 安装 PyTorch（根据你的 CUDA 版本）
pip install torch --index-url https://download.pytorch.org/whl/cu121

# 安装其他依赖
pip install -r requirements.txt
```

### 4. 可编辑安装

```bash
# 开发模式安装（推荐）
pip install --no-build-isolation -e . -v

# 说明：
# --no-build-isolation: 使用当前环境的依赖
# -e: 可编辑模式，修改源码后无需重新安装
# -v: 显示详细的构建信息
```

### 5. 验证安装

```bash
# 运行测试
python -c "import flashinfer; print(flashinfer.__version__)"

# 查看配置
flashinfer show-config

# 运行单元测试
pytest tests/test_artifacts.py -v
```

## 代码结构导航

### 添加新功能的典型流程

```
1. 定义内核 (include/flashinfer/)
   ↓
2. 实现绑定 (csrc/)
   ↓
3. 封装 Python API (flashinfer/)
   ↓
4. 编写测试 (tests/)
   ↓
5. 更新文档 (docs/)
   ↓
6. (可选) 添加 Benchmark (benchmarks/)
```

## 实战案例：添加新算子

### 案例 1：添加一个简单的激活函数

#### 步骤 1：定义 CUDA 内核

创建 `include/flashinfer/my_activation.cuh`:

```cuda
#ifndef FLASHINFER_MY_ACTIVATION_CUH_
#define FLASHINFER_MY_ACTIVATION_CUH_

#include <cuda_runtime.h>
#include "flashinfer/utils.cuh"

namespace flashinfer {

// SwiGLU 激活函数
template <typename T>
__device__ __forceinline__ T swiglu(T x, T gate) {
  // SwiGLU(x, gate) = x * sigmoid(gate)
  T sigmoid_gate = T(1.0) / (T(1.0) + expf(-float(gate)));
  return x * sigmoid_gate;
}

// Kernel 实现
template <typename T>
__global__ void SwiGLUKernel(
    const T* __restrict__ x,
    const T* __restrict__ gate,
    T* __restrict__ output,
    int n) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (idx < n) {
    output[idx] = swiglu(x[idx], gate[idx]);
  }
}

// Host 函数
template <typename T>
cudaError_t ApplySwiGLU(
    const T* x,
    const T* gate,
    T* output,
    int n,
    cudaStream_t stream = nullptr) {
  const int threads = 256;
  const int blocks = (n + threads - 1) / threads;
  
  SwiGLUKernel<T><<<blocks, threads, 0, stream>>>(x, gate, output, n);
  
  return cudaGetLastError();
}

}  // namespace flashinfer

#endif  // FLASHINFER_MY_ACTIVATION_CUH_
```

#### 步骤 2：创建 PyTorch 绑定

创建 `csrc/my_activation.cu`:

```cuda
#include <torch/extension.h>
#include <ATen/cuda/CUDAContext.h>
#include "flashinfer/my_activation.cuh"

torch::Tensor swiglu_forward(
    torch::Tensor x,
    torch::Tensor gate) {
  TORCH_CHECK(x.is_cuda(), "x must be a CUDA tensor");
  TORCH_CHECK(gate.is_cuda(), "gate must be a CUDA tensor");
  TORCH_CHECK(x.sizes() == gate.sizes(), "x and gate must have same shape");
  
  auto output = torch::empty_like(x);
  int n = x.numel();
  
  AT_DISPATCH_FLOATING_TYPES_AND_HALF(
    x.scalar_type(), "swiglu_forward", [&] {
      flashinfer::ApplySwiGLU<scalar_t>(
        x.data_ptr<scalar_t>(),
        gate.data_ptr<scalar_t>(),
        output.data_ptr<scalar_t>(),
        n,
        at::cuda::getCurrentCUDAStream()
      );
    });
  
  return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
  m.def("swiglu_forward", &swiglu_forward, "SwiGLU activation forward");
}
```

#### 步骤 3：添加 Python API

在 `flashinfer/activation.py` 中添加:

```python
import torch
from typing import Optional

# 延迟导入 C++ 扩展
_C = None

def _import_c_extension():
    global _C
    if _C is None:
        try:
            from . import _kernels  # 假设编译后的模块名
            _C = _kernels
        except ImportError as e:
            raise ImportError(f"Failed to import FlashInfer C++ extension: {e}")
    return _C

def swiglu(x: torch.Tensor, gate: torch.Tensor) -> torch.Tensor:
    """
    SwiGLU activation function.
    
    Args:
        x: Input tensor of shape [..., D]
        gate: Gate tensor of same shape as x
        
    Returns:
        Output tensor of same shape as x
        
    Example:
        >>> x = torch.randn(32, 128, device='cuda')
        >>> gate = torch.randn(32, 128, device='cuda')
        >>> output = swiglu(x, gate)
    """
    _import_c_extension()
    return _C.swiglu_forward(x, gate)
```

#### 步骤 4：更新构建配置

修改 `build_backend.py` 或创建相应的构建配置，将 `csrc/my_activation.cu` 添加到编译列表。

#### 步骤 5：编写测试

创建 `tests/test_my_activation.py`:

```python
import torch
import pytest
import flashinfer

def test_swiglu_basic():
    """基础功能测试"""
    x = torch.randn(32, 128, dtype=torch.float16, device='cuda')
    gate = torch.randn(32, 128, dtype=torch.float16, device='cuda')
    
    output = flashinfer.swiglu(x, gate)
    
    assert output.shape == x.shape
    assert output.dtype == x.dtype
    assert output.device == x.device

def test_swiglu_correctness():
    """正确性测试"""
    x = torch.randn(32, 128, dtype=torch.float32, device='cuda')
    gate = torch.randn(32, 128, dtype=torch.float32, device='cuda')
    
    # FlashInfer 实现
    output_flashinfer = flashinfer.swiglu(x, gate)
    
    # PyTorch 参考实现
    sigmoid_gate = torch.sigmoid(gate)
    output_reference = x * sigmoid_gate
    
    # 比较结果
    torch.testing.assert_close(
        output_flashinfer, 
        output_reference, 
        rtol=1e-3, 
        atol=1e-5
    )

def test_swiglu_fp16():
    """FP16 测试"""
    x = torch.randn(1024, 4096, dtype=torch.float16, device='cuda')
    gate = torch.randn(1024, 4096, dtype=torch.float16, device='cuda')
    
    output = flashinfer.swiglu(x, gate)
    assert not torch.isnan(output).any()
    assert not torch.isinf(output).any()

@pytest.mark.parametrize("shape", [
    (32, 128),
    (1, 1),
    (1024, 4096),
    (7, 13, 17),  # 非2的幂次
])
def test_swiglu_shapes(shape):
    """多种形状测试"""
    x = torch.randn(*shape, dtype=torch.float16, device='cuda')
    gate = torch.randn(*shape, dtype=torch.float16, device='cuda')
    
    output = flashinfer.swiglu(x, gate)
    assert output.shape == x.shape
```

运行测试：

```bash
pytest tests/test_my_activation.py -v
```

### 案例 2：优化现有内核

假设你想优化 BatchDecode 内核以支持更大的 batch size。

#### 步骤 1：找到相关代码

```bash
# 搜索 BatchDecode 相关文件
find . -name "*batch_decode*"
# 输出：
# ./csrc/batch_decode.cu
# ./csrc/batch_decode_jit_binding.cu
# ./include/flashinfer/attention/decode_params.cuh
# ./flashinfer/decode.py
```

#### 步骤 2：理解现有实现

查看 `include/flashinfer/attention/` 下的内核实现：

```bash
# 阅读关键文件
cat include/flashinfer/attention/decode_params.cuh
cat csrc/batch_decode.cu
```

#### 步骤 3：修改内核

例如，修改线程块配置：

```cuda
// 原代码
constexpr int THREADS_PER_BLOCK = 128;

// 优化后
constexpr int THREADS_PER_BLOCK = 256;  // 增加线程数
```

或优化内存访问模式：

```cuda
// 使用 shared memory 缓存
__shared__ float shared_kv[THREADS_PER_BLOCK][HEAD_DIM];
```

#### 步骤 4：重新编译和测试

```bash
# 重新编译（可编辑模式会自动检测更改）
pip install --no-build-isolation -e . -v

# 运行相关测试
pytest tests/attention/test_batch_decode.py -v

# Benchmark 对比
python benchmarks/bench_batch_decode.py
```

#### 步骤 5：性能分析

使用 NVIDIA Profiler：

```bash
# 安装 profiler
pip install nvidia-nsight-systems

# 分析性能
nsys profile -o profile.qdrep python benchmarks/bench_batch_decode.py

# 查看结果
nsys-ui profile.qdrep
```

或使用 FlashInfer 内置的 profiler：

```python
from flashinfer.profiler import profile_kernel

with profile_kernel("batch_decode"):
    output = wrapper.run(q, paged_kv_cache)
```

## 常见开发任务

### 1. 添加新的数据类型支持

**示例：添加 INT8 支持**

1. 在 `include/flashinfer/vec_dtypes.cuh` 添加类型定义：

```cuda
template <>
struct Vec<int8_t, 4> {
  int32_t data;
  
  __device__ __forceinline__ Vec() {}
  __device__ __forceinline__ Vec(int32_t data) : data(data) {}
  
  __device__ __forceinline__ int8_t& operator[](int i) {
    return reinterpret_cast<int8_t*>(&data)[i];
  }
};
```

2. 在 Jinja2 模板中添加类型：

```jinja
{% for dtype in ['float16', 'bfloat16', 'int8'] %}
  // 生成 dtype={{dtype}} 的内核
{% endfor %}
```

3. 更新绑定代码支持新类型。

### 2. 添加新的注意力掩码模式

使用 JIT 系统：

```python
import flashinfer
from flashinfer.jit import define_variant

# 定义自定义掩码
def sliding_window_mask(q_idx, kv_idx, window_size):
    """滑动窗口注意力"""
    return abs(q_idx - kv_idx) <= window_size

# 创建变体
sliding_window_variant = define_variant(
    mask_mode="CUSTOM",
    mask_fn=sliding_window_mask,
)

# 使用
output = sliding_window_variant.run(q, k, v, window_size=256)
```

### 3. 集成到自己的项目

**方式 1：作为 Python 依赖**

```bash
pip install flashinfer-python
```

在代码中使用：

```python
import torch
import flashinfer

# 使用 FlashInfer 的内核
output = flashinfer.single_decode_with_kv_cache(q, k, v)
```

**方式 2：作为 C++ 库**

将 FlashInfer 作为头文件库包含：

```cmake
# CMakeLists.txt
include_directories(/path/to/flashinfer/include)

add_executable(my_app main.cu)
target_link_libraries(my_app cudart)
```

在代码中使用：

```cuda
#include "flashinfer/attention/decode.cuh"

// 直接调用 FlashInfer 内核
flashinfer::SingleDecodeWithKVCache<128, float16_t>(...);
```

### 4. 调试技巧

**CUDA 错误检查**：

```python
import torch
torch.cuda.synchronize()  # 同步，捕获异步错误

# 启用 CUDA 错误检查
import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
```

**打印调试信息**：

```cuda
// 在 CUDA 内核中
if (threadIdx.x == 0 && blockIdx.x == 0) {
  printf("Debug: value = %f\n", value);
}
```

**使用 cuda-gdb**：

```bash
cuda-gdb --args python test_script.py
(cuda-gdb) break my_kernel
(cuda-gdb) run
```

**使用 compute-sanitizer**：

```bash
# 检测内存错误
compute-sanitizer --tool memcheck python test_script.py

# 检测竞态条件
compute-sanitizer --tool racecheck python test_script.py
```

### 5. 性能优化建议

**内存访问优化**：
- 使用 coalesced memory access（连续访问）
- 使用 shared memory 缓存频繁访问的数据
- 避免 bank conflicts

**计算优化**：
- 使用 Tensor Cores（`wmma` 或 `mma`）
- 向量化操作（`float4`, `half2`）
- 减少分支和同步

**占用率优化**：
- 调整线程块大小
- 控制寄存器使用
- 控制 shared memory 使用

**示例：优化内存访问**

```cuda
// 差的实现：非连续访问
__global__ void bad_kernel(float* data, int n, int stride) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (idx < n) {
    float value = data[idx * stride];  // 非连续
    // ...
  }
}

// 好的实现：连续访问
__global__ void good_kernel(float* data, int n) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (idx < n) {
    float value = data[idx];  // 连续访问
    // ...
  }
}
```

## 代码规范

### 1. C++/CUDA 代码规范

- 使用 Google C++ Style Guide
- 文件名：小写字母 + 下划线（例如：`batch_decode.cu`）
- 类名：大驼峰（例如：`BatchDecodeHandler`）
- 函数名：大驼峰（例如：`ApplyAttention`）
- 变量名：小写字母 + 下划线（例如：`num_heads`）
- 常量：全大写 + 下划线（例如：`MAX_THREADS`）

### 2. Python 代码规范

- 遵循 PEP 8
- 使用 type hints
- 函数名：小写字母 + 下划线（例如：`single_decode_with_kv_cache`）
- 类名：大驼峰（例如：`BatchDecodeWrapper`）

### 3. 注释规范

```cuda
/*!
 * \brief Brief description of the function
 * \tparam T Data type
 * \param input Input tensor
 * \param output Output tensor
 * \param n Number of elements
 * \return Error code
 */
template <typename T>
cudaError_t MyFunction(const T* input, T* output, int n);
```

```python
def my_function(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
    """
    Brief description.
    
    Args:
        x: Input tensor of shape (B, N, D)
        y: Another tensor of shape (B, N, D)
        
    Returns:
        Output tensor of shape (B, N, D)
        
    Raises:
        ValueError: If x and y have different shapes
        
    Example:
        >>> x = torch.randn(2, 10, 64)
        >>> y = torch.randn(2, 10, 64)
        >>> out = my_function(x, y)
    """
    pass
```

## 贡献代码

### 提交 Pull Request 的流程

1. **Fork 仓库**

```bash
# 在 GitHub 上 Fork flashinfer 仓库
# 克隆你的 Fork
git clone https://github.com/YOUR_USERNAME/flashinfer.git
cd flashinfer
git remote add upstream https://github.com/flashinfer-ai/flashinfer.git
```

2. **创建分支**

```bash
git checkout -b my-feature-branch
```

3. **开发和测试**

```bash
# 进行修改
vim csrc/my_feature.cu

# 运行测试
pytest tests/ -v

# 运行 linter
ruff check flashinfer/
```

4. **提交代码**

```bash
git add .
git commit -m "feat: add my awesome feature"
```

Commit message 规范：
- `feat:` 新功能
- `fix:` 错误修复
- `docs:` 文档更新
- `perf:` 性能优化
- `refactor:` 代码重构
- `test:` 测试相关
- `chore:` 构建或工具相关

5. **推送并创建 PR**

```bash
git push origin my-feature-branch
# 在 GitHub 上创建 Pull Request
```

6. **响应 Review**

根据 maintainer 的反馈修改代码。

### 代码审查清单

提交前检查：
- [ ] 代码能正常编译
- [ ] 所有测试通过
- [ ] 添加了必要的测试
- [ ] 更新了文档
- [ ] 遵循代码规范
- [ ] 没有引入明显的性能回退
- [ ] Commit message 清晰

## 常见问题

### Q1: 编译错误 "undefined reference to ..."

**原因**：链接问题，通常是缺少库或链接顺序错误。

**解决**：
```bash
# 检查 CUDA 路径
echo $CUDA_HOME

# 重新安装
pip uninstall flashinfer-python
pip install --no-build-isolation -e . -v
```

### Q2: 运行时错误 "CUDA error: invalid configuration argument"

**原因**：内核配置（grid/block）不合法。

**解决**：检查线程块大小和网格大小是否超过硬件限制。

### Q3: 测试失败 "numerical mismatch"

**原因**：浮点数精度问题。

**解决**：调整容差范围：
```python
torch.testing.assert_close(a, b, rtol=1e-2, atol=1e-3)
```

### Q4: 性能不如预期

**原因**：可能的因素很多。

**解决**：
1. 使用 profiler 分析瓶颈
2. 检查内存访问模式
3. 检查占用率
4. 对比 baseline 实现

## 学习资源

### 官方资源
- 文档：https://docs.flashinfer.ai
- 博客：https://flashinfer.ai/blog
- GitHub：https://github.com/flashinfer-ai/flashinfer
- Slack：加入社区讨论

### CUDA 编程学习
- CUDA C Programming Guide
- CUDA Best Practices Guide
- CUTLASS 文档

### 相关论文
- FlashAttention: Fast and Memory-Efficient Exact Attention
- FlashAttention-2: Faster Attention with Better Parallelism
- Paged Attention (vLLM)

## 总结

二次开发 FlashInfer 的关键：

1. **熟悉架构**：理解分层设计和代码组织
2. **从简单开始**：先尝试添加简单功能
3. **充分测试**：编写全面的测试用例
4. **性能分析**：使用 profiler 指导优化
5. **参考现有代码**：学习已有的优秀实现
6. **积极交流**：在社区寻求帮助

祝你开发顺利！

