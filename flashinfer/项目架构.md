# FlashInfer 项目架构

## 整体架构

FlashInfer 采用分层架构设计，从底层 CUDA 内核到高层 Python API，清晰地分离了不同的关注点。

```
┌─────────────────────────────────────────────────────────┐
│                   Python API (用户层)                    │
│         flashinfer/  - PyTorch 风格的 Python 接口       │
└─────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────┐
│              Framework Binding (框架绑定层)             │
│       csrc/  - PyTorch C++ Extension, TVM FFI          │
└─────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────┐
│              Kernel Library (内核实现层)                │
│      include/flashinfer/  - 纯 CUDA/C++ 头文件库       │
└─────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────┐
│           Dependencies (依赖层)                          │
│   CUDA Toolkit, cuDNN, CUTLASS, TVM FFI, PyTorch       │
└─────────────────────────────────────────────────────────┘
```

## 目录结构详解

### 1. `flashinfer/` - Python 模块

这是用户直接使用的 Python API。

```
flashinfer/
├── __init__.py              # 模块入口，导出公共 API
├── __main__.py              # CLI 工具入口
├── attention.py             # 注意力机制通用接口
├── decode.py                # Decode 阶段 API
├── prefill.py               # Prefill 阶段 API
├── cascade.py               # Cascade Attention API
├── sampling.py              # 采样算子 API
├── quantization.py          # 量化操作 API
├── rope.py                  # RoPE 位置编码 API
├── norm.py                  # 归一化操作 API
├── page.py                  # Page 管理 API
├── gemm/                    # GEMM 相关 API
├── jit/                     # JIT 编译框架
├── autotuner.py             # 自动调优
├── compilation_context.py   # 编译上下文管理
├── artifacts.py             # 构建产物管理
├── cuda_utils.py            # CUDA 工具函数
└── utils.py                 # 通用工具函数
```

**关键模块说明**：

- **attention.py**: 提供统一的注意力接口，自动选择合适的内核
- **decode.py/prefill.py**: 分别处理 Decode 和 Prefill 阶段的批量计算
- **cascade.py**: 实现分层 KV-Cache 管理
- **jit/**: JIT 编译系统，支持自定义内核
- **autotuner.py**: 自动搜索最佳内核配置

### 2. `csrc/` - C++ 扩展源码

这一层将 CUDA 内核绑定到 PyTorch。

```
csrc/
├── flashinfer_*_binding.cu  # PyTorch 绑定代码
├── batch_decode.cu          # 批量 Decode 实现
├── batch_prefill.cu         # 批量 Prefill 实现
├── single_decode.cu         # 单请求 Decode
├── single_prefill.cu        # 单请求 Prefill
├── batch_decode_mla_*.cu    # MLA (Multi-head Latent Attention)
├── cascade.cu               # Cascade Attention
├── sampling.cu              # 采样算子
├── quantization.cu          # 量化算子
├── rope.cu                  # RoPE 算子
├── norm.cu                  # 归一化算子
├── page.cu                  # Page 管理
├── gemm_*.cu                # GEMM 实现
├── *_jit_binding.cu         # JIT 模板绑定
├── *_kernel_inst.jinja      # 内核实例化模板（Jinja2）
└── *_customize_config.jinja # 自定义配置模板
```

**命名规范**：
- `*_binding.cu`: PyTorch 绑定入口
- `*_kernel_inst.jinja`: Jinja2 模板，用于生成不同参数组合的内核
- `*_jit_binding.cu`: 支持 JIT 编译的绑定

**关键文件**：
- **batch_decode.cu**: 实现批量解码的核心逻辑
- **batch_prefill.cu**: 实现批量预填充的核心逻辑
- **\*_jinja**: 使用模板元编程生成多种内核实例

### 3. `include/flashinfer/` - 核心内核库

这是 FlashInfer 的核心，纯 C++/CUDA 头文件库，框架无关。

```
include/flashinfer/
├── attention/              # 注意力内核实现
│   ├── decode_params.cuh   # Decode 参数结构
│   ├── prefill_params.cuh  # Prefill 参数结构
│   ├── handler.cuh         # 内核调度器
│   ├── scheduler.cuh       # 负载均衡调度器
│   └── kernels/            # 具体的内核实现
├── gemm/                   # GEMM 内核
│   ├── group_gemm.cuh
│   └── low_latency_gemm.cuh
├── sampling.cuh            # 采样内核
├── quantization.cuh        # 量化内核
├── rope.cuh                # RoPE 内核
├── norm.cuh                # 归一化内核
├── page.cuh                # Page 管理
├── pos_enc.cuh             # 位置编码
├── math.cuh                # 数学工具函数
├── vec_dtypes.cuh          # 向量化数据类型
├── layout.cuh              # 内存布局工具
├── cp_async.cuh            # 异步拷贝工具
├── mma.cuh                 # MMA (矩阵乘累加) 工具
├── utils.cuh               # 通用工具
├── exception.h             # 异常处理
└── logging.h               # 日志工具
```

**设计原则**：
- 纯头文件库，易于集成
- 框架无关，接受原始指针
- 高度模板化，支持多种配置
- 丰富的编译时优化

**关键头文件**：
- **attention/**: 注意力计算的核心实现
- **sampling.cuh**: 高效采样算法（Top-P, Top-K）
- **vec_dtypes.cuh**: 向量化类型，支持 SIMD 操作
- **math.cuh**: 高性能数学函数（exp, log 等）

### 4. `docs/` - 文档

```
docs/
├── index.rst               # 文档首页
├── installation.rst        # 安装指南
├── api/                    # API 文档
│   ├── decode.rst
│   ├── prefill.rst
│   ├── cascade.rst
│   ├── sampling.rst
│   └── ...
├── tutorials/              # 教程
│   ├── kv_layout.rst       # KV-Cache 布局
│   └── recipes.rst         # 使用示例
└── conf.py                 # Sphinx 配置
```

### 5. `tests/` - 测试

```
tests/
├── attention/              # 注意力测试
│   ├── test_batch_decode.py
│   ├── test_batch_prefill.py
│   └── test_single_decode.py
├── gemm/                   # GEMM 测试
├── model_optimizations/    # 端到端模型测试
├── utils/                  # 测试工具
│   ├── test_jit_example.py  # JIT 使用示例
│   └── test_helpers/
└── conftest.py             # pytest 配置
```

### 6. `benchmarks/` - 性能测试

```
benchmarks/
├── bench_batch_decode.py
├── bench_batch_prefill.py
├── bench_sampling.py
└── README.md
```

### 7. `3rdparty/` - 第三方依赖

```
3rdparty/
├── cutlass/                # NVIDIA CUTLASS 库
├── spdlog/                 # 日志库
└── ...
```

### 8. 构建和配置文件

```
/
├── pyproject.toml          # Python 项目配置
├── build_backend.py        # 自定义构建后端
├── build_utils.py          # 构建工具函数
├── CMakeLists.txt          # (可能存在) CMake 配置
├── setup.py                # (旧式) 安装脚本
├── requirements.txt        # Python 依赖
└── pytest.ini              # pytest 配置
```

## 核心设计模式

### 1. 模板元编程

FlashInfer 大量使用 C++ 模板和 Jinja2 模板：

**C++ 模板**：
```cpp
template <uint32_t HEAD_DIM, typename T, typename IdType>
__global__ void BatchDecodeKernel(...) {
    // 内核实现
}
```

**Jinja2 模板**：
```jinja
{% for head_dim in [64, 128, 256] %}
{% for dtype in ['float16', 'bfloat16'] %}
    // 生成 head_dim={{head_dim}}, dtype={{dtype}} 的内核
{% endfor %}
{% endfor %}
```

优势：
- 编译时特化，零运行时开销
- 生成多种参数组合
- 保持代码简洁

### 2. Plan-Run 分离

```python
# Plan 阶段：分析输入，生成调度策略
wrapper.plan(
    indptr=indptr,              # 序列边界
    last_page_len=last_page_len,
    num_qo_heads=num_qo_heads,
    num_kv_heads=num_kv_heads,
)

# Run 阶段：执行计算（可 CUDAGraph 捕获）
output = wrapper.run(q, paged_kv_cache)
```

### 3. Wrapper 模式

FlashInfer 使用 Wrapper 类管理内核状态：

```python
wrapper = BatchDecodeWithPagedKVCacheWrapper(
    workspace_buffer=workspace_buffer,
    kv_layout="NHD",  # or "HND"
)
```

Wrapper 负责：
- 管理工作空间内存
- 缓存调度计划
- 选择合适的内核
- 处理错误和异常

### 4. 即时编译（JIT）

```python
# 定义自定义变体
variant = flashinfer.jit.define_variant(
    mask_mode="CUSTOM",
    score_fn=custom_score_function,
)

# 编译
kernel = flashinfer.jit.compile(variant)

# 使用
output = kernel.run(q, k, v)
```

## 构建系统

### 构建流程

```
源代码 (*.cu, *.cuh)
    ↓
Jinja2 模板渲染
    ↓
生成 CUDA 源文件
    ↓
NVCC 编译
    ↓
PyTorch C++ Extension
    ↓
Python Wheel 包
```

### 核心构建文件

1. **pyproject.toml**：定义项目元数据和构建系统
2. **build_backend.py**：自定义构建逻辑
3. **build_utils.py**：构建辅助函数

### 构建选项

通过环境变量控制构建：

```bash
# 指定 GPU 架构
export FLASHINFER_CUDA_ARCH_LIST="7.5 8.0 8.9 9.0"

# 启用调试
export FLASHINFER_DEBUG=1

# 禁用 JIT
export FLASHINFER_DISABLE_JIT=1
```

## 内存管理

### Workspace Buffer

FlashInfer 使用 workspace buffer 存储临时数据：

```python
workspace_buffer = torch.empty(32 * 1024 * 1024, dtype=torch.uint8, device='cuda')
wrapper = BatchDecodeWrapper(workspace_buffer=workspace_buffer)
```

内容包括：
- 调度元数据
- 中间计算结果
- 原子操作计数器

### 页面 KV-Cache

```python
# 页面布局：[num_pages, page_size, num_heads, head_dim]
paged_kv_cache = torch.empty(
    (num_pages, page_size, num_kv_heads, head_dim),
    dtype=torch.float16,
    device='cuda'
)
```

## 多架构支持

FlashInfer 针对不同 GPU 架构编译不同的内核：

```python
# 运行时选择
if torch.cuda.get_device_capability()[0] >= 9:  # SM90+
    use_sm90_kernel()
elif torch.cuda.get_device_capability()[0] >= 8:  # SM80+
    use_sm80_kernel()
else:  # SM75+
    use_sm75_kernel()
```

## 扩展点

### 1. 添加新内核

1. 在 `include/flashinfer/` 定义内核
2. 在 `csrc/` 添加绑定
3. 在 `flashinfer/` 添加 Python API
4. 在 `tests/` 添加测试

### 2. 添加新的注意力变体

使用 JIT 系统：

```python
from flashinfer.jit import define_variant

my_variant = define_variant(
    mask_mode="CUSTOM",
    score_fn=my_score_function,
    post_process_fn=my_post_process,
)
```

### 3. 添加新的数据类型

1. 在 `vec_dtypes.cuh` 添加类型定义
2. 在 Jinja2 模板添加类型支持
3. 更新绑定代码
4. 添加测试

## 依赖关系图

```
flashinfer (Python)
  ├── torch (PyTorch)
  ├── numpy
  ├── apache-tvm-ffi (TVM FFI)
  ├── nvidia-cudnn-frontend
  ├── nvidia-cutlass-dsl
  └── einops

flashinfer (C++)
  ├── CUDA Toolkit
  ├── CUTLASS (头文件)
  ├── cuDNN (运行时)
  └── PyTorch C++ (编译时)
```

## 总结

FlashInfer 的架构体现了以下设计原则：

1. **分层清晰**：从底层 CUDA 到高层 Python API 层次分明
2. **模块化**：每个模块职责单一，易于维护
3. **可扩展**：通过 JIT 和模板支持自定义
4. **高性能**：大量编译时优化，零运行时开销
5. **易用性**：提供 PyTorch 风格的友好 API
6. **生产就绪**：完善的测试、文档和错误处理

理解这个架构是进行二次开发和深入优化的基础。

