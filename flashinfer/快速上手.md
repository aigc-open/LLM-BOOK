# FlashInfer å¿«é€Ÿä¸Šæ‰‹æŒ‡å—

æœ¬æŒ‡å—å°†å¸®åŠ©ä½ åœ¨æœ€çŸ­æ—¶é—´å†…å¼€å§‹ä½¿ç”¨ FlashInferã€‚æˆ‘ä»¬ä¼šä»å®‰è£…åˆ°è¿è¡Œç¬¬ä¸€ä¸ªç¤ºä¾‹ï¼Œä¸€æ­¥æ­¥å¼•å¯¼ä½ ã€‚

## å‰ç½®å‡†å¤‡

### æ£€æŸ¥ç¯å¢ƒ

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿ä½ æœ‰ï¼š

1. **NVIDIA GPU**ï¼ˆæ”¯æŒ CUDA è®¡ç®—èƒ½åŠ› 7.5+ï¼‰
   ```bash
   # æ£€æŸ¥ GPU
   nvidia-smi
   
   # æŸ¥çœ‹è®¡ç®—èƒ½åŠ›
   nvidia-smi --query-gpu=compute_cap --format=csv
   ```

2. **CUDA Toolkit**ï¼ˆæ¨è 12.0+ï¼‰
   ```bash
   nvcc --version
   ```

3. **Python**ï¼ˆ3.10 æˆ–æ›´é«˜ï¼‰
   ```bash
   python --version
   ```

4. **PyTorch**ï¼ˆå»ºè®®å…ˆå®‰è£…ï¼‰
   ```bash
   pip install torch --index-url https://download.pytorch.org/whl/cu121
   ```

## å¿«é€Ÿå®‰è£…

### æ–¹å¼ 1ï¼šä» PyPI å®‰è£…ï¼ˆæ¨èæ–°æ‰‹ï¼‰

æœ€ç®€å•çš„å®‰è£…æ–¹å¼ï¼š

```bash
# å®‰è£…æ ¸å¿ƒåŒ…
pip install flashinfer-python

# ï¼ˆå¯é€‰ï¼‰å®‰è£…é¢„ç¼–è¯‘çš„å†…æ ¸ï¼ŒåŠ å¿«åˆå§‹åŒ–é€Ÿåº¦
pip install flashinfer-cubin

# ï¼ˆå¯é€‰ï¼‰å®‰è£… JIT ç¼“å­˜ï¼ˆæ›¿æ¢ cu129 ä¸ºä½ çš„ CUDA ç‰ˆæœ¬ï¼šcu128/cu129/cu130ï¼‰
pip install flashinfer-jit-cache --index-url https://flashinfer.ai/whl/cu129
```

### æ–¹å¼ 2ï¼šä»æºç å®‰è£…ï¼ˆå¼€å‘è€…ï¼‰

å¦‚æœä½ æƒ³ä¿®æ”¹æºç æˆ–å‚ä¸å¼€å‘ï¼š

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/flashinfer-ai/flashinfer.git --recursive
cd flashinfer

# å®‰è£…
pip install -v .

# æˆ–è€…å¯ç¼–è¾‘æ¨¡å¼ï¼ˆæ¨èå¼€å‘ï¼‰
pip install --no-build-isolation -e . -v
```

### éªŒè¯å®‰è£…

```bash
# éªŒè¯å®‰è£…
flashinfer show-config

# æµ‹è¯•å¯¼å…¥
python -c "import flashinfer; print(f'FlashInfer {flashinfer.__version__} installed successfully!')"
```

å¦‚æœçœ‹åˆ°ç‰ˆæœ¬å·è¾“å‡ºï¼Œè¯´æ˜å®‰è£…æˆåŠŸï¼

## ç¬¬ä¸€ä¸ªä¾‹å­ï¼šå•è¯·æ±‚æ³¨æ„åŠ›

è®©æˆ‘ä»¬ä»æœ€ç®€å•çš„ä¾‹å­å¼€å§‹ - å•ä¸ªè¯·æ±‚çš„æ³¨æ„åŠ›è®¡ç®—ã€‚

### ç¤ºä¾‹ 1ï¼šDecode é˜¶æ®µï¼ˆå• token ç”Ÿæˆï¼‰

```python
import torch
import flashinfer

# è®¾ç½®è®¾å¤‡
device = 'cuda:0'

# æ¨¡æ‹Ÿå‚æ•°ï¼ˆä»¥ LLaMA-7B ä¸ºä¾‹ï¼‰
kv_len = 2048          # KV-Cache é•¿åº¦ï¼ˆå·²ç”Ÿæˆçš„ tokensï¼‰
num_qo_heads = 32      # Query/Output å¤´æ•°
num_kv_heads = 32      # KV å¤´æ•°ï¼ˆGQA æ—¶å¯ä»¥å°äº qo_headsï¼‰
head_dim = 128         # æ¯ä¸ªå¤´çš„ç»´åº¦

# åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
k = torch.randn(kv_len, num_kv_heads, head_dim, dtype=torch.float16, device=device)
v = torch.randn(kv_len, num_kv_heads, head_dim, dtype=torch.float16, device=device)
q = torch.randn(num_qo_heads, head_dim, dtype=torch.float16, device=device)

# è°ƒç”¨ FlashInfer
output = flashinfer.single_decode_with_kv_cache(q, k, v)

print(f"Input query shape: {q.shape}")
print(f"KV cache shape: {k.shape}")
print(f"Output shape: {output.shape}")
print(f"Output dtype: {output.dtype}")
```

**è¾“å‡ºï¼š**
```
Input query shape: torch.Size([32, 128])
KV cache shape: torch.Size([2048, 32, 128])
Output shape: torch.Size([32, 128])
Output dtype: torch.float16
```

### ç¤ºä¾‹ 2ï¼šPrefill é˜¶æ®µï¼ˆå¤„ç†å®Œæ•´ promptï¼‰

```python
import torch
import flashinfer

device = 'cuda:0'

# Prefill å‚æ•°
qo_len = 512           # Query é•¿åº¦ï¼ˆprompt é•¿åº¦ï¼‰
kv_len = 512           # KV é•¿åº¦ï¼ˆé€šå¸¸ç­‰äº qo_lenï¼‰
num_qo_heads = 32
num_kv_heads = 32
head_dim = 128

# åˆ›å»ºæ•°æ®
q = torch.randn(qo_len, num_qo_heads, head_dim, dtype=torch.float16, device=device)
k = torch.randn(kv_len, num_kv_heads, head_dim, dtype=torch.float16, device=device)
v = torch.randn(kv_len, num_kv_heads, head_dim, dtype=torch.float16, device=device)

# Prefill æ³¨æ„åŠ›ï¼ˆå¸¦å› æœæ©ç ï¼‰
output = flashinfer.single_prefill_with_kv_cache(q, k, v, causal=True)

print(f"Query shape: {q.shape}")
print(f"Output shape: {output.shape}")
```

### ç¤ºä¾‹ 3ï¼šå¸¦ RoPE çš„æ³¨æ„åŠ›

```python
import torch
import flashinfer

device = 'cuda:0'

# å‚æ•°
kv_len = 2048
num_qo_heads = 32
num_kv_heads = 32
head_dim = 128

# åˆ›å»ºæ•°æ®
k = torch.randn(kv_len, num_kv_heads, head_dim, dtype=torch.float16, device=device)
v = torch.randn(kv_len, num_kv_heads, head_dim, dtype=torch.float16, device=device)
q = torch.randn(num_qo_heads, head_dim, dtype=torch.float16, device=device)

# ä½¿ç”¨ LLaMA é£æ ¼çš„ RoPE
output_with_rope = flashinfer.single_decode_with_kv_cache(
    q, k, v, 
    pos_encoding_mode="ROPE_LLAMA"  # å¯ç”¨ RoPE
)

print(f"Output with RoPE: {output_with_rope.shape}")
```

## æ‰¹é‡å¤„ç†

åœ¨å®é™…çš„ LLM æœåŠ¡ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸éœ€è¦åŒæ—¶å¤„ç†å¤šä¸ªè¯·æ±‚ã€‚

### ç¤ºä¾‹ 4ï¼šæ‰¹é‡ Decode

```python
import torch
import flashinfer

device = 'cuda:0'

# æ‰¹é‡å‚æ•°
batch_size = 8
max_seq_len = 2048
num_qo_heads = 32
num_kv_heads = 32
head_dim = 128

# æ¯ä¸ªè¯·æ±‚çš„åºåˆ—é•¿åº¦ï¼ˆå˜é•¿ï¼‰
seq_lens = torch.tensor([128, 256, 512, 1024, 1500, 2000, 2048, 1800], device=device)

# åˆ›å»º Wrapper
workspace_buffer = torch.empty(128 * 1024 * 1024, dtype=torch.uint8, device=device)
wrapper = flashinfer.BatchDecodeWithPagedKVCacheWrapper(
    workspace_buffer=workspace_buffer,
    kv_layout="NHD",
)

# å‡†å¤‡æ‰¹é‡æ•°æ®
# queries: [batch_size, num_qo_heads, head_dim]
queries = torch.randn(batch_size, num_qo_heads, head_dim, dtype=torch.float16, device=device)

# å‡†å¤‡ paged KV-Cacheï¼ˆç®€åŒ–ç¤ºä¾‹ï¼Œå®é™…ä½¿ç”¨ä¸­ä¼šæ›´å¤æ‚ï¼‰
# è¿™é‡Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªè¿ç»­çš„ KV cache ä½œä¸ºæ¼”ç¤º
num_pages = 256
page_size = 16
paged_k = torch.randn(num_pages, page_size, num_kv_heads, head_dim, dtype=torch.float16, device=device)
paged_v = torch.randn(num_pages, page_size, num_kv_heads, head_dim, dtype=torch.float16, device=device)

# åˆ›å»º indptrï¼ˆæŒ‡ç¤ºæ¯ä¸ªè¯·æ±‚åœ¨ batch ä¸­çš„è¾¹ç•Œï¼‰
indptr = torch.zeros(batch_size + 1, dtype=torch.int32, device=device)
indptr[1:] = torch.cumsum(seq_lens // page_size, dim=0)

# Plan é˜¶æ®µ
wrapper.plan(
    indptr=indptr,
    indices=torch.arange(num_pages, dtype=torch.int32, device=device),
    last_page_len=torch.full((batch_size,), page_size, dtype=torch.int32, device=device),
    num_qo_heads=num_qo_heads,
    num_kv_heads=num_kv_heads,
    head_dim=head_dim,
    page_size=page_size,
)

# Run é˜¶æ®µ
output = wrapper.run(queries, paged_k, paged_v)

print(f"Batch queries shape: {queries.shape}")
print(f"Batch output shape: {output.shape}")
```

## å®æˆ˜ç¤ºä¾‹ï¼šç®€å•çš„ç”Ÿæˆå¾ªç¯

è®©æˆ‘ä»¬å®ç°ä¸€ä¸ªç®€å•çš„ token ç”Ÿæˆå¾ªç¯ã€‚

### ç¤ºä¾‹ 5ï¼šæ–‡æœ¬ç”Ÿæˆ

```python
import torch
import flashinfer

def generate_tokens(prompt_tokens, max_new_tokens=20):
    """
    ç®€åŒ–çš„æ–‡æœ¬ç”Ÿæˆå‡½æ•°
    
    Args:
        prompt_tokens: prompt çš„ token IDs
        max_new_tokens: ç”Ÿæˆçš„æ–° token æ•°é‡
    """
    device = 'cuda:0'
    
    # æ¨¡å‹å‚æ•°
    vocab_size = 32000
    num_heads = 32
    head_dim = 128
    hidden_size = num_heads * head_dim
    
    # åˆå§‹åŒ– KV-Cache
    max_seq_len = len(prompt_tokens) + max_new_tokens
    k_cache = torch.zeros(max_seq_len, num_heads, head_dim, dtype=torch.float16, device=device)
    v_cache = torch.zeros(max_seq_len, num_heads, head_dim, dtype=torch.float16, device=device)
    
    # Prefill é˜¶æ®µï¼šå¤„ç† prompt
    prompt_len = len(prompt_tokens)
    
    # è¿™é‡Œæˆ‘ä»¬æ¨¡æ‹Ÿ embedding å’Œ Q/K/V æŠ•å½±
    # å®é™…ä½¿ç”¨ä¸­ï¼Œè¿™äº›åº”è¯¥æ¥è‡ªçœŸå®çš„æ¨¡å‹
    q_prefill = torch.randn(prompt_len, num_heads, head_dim, dtype=torch.float16, device=device)
    k_prefill = torch.randn(prompt_len, num_heads, head_dim, dtype=torch.float16, device=device)
    v_prefill = torch.randn(prompt_len, num_heads, head_dim, dtype=torch.float16, device=device)
    
    # å¡«å…… KV-Cache
    k_cache[:prompt_len] = k_prefill
    v_cache[:prompt_len] = v_prefill
    
    # Prefill attention
    prefill_output = flashinfer.single_prefill_with_kv_cache(
        q_prefill, 
        k_cache[:prompt_len], 
        v_cache[:prompt_len], 
        causal=True
    )
    
    print(f"Prefill completed for {prompt_len} tokens")
    
    # Decode é˜¶æ®µï¼šé€ä¸ªç”Ÿæˆ token
    generated_tokens = []
    current_pos = prompt_len
    
    for step in range(max_new_tokens):
        # æ¨¡æ‹Ÿå½“å‰ token çš„ Q/K/Vï¼ˆå®é™…åº”è¯¥æ¥è‡ªæ¨¡å‹ï¼‰
        q_decode = torch.randn(num_heads, head_dim, dtype=torch.float16, device=device)
        k_decode = torch.randn(num_heads, head_dim, dtype=torch.float16, device=device)
        v_decode = torch.randn(num_heads, head_dim, dtype=torch.float16, device=device)
        
        # æ·»åŠ åˆ° KV-Cache
        k_cache[current_pos] = k_decode
        v_cache[current_pos] = v_decode
        current_pos += 1
        
        # Decode attention
        decode_output = flashinfer.single_decode_with_kv_cache(
            q_decode,
            k_cache[:current_pos],
            v_cache[:current_pos]
        )
        
        # æ¨¡æ‹Ÿ logits å’Œé‡‡æ ·ï¼ˆå®é™…åº”è¯¥é€šè¿‡ LM headï¼‰
        logits = torch.randn(vocab_size, device=device)
        next_token = torch.argmax(logits).item()
        
        generated_tokens.append(next_token)
        print(f"Step {step + 1}: Generated token {next_token}")
    
    return generated_tokens

# ä½¿ç”¨ç¤ºä¾‹
prompt = [1, 2, 3, 4, 5]  # æ¨¡æ‹Ÿçš„ token IDs
generated = generate_tokens(prompt, max_new_tokens=5)
print(f"\nGenerated tokens: {generated}")
```

## é«˜çº§åŠŸèƒ½

### ç¤ºä¾‹ 6ï¼šä½¿ç”¨ PageAttention

PageAttention æ˜¯é«˜æ•ˆç®¡ç† KV-Cache çš„å…³é”®æŠ€æœ¯ã€‚

```python
import torch
import flashinfer

device = 'cuda:0'

# Page å‚æ•°
page_size = 16        # æ¯ä¸ª page çš„ token æ•°
num_pages = 128       # æ€» page æ•°
num_heads = 32
head_dim = 128

# åˆ›å»º paged KV-Cache
# å¸ƒå±€ï¼š[num_pages, page_size, num_heads, head_dim]
paged_k = torch.randn(num_pages, page_size, num_heads, head_dim, dtype=torch.float16, device=device)
paged_v = torch.randn(num_pages, page_size, num_heads, head_dim, dtype=torch.float16, device=device)

# åˆ›å»º page tableï¼ˆæ¯ä¸ªè¯·æ±‚ä½¿ç”¨å“ªäº› pagesï¼‰
# ä¾‹å¦‚ï¼šç¬¬ä¸€ä¸ªè¯·æ±‚ä½¿ç”¨ page 0-7ï¼ˆ128 tokensï¼‰
batch_size = 4
max_pages_per_seq = 8

page_table = torch.zeros(batch_size, max_pages_per_seq, dtype=torch.int32, device=device)
for i in range(batch_size):
    start_page = i * max_pages_per_seq
    page_table[i] = torch.arange(start_page, start_page + max_pages_per_seq, dtype=torch.int32)

# æ¯ä¸ªè¯·æ±‚å®é™…ä½¿ç”¨çš„ page æ•°
num_pages_per_seq = torch.tensor([8, 6, 7, 8], dtype=torch.int32, device=device)

print(f"Paged KV-Cache shape: {paged_k.shape}")
print(f"Page table shape: {page_table.shape}")
print(f"Pages per sequence: {num_pages_per_seq}")
```

### ç¤ºä¾‹ 7ï¼šé‡‡æ ·ï¼ˆTop-P, Top-Kï¼‰

FlashInfer æä¾›é«˜æ•ˆçš„é‡‡æ ·ç®—å­ã€‚

```python
import torch
import flashinfer

device = 'cuda:0'

# æ¨¡æ‹Ÿ logits
batch_size = 4
vocab_size = 32000
logits = torch.randn(batch_size, vocab_size, dtype=torch.float32, device=device)

# Top-P é‡‡æ ·ï¼ˆnucleus samplingï¼‰
sampled_tokens_top_p, success = flashinfer.sampling.top_p_sampling_from_logits(
    logits, 
    p=0.9,  # ç´¯ç§¯æ¦‚ç‡é˜ˆå€¼
)

print(f"Top-P sampled tokens: {sampled_tokens_top_p}")

# Top-K é‡‡æ ·
sampled_tokens_top_k, success = flashinfer.sampling.top_k_sampling_from_logits(
    logits,
    k=50,  # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ 50 ä¸ª tokens
)

print(f"Top-K sampled tokens: {sampled_tokens_top_k}")

# Top-K + Top-P ç»„åˆ
sampled_tokens_combined, success = flashinfer.sampling.top_k_top_p_sampling_from_logits(
    logits,
    k=50,
    p=0.9,
)

print(f"Combined sampled tokens: {sampled_tokens_combined}")
```

### ç¤ºä¾‹ 8ï¼šä½ç²¾åº¦è®¡ç®—ï¼ˆFP8ï¼‰

FlashInfer æ”¯æŒ FP8 ç­‰ä½ç²¾åº¦æ ¼å¼ä»¥èŠ‚çœæ˜¾å­˜å’ŒåŠ é€Ÿè®¡ç®—ã€‚

```python
import torch
import flashinfer

device = 'cuda:0'

# æ£€æŸ¥æ˜¯å¦æ”¯æŒ FP8ï¼ˆéœ€è¦ H100 æˆ–æ›´æ–°çš„ GPUï¼‰
if torch.cuda.get_device_capability()[0] >= 9:
    # FP8 å‚æ•°
    kv_len = 2048
    num_qo_heads = 32
    num_kv_heads = 32
    head_dim = 128
    
    # åˆ›å»º FP16 çš„ Q
    q = torch.randn(num_qo_heads, head_dim, dtype=torch.float16, device=device)
    
    # åˆ›å»º FP8 çš„ K, Vï¼ˆä½¿ç”¨ torch.float8_e4m3fnï¼‰
    k_fp8 = torch.randn(kv_len, num_kv_heads, head_dim, dtype=torch.float8_e4m3fn, device=device)
    v_fp8 = torch.randn(kv_len, num_kv_heads, head_dim, dtype=torch.float8_e4m3fn, device=device)
    
    # ä½¿ç”¨ FP8 KV-Cache
    output = flashinfer.single_decode_with_kv_cache(q, k_fp8, v_fp8)
    
    print(f"FP8 attention output: {output.shape}")
else:
    print("FP8 not supported on this GPU (éœ€è¦ SM90+)")
```

## æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. ä½¿ç”¨ CUDAGraph

CUDAGraph å¯ä»¥æ˜¾è‘—å‡å°‘ kernel å¯åŠ¨å¼€é”€ï¼š

```python
import torch
import flashinfer

device = 'cuda:0'

# å‡†å¤‡æ•°æ®
q = torch.randn(32, 128, dtype=torch.float16, device=device)
k = torch.randn(2048, 32, 128, dtype=torch.float16, device=device)
v = torch.randn(2048, 32, 128, dtype=torch.float16, device=device)
output = torch.empty_like(q)

# Warmup
for _ in range(3):
    output = flashinfer.single_decode_with_kv_cache(q, k, v)

# æ•è· CUDAGraph
graph = torch.cuda.CUDAGraph()
with torch.cuda.graph(graph):
    output = flashinfer.single_decode_with_kv_cache(q, k, v)

# ä½¿ç”¨ CUDAGraph è¿è¡Œï¼ˆæ›´å¿«ï¼ï¼‰
graph.replay()

print("CUDAGraph captured and replayed successfully!")
```

### 2. æ‰¹é‡å¤„ç†ä»¥æé«˜ååé‡

```python
# ä¸å¥½ï¼šé€ä¸ªå¤„ç†
for i in range(batch_size):
    output_i = flashinfer.single_decode_with_kv_cache(q[i], k[i], v[i])

# å¥½ï¼šæ‰¹é‡å¤„ç†
wrapper = flashinfer.BatchDecodeWithPagedKVCacheWrapper(...)
output = wrapper.run(q_batch, k_batch, v_batch)
```

### 3. é‡ç”¨ Wrapper

```python
# ä¸å¥½ï¼šæ¯æ¬¡åˆ›å»ºæ–°çš„ wrapper
for batch in batches:
    wrapper = flashinfer.BatchDecodeWrapper(...)
    output = wrapper.run(...)

# å¥½ï¼šé‡ç”¨ wrapper
wrapper = flashinfer.BatchDecodeWrapper(...)
for batch in batches:
    wrapper.plan(...)  # åªéœ€ re-plan
    output = wrapper.run(...)
```

## å¸¸è§é—®é¢˜æ’æŸ¥

### Q1: å¯¼å…¥é”™è¯¯

```python
# é”™è¯¯ï¼šImportError: cannot import name 'xxx'
# è§£å†³ï¼šç¡®ä¿æ­£ç¡®å®‰è£…
pip install --upgrade flashinfer-python

# æˆ–é‡æ–°ç¼–è¯‘
pip install --no-build-isolation --force-reinstall -e . -v
```

### Q2: CUDA å†…å­˜ä¸è¶³

```python
# é”™è¯¯ï¼šRuntimeError: CUDA out of memory
# è§£å†³ï¼šå‡å° batch size æˆ–åºåˆ—é•¿åº¦
# æˆ–ä½¿ç”¨ä½ç²¾åº¦ï¼ˆFP8, FP16ï¼‰

# æ¸…ç†ç¼“å­˜
torch.cuda.empty_cache()
```

### Q3: æ€§èƒ½ä¸å¦‚é¢„æœŸ

```python
# æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†ä¼˜åŒ–çš„å®ç°
import flashinfer
print(flashinfer.get_cuda_version())

# ç¡®ä¿æ•°æ®åœ¨ GPU ä¸Š
assert tensor.is_cuda

# ä½¿ç”¨ FP16/BF16 è€Œé FP32
tensor = tensor.half()  # æˆ– .bfloat16()
```

### Q4: æ•°å€¼ä¸åŒ¹é…

```python
# å¯èƒ½æ˜¯ç²¾åº¦é—®é¢˜ï¼Œä½¿ç”¨æ›´å®½æ¾çš„å®¹å·®
torch.testing.assert_close(output1, output2, rtol=1e-2, atol=1e-3)
```

## ä¸‹ä¸€æ­¥å­¦ä¹ 

æ­å–œä½ å®Œæˆå¿«é€Ÿä¸Šæ‰‹ï¼æ¥ä¸‹æ¥å¯ä»¥ï¼š

1. **æ·±å…¥å­¦ä¹ **ï¼š
   - é˜…è¯»ã€Šé¡¹ç›®æ¶æ„.mdã€‹äº†è§£å†…éƒ¨è®¾è®¡
   - é˜…è¯»ã€ŠäºŒæ¬¡å¼€å‘æŒ‡å—.mdã€‹å­¦ä¹ å¦‚ä½•æ‰©å±•

2. **æŸ¥çœ‹å®˜æ–¹ç¤ºä¾‹**ï¼š
   ```bash
   cd flashinfer/tests
   # æŸ¥çœ‹æ›´å¤šæµ‹è¯•ç”¨ä¾‹ä½œä¸ºç¤ºä¾‹
   ls test_*.py
   ```

3. **é˜…è¯»æ–‡æ¡£**ï¼š
   - å®˜æ–¹æ–‡æ¡£ï¼šhttps://docs.flashinfer.ai
   - API å‚è€ƒï¼šhttps://docs.flashinfer.ai/api/

4. **å®è·µé¡¹ç›®**ï¼š
   - é›†æˆåˆ°è‡ªå·±çš„ LLM æœåŠ¡
   - å°è¯•ä¸åŒçš„æ³¨æ„åŠ›å˜ä½“
   - æ€§èƒ½è°ƒä¼˜å’ŒåŸºå‡†æµ‹è¯•

5. **å‚ä¸ç¤¾åŒº**ï¼š
   - GitHub: https://github.com/flashinfer-ai/flashinfer
   - Slack: åŠ å…¥ç¤¾åŒºè®¨è®º
   - æå‡ºé—®é¢˜å’Œè´¡çŒ®ä»£ç 

## å®Œæ•´ç¤ºä¾‹ï¼šç«¯åˆ°ç«¯ Demo

```python
import torch
import flashinfer

def complete_demo():
    """å®Œæ•´çš„ FlashInfer ä½¿ç”¨ç¤ºä¾‹"""
    print("=" * 60)
    print("FlashInfer Complete Demo")
    print("=" * 60)
    
    device = 'cuda:0'
    
    # 1. å•è¯·æ±‚ Decode
    print("\n1. Single Decode Attention")
    q = torch.randn(32, 128, dtype=torch.float16, device=device)
    k = torch.randn(2048, 32, 128, dtype=torch.float16, device=device)
    v = torch.randn(2048, 32, 128, dtype=torch.float16, device=device)
    
    output_decode = flashinfer.single_decode_with_kv_cache(q, k, v)
    print(f"   Output shape: {output_decode.shape}")
    
    # 2. å•è¯·æ±‚ Prefill
    print("\n2. Single Prefill Attention")
    q = torch.randn(512, 32, 128, dtype=torch.float16, device=device)
    k = torch.randn(512, 32, 128, dtype=torch.float16, device=device)
    v = torch.randn(512, 32, 128, dtype=torch.float16, device=device)
    
    output_prefill = flashinfer.single_prefill_with_kv_cache(q, k, v, causal=True)
    print(f"   Output shape: {output_prefill.shape}")
    
    # 3. RoPE
    print("\n3. Attention with RoPE")
    q = torch.randn(32, 128, dtype=torch.float16, device=device)
    k = torch.randn(2048, 32, 128, dtype=torch.float16, device=device)
    v = torch.randn(2048, 32, 128, dtype=torch.float16, device=device)
    
    output_rope = flashinfer.single_decode_with_kv_cache(
        q, k, v, pos_encoding_mode="ROPE_LLAMA"
    )
    print(f"   Output shape: {output_rope.shape}")
    
    # 4. é‡‡æ ·
    print("\n4. Sampling")
    logits = torch.randn(4, 32000, dtype=torch.float32, device=device)
    sampled_tokens, _ = flashinfer.sampling.top_p_sampling_from_logits(logits, p=0.9)
    print(f"   Sampled tokens: {sampled_tokens}")
    
    print("\n" + "=" * 60)
    print("Demo completed successfully!")
    print("=" * 60)

if __name__ == "__main__":
    complete_demo()
```

è¿è¡Œï¼š

```bash
python demo.py
```

ç¥ä½ ä½¿ç”¨æ„‰å¿«ï¼ğŸš€

